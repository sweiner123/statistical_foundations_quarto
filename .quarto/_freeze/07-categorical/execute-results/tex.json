{
  "hash": "f6cd9f3d3a1eaa4efe4564aa05c60931",
  "result": {
    "markdown": "# Categorical Data Analysis {#sec-cat}\n\nEverything analysis covered so far has used a continuous variable as a target variable of interest. What if our target variable was categorical instead of continuous? Our analysis must change to adjust.\n\nThis Chapter aims to answer the following questions:\n\n<ul>\n\n<li>\n\nHow do you explore categorical variables?\n\n<ul>\n\n<li>\n\nNominal vs. Ordinal\n\n<li>\n\nTests of Association\n\n<li>\n\nMeasures of Association\n\n</ul>\n\n<li>\n\nHow do you model a categorical target variable?\n\n<ul>\n\n<li>\n\nLogistic Regression\n\n<li>\n\nInterpreting Logistic Regression\n\n<li>\n\nAssessing Logistic Regression\n\n</ul>\n\n</ul>\n\n## Describing Categorical Data\n\nSimilar to Chapter @sec-eda, we need to first explore our data before building any models to try and explain/predict our categorical target variable. With categorical variables, we can look at the distribution of the categories as well as see if this distribution has any association with other variables. For this analysis we are going to still use our Ames housing data. Imagine you worked for a real estate agency and got a bonus check if you sold a house above \\$175,000 in value. Let's create this variable in our data:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nlibrary(AmesHousing)\names <- make_ordinal_ames()\n\nset.seed(123)\n\names <- ames %>% mutate(id = row_number())\n\ntrain <- ames %>% sample_frac(0.7)\n\ntest <- anti_join(ames, train, by = 'id')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- train %>%\n  mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\n```\n:::\n\n\n\nYou are interested in what variables might be associated with obtaining a higher chance of getting a bonus (selling a house above \\$175,000). An association exists between two categorical variables if the distribution of one variable changes when the value of the other categorical changes. If there is no association, the distribution of the first variable is the same regardless of the value of the other variable. For example, if we wanted to know if obtaining a bonus on selling a house in Ames, Iowa was associated with whether the house had central air we could look at the distribution of bonus eligible houses. If we observe that 42% of homes with central air are bonus eligible and 42% of homes without central air are bonus eligible, then it appears that central air has no bearing on whether the home is bonus eligible. However, if instead we observe that only 3% of homes without central air are bonus eligible, but 44% of home with central air are bonus eligible, then it appears that having central air might be related to a home being bonus eligible.\n\nTo understand the distribution of categorical variables we need to look at frequency tables. A frequency table shows the number of observations that occur in certain categories or intervals. A one way frequency table examines all the categories of one variable. These are easily visualized with bar charts.\n\nLet's look at the distribution of both bonus eligibility and central air using the `table` function. The `ggplot` function with the `geom_bar` function allows us to view our data in a bar chart.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   0    1 \n1211  840 \n```\n:::\n\n```{.r .cell-code}\nggplot(data = train) +\n  geom_bar(mapping = aes(x = Bonus))\n```\n\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(train$Central_Air)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   N    Y \n 147 1904 \n```\n:::\n\n```{.r .cell-code}\nggplot(data = train) +\n  geom_bar(mapping = aes(x = Central_Air))\n```\n\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nFrequency tables show single variables, but if we want to explore two variables together we look at **cross-tabulation** tables. A cross-tabulation table shows the number of observations for each combination of the row and column variables.\n\nLet's again examine bonus eligibility, but this time across levels of central air. Again, we can use the `table` function. The `prop.table` function allows us to compare two variables in terms of proportions instead of frequencies.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(train$Central_Air, train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n       0    1\n  N  142    5\n  Y 1069  835\n```\n:::\n\n```{.r .cell-code}\nprop.table(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n              0           1\n  N 0.069234520 0.002437835\n  Y 0.521209166 0.407118479\n```\n:::\n\n```{.r .cell-code}\nggplot(data = train) +\n  geom_bar(mapping = aes(x = Bonus, fill = Central_Air))\n```\n\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nFrom the above output we can see that 147 homes have no central air with only 5 of them being bonus eligible. However, there are 1904 homes that have central air with 835 of them being bonus eligible. For an even more detailed breakdown we can use the `CrossTable` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gmodels)\n\nCrossTable(train$Central_Air, train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  2051 \n\n \n                  | train$Bonus \ntrain$Central_Air |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                N |       142 |         5 |       147 | \n                  |    35.112 |    50.620 |           | \n                  |     0.966 |     0.034 |     0.072 | \n                  |     0.117 |     0.006 |           | \n                  |     0.069 |     0.002 |           | \n------------------|-----------|-----------|-----------|\n                Y |      1069 |       835 |      1904 | \n                  |     2.711 |     3.908 |           | \n                  |     0.561 |     0.439 |     0.928 | \n                  |     0.883 |     0.994 |           | \n                  |     0.521 |     0.407 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |      1211 |       840 |      2051 | \n                  |     0.590 |     0.410 |           | \n------------------|-----------|-----------|-----------|\n\n \n```\n:::\n:::\n\n\n\nThe advantage of the `CrossTable` function is that we can easily get not only the frequencies, but the cell, row, and column proportions. For example, the third number in each cell gives us the row proportion. For homes without central air, 96.6% of them are not bonus eligible, while 3.4% of them are. For homes with central air, 56.1% of the homes are not bonus eligible, while 43.9% of them are. This would appear that the distribution of bonus eligible homes changes across levels of central air - a relationship between the two variables. This expected relationship needs to be tested statistically for verification.\n\n## Tests of Association\n\nMuch like in Chapter @sec-slr we have statistical tests to evaluate relationships between two categorical variables. The null hypothesis for these statistical tests is that the two variables have no association - the distribution of one variable does not change across levels of another variable. The alternative hypothesis is an association between the two variables - the distribution of one variable changes across levels of another variable.\n\nThese statistical tests follow a $\\chi^2$-distribution. The $\\chi^2$-distribution is a distribution that has the following characteristics:\n\n<ul>\n\n<li>\n\nBounded below by 0\n\n<li>\n\nRight-skewed\n\n<li>\n\nOne set of degrees of freedom\n\n</ul>\n\nA plot of a variety of $\\chi^2$-distributions is shown here:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n\nTwo common $\\chi^2$ tests are the Pearson and Likelihood Ratio $\\chi^2$ tests. They compare the observed count of observations in each cell of a cross-tabulation table between two variables to their expected count **if** there was no relationship. The expected cell count applies the overall distribution of one variable across all the levels of the other variable. For example, overall 59% of all homes are not bonus eligible. **If** that were to apply to every level of central air, then the 140 homes without central air would be expected to have 86.73 ( \\$ = 147 \\times 0.59 \\$ ) of them would be bonus eligible while 60.27 ( \\$ = 147 \\times 0.41\\$ ) of them would not be bonus eligible. We actually observe 142 and 5 homes for each of these categories respectively. The further the observed data is from the expected data, the more evidence we have that there is a relationship between the two variables.\n\nThe test statistic for the Pearson $\\chi^2$ test is the following:\n\n$$\n\\chi^2_P = \\sum_{i=1}^R \\sum_{j=1}^C \\frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}\n$$ From the equation above, the closer that the observed count of each cross-tabulation table cell to the expected count, the smaller the test statistic. As with all previous hypothesis tests, the smaller the test statistic, the larger the p-value, implying less evidence for the alternative hypothesis.\n\nLet's examine the relationship between central air and bonus eligibility using the `chisq.test` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(train$Central_Air, train$Bonus)\nX-squared = 90.686, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\n\nThe above results shows an extremely small p-value that is below any reasonable significance level. This implies that we have statistical evidence for a relationship between having central air and bonus eligibility of homes. The p-value comes from a $\\chi^2$-distribution with degrees of freedom that equal the product of the number of rows minus one and the number of columns minus one.\n\nAnother common test is the Likelihood Ratio test. The test statistic for this is the following:\n\n$$\n\\chi^2_L = 2 \\times \\sum_{i=1}^R \\sum_{j=1}^C Obs_{i,j} \\times \\log(\\frac{Obs_{i,j}}{Exp_{i,j}})\n$$\n\nThe p-value comes from a $\\chi^2$-distribution with degrees of freedom that equal the product of the number of rows minus one and the number of columns minus one. Both of the above tests have a sample size requirement. The sample size requirement is 80% or more of the cells in the cross-tabulation table need **expected** count larger than 5.\n\nFor smaller sample sizes, this might be hard to meet. In those situations, we can use a more computationally expensive test called Fisher's exact test. This test calculates every possible permutation of the data being evaluated to calculate the p-value without any distributional assumptions. To perform this test we can use the `fisher.test` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.test(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  table(train$Central_Air, train$Bonus)\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  9.213525 69.646380\nsample estimates:\nodds ratio \n  22.16545 \n```\n:::\n:::\n\n\n\nWe see the same results as with the Pearson test because the assumptions were met for sample size.\n\nBoth the Pearson and Likelihood Ratio $\\chi^2$ tests can handle any type of categorical variable either ordinal, nominal, or both. However, ordinal variables provide us extra information since the order of the categories actually matters compared to nominal. We can test for even more with ordinal variables against other ordinal variables whether two ordinal variables have a **linear relationship** as compared to just a general one. An ordinal test for association is the Mantel-Haenszel $\\chi^2$ test. The test statistic for the Mantel-Haenszel $\\chi^2$ test is the following:\n\n$$\n\\chi^2_{MH} = (n-1)r^2\n$$ where $r^2$ is the Pearson correlation between the column and row variables. This test follows a $\\chi^2$-distribution with only one degree of freedom.\n\nSince both the central air and bonus eligibility variables are binary, they are ordinal. Since they are both ordinal, we should use the Mantel-Haenszel $\\chi^2$ test with the `CMHtest` function. In the main output table, the first row is the Mantel-Haenszel $\\chi^2$ test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vcdExtra)\n\nCMHtest(table(train$Central_Air, train$Bonus))$table[1,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Chisq           Df         Prob \n9.230619e+01 1.000000e+00 7.425180e-22 \n```\n:::\n:::\n\n\n\nFrom here we can see another extremely small p-value as we saw in earlier, more general $\\chi^2$ tests.\n\n## Measures of Association\n\nTests of association are best designed for just that, testing the existence of an association between two categorical variables. However, just like we saw in Chapter @sec-eda, hypothesis tests are impacted by sample size. When we have the same sample size, tests of association can rank significance of variables with p-values. However, when sample sizes are not the same between two tests, the tests of association are not best for comparing the strength of an association. In those scenarios, we have measures of strength of association that can be compared across any sample size.\n\nMeasures of association were not designed to test if an association exists, as that is what statistical testing is for. They are designed to measure the strength of association. There are dozens of these measures. Three of the most common are the following: - Odds Ratios (only for comparing two binary variables) - Cramer's V (able to compare nominal variables with any number of categories) - Spearman's Correlation (able to compare ordinal variables with any number of categories)\n\nAn **odds ratio** indicates how much more likely, with respect to **odds**, a certain event occurs in one group relative to its occurrence in another group. The odds of an event occurring is *not* the same as the probability that an event occurs. The odds of an event occurring is the probability the event occurs divided by the probability that event does not occur.\n\n$$\nOdds = \\frac{p}{1-p}\n$$\n\nLet's again examine the cross-tabulation table between central air and bonus eligibility.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  2051 \n\n \n                  | train$Bonus \ntrain$Central_Air |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                N |       142 |         5 |       147 | \n                  |    35.112 |    50.620 |           | \n                  |     0.966 |     0.034 |     0.072 | \n                  |     0.117 |     0.006 |           | \n                  |     0.069 |     0.002 |           | \n------------------|-----------|-----------|-----------|\n                Y |      1069 |       835 |      1904 | \n                  |     2.711 |     3.908 |           | \n                  |     0.561 |     0.439 |     0.928 | \n                  |     0.883 |     0.994 |           | \n                  |     0.521 |     0.407 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |      1211 |       840 |      2051 | \n                  |     0.590 |     0.410 |           | \n------------------|-----------|-----------|-----------|\n\n \n```\n:::\n:::\n\n\n\nLet's look at the row without central air. The probability that a home without central air is not bonus eligible is 96.6%. That implies that the odds of not being bonus eligible in homes without central air is 28.41 (= 0.966/0.034). For homes with central air, the odds of not being bonus eligible are 1.28 (= 0.561/0.439). The odds ratio between these two would be approximately 22.2 (= 28.41/1.28). In other words, homes without central air are 22.2 times more likely (in terms of odds) to not be bonus eligible as compared to homes with central air. This relationship is intuitive based on the numbers we have seen. Without going into details, it can also be shown that homes with central air are 22.2 times as likely (in terms of odds) to be bonus eligible.\n\nWe can use the `OddsRatio` function to get these same results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\n\nOddsRatio(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 22.18335\n```\n:::\n:::\n\n\n\n**Cramer's V** is another measure of strength of association. Cramer's V is calculated as follows:\n\n$$\nV = \\sqrt{\\frac{\\chi^2_P/n}{\\min(Rows-1, Columns-1)}}\n$$\n\nCramer's V is bounded between 0 and 1 for every comparison other than two binary variables. For two binary variables being compared the bounds are -1 to 1. The idea is still the same for both. The further the value is from 0, the stronger the relationship. Unfortunately, unlike $R^2$, Cramer's V has no interpretative value. It can only be used for comparison.\n\nWe use the `assocstats` function to get the Cramer's V value. This function also provides the Pearson and Likelihood Ratio $\\chi^2$ tests as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassocstats(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     X^2 df P(> X^2)\nLikelihood Ratio 121.499  1        0\nPearson           92.351  1        0\n\nPhi-Coefficient   : 0.212 \nContingency Coeff.: 0.208 \nCramer's V        : 0.212 \n```\n:::\n:::\n\n\n\nLastly, we have Spearman's correlation. Much like the Mantel-Haenszel test of association was specifically designed for comparing two ordinal variables, Spearman correlation measures the strength of association between two ordinal variables. Spearman is not limited to only categorical data analysis as it was also seen back in Chapter @sec-diag with detecting heteroskedasticity. Remember, Spearman correlation is a correlation on the ranks of the observations as compared to the actual values of the observations.\n\nThe `cor.test` function that gave us Pearson's correlation also provides Spearman's correlation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(x = as.numeric(ordered(train$Central_Air)), \n         y = as.numeric(ordered(train$Bonus)), \n         method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  x and y\nS = 1132826666, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2121966 \n```\n:::\n:::\n\n\n\nAs previously mentioned, these are only a few of the dozens of different measures of association that exist. However, they are the most used ones.\n\n## Introduction to Logistic Regression\n\nAfter exploring the categorical target variable, we can move on to modeling the categorical target variable. Logistic regression is a fundamental statistical analysis for data science and analytics. It part of a class of modeling techniques known as classification models since they are trying to predict categorical target variables. This target variable can be binary, ordinal, or even nominal in its structure. The primary focus will be binary logistic regression. It is the most common type of logistic regression, and sets up the foundation for both ordinal and nominal logistic regression.\n\nOrdinary least squares regression is not the best approach to modeling categorical target variables. Mathematically, it can be shown that with a binary target variable coded as 0 and 1, an OLS linear regression model will produce the **linear probability model**.\n\n### Linear Probability Model\n\nThe linear probability model is not as widely used since probabilities do not tend to follow the properties of linearity in relation to their predictors. Also, the linear probability model possibly produces predictions outside of the bounds of 0 and 1 (where probabilities should be!). For completeness sake however, here is the linear probability model using the `lm` function to try and predict bonus eligibility.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlp.model <- lm(Bonus ~ Gr_Liv_Area, data = train)\n\nwith(train, plot(x = Gr_Liv_Area, y = Bonus,\n               main = 'OLS Regression?',\n               xlab = 'Greater Living Area (Sqft)',\n               ylab = 'Bonus Eligibility'))\nabline(lp.model)\n```\n\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nEven though it doesn't appear to really look like our data, let's fit this linear probability model anyway for completeness sake.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlp.model <- lm(Bonus ~ Gr_Liv_Area, data = train)\n\nsummary(lp.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Bonus ~ Gr_Liv_Area, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.70766 -0.29160 -0.09983  0.39432  0.86198 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.149e-01  2.776e-02  -14.94   <2e-16 ***\nGr_Liv_Area  5.534e-04  1.765e-05   31.36   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4044 on 2049 degrees of freedom\nMultiple R-squared:  0.3243,\tAdjusted R-squared:  0.324 \nF-statistic: 983.6 on 1 and 2049 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nqqnorm(rstandard(lp.model),\n       ylab = \"Standardized Residuals\",\n       xlab = \"Normal Scores\",\n       main = \"QQ-Plot of Residuals\")\nqqline(rstandard(lp.model))\n```\n\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-17-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot(predict(lp.model), resid(lp.model), \n     ylab=\"Residuals\", xlab=\"Predicted Values\", \n     main=\"Residuals of Linear Probability Model\") \nabline(0, 0) \n```\n\n::: {.cell-output-display}\n![](07-categorical_files/figure-pdf/unnamed-chunk-17-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nAs we can see from the charts above, the assumptions of ordinary least squares don't really hold in this situation. Therefore, we should be careful interpreting the results of the model. Maybe a better model won't have these problems?\n\n### Binary Logistic Regression\n\nDue to the limitations of the linear probability model, people typically just use the binary logistic regression model. The logistic regression model does not have the limitations of the linear probability model. The outcome of the logistic regression model is the probability of getting a 1 in a binary variable. That probability is calculated as follows:\n\n$$\np_i = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i})}}\n$$\n\nThis function has the desired properties for predicting probabilities. The predicted probability from the above equation will always be between 0 and 1. The parameter estimates do not enter the function linearly (this is a non-linear regression model), and the rate of change of the probability varies as the predictor variables vary as seen in Figure @fig-logistic.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Example of a Logistic Curve](img/logistic.png){#fig-logistic fig-align='center' width=50%}\n:::\n:::\n\n\n\nTo create a linear model, a **link function** is applied to the probabilities. The specific link function for logistic regression is called the **logit** function.\n\n$$\nlogit(p_i) = \\log(\\frac{p_i}{1-p_i}) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i}\n$$\n\nThe relationship between the predictor variables and the logits are linear in nature as the logits themselves are unbounded. This structure looks much more like our linear regression model structure. However, logistic regression does not use OLS to estimate the coefficients in our model. OLS requires residuals which the logistic regression model does not provide. The target variable is binary in nature, but the predictions are probabilities. Therefore, we cannot calculate a traditional residual. Instead, logistic regression uses maximum likelihood estimation. This is not covered here.\n\nThere are two main assumptions for logistic regression:\n\n1.  Independence of observations\n2.  Linearity of the logit\n\nThe first assumption of independence is the same as we had for linear regression. The second assumption implies that the logistic function transformation (the logit) actually makes a linear relationship with our predictor variables. This assumption can be tested, but will not be covered in this brief introduction to logistic regression.\n\nLet's build a logistic regression model. We will use the `glm` function to do this. The `glm` function has a similar structure to the `lm` function. The main difference is the `family = binomial(link = \"logit\")` option to specify that we are uses a logistic regression model. Again, there are many different link functions, but only the logistic link function (the logit) is being used here.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_logit <- glm(Bonus ~ Gr_Liv_Area, \n                   data = train, family = binomial(link = \"logit\"))\nsummary(ames_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Bonus ~ Gr_Liv_Area, family = binomial(link = \"logit\"), \n    data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.5796  -0.6942  -0.3647   0.8060   2.1857  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -6.1348858  0.2757473  -22.25   <2e-16 ***\nGr_Liv_Area  0.0038463  0.0001799   21.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1926.4  on 2049  degrees of freedom\nAIC: 1930.4\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\n\nLet's examine the above output. Scanning down the output, you can see the actual logistic regression equation for the variable `Gr_Liv_Area`. Here we can see that it appears to be a significant variable at predicting bonus eligibility. However, the coefficient reported does not have the same usable interpretation as in linear regression. An increase of one unit of greater living area square footage is linearly related to the logit **not** the probability of bonus eligibility. We can transform this coefficient to make it more interpretable. A single unit increase in greater living area square footage **does** have a $100 \\times (e^\\hat{\\beta}-1)\\%$ increase in the average **odds** of bonus eligibility. We can use a combination of the `exp` and `coef` functions to obtain this number.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n100*(exp(cbind(coef(ames_logit), confint(ames_logit)))-1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                              2.5 %      97.5 %\n(Intercept) -99.7834027 -99.8755103 -99.6328865\nGr_Liv_Area   0.3853699   0.3508132   0.4216532\n```\n:::\n:::\n\n\n\nIn other words, every additional square foot in greater living area in the home leads to an average increase in odds of 0.385% to be bonus eligible.\n\n### Adding Categorical Variables\n\nSimilar to linear regression as we learned in Chapter @sec-mlr, logistic regression can have both continuous and categorical predictors for our categorical target variable. Let's add both central air as well as number of fireplaces to our logistic regression model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_logit2 <- glm(Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), \n                  data = train, family = binomial(link = \"logit\"))\nsummary(ames_logit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), \n    family = binomial(link = \"logit\"), data = train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.4340  -0.6467  -0.2822   0.6610   2.8866  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -9.970e+00  6.549e-01 -15.223  < 2e-16 ***\nGr_Liv_Area          3.759e-03  2.031e-04  18.506  < 2e-16 ***\nCentral_AirY         3.564e+00  5.310e-01   6.711 1.93e-11 ***\nfactor(Fireplaces)1  9.822e-01  1.253e-01   7.837 4.60e-15 ***\nfactor(Fireplaces)2  6.734e-01  2.406e-01   2.799  0.00513 ** \nfactor(Fireplaces)3 -3.993e-02  8.711e-01  -0.046  0.96344    \nfactor(Fireplaces)4  9.025e+00  3.247e+02   0.028  0.97783    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1746.2  on 2044  degrees of freedom\nAIC: 1760.2\n\nNumber of Fisher Scoring iterations: 11\n```\n:::\n:::\n\n\n\nJust like with linear regression, categorical predictor variables are a comparison between two categories. Again, the coefficients from the logistic regression model need to be transformed to be interpreted.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n100*(exp(cbind(coef(ames_logit2), confint(ames_logit2)))-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                         2.5 %        97.5 %\n(Intercept)         -9.999532e+01  -99.9988238   -99.9843742\nGr_Liv_Area          3.766413e-01    0.3376191     0.4175879\nCentral_AirY         3.429115e+03 1264.8396217 11177.2021480\nfactor(Fireplaces)1  1.670410e+02  108.9856565   241.6360910\nfactor(Fireplaces)2  9.607987e+01   22.4764491   214.9571145\nfactor(Fireplaces)3 -3.914247e+00  -81.9507819   497.4825425\nfactor(Fireplaces)4  8.308750e+05 -100.0000000            NA\n```\n:::\n:::\n\n\n\nLet's use the first fireplace variable as an example. A home with one fireplace has, on average, 167.04% higher odds of being bonus eligible as compared to a home with zero fireplaces.\n\n### Model Assessment\n\nThere are dozens of different ways to evaluate a logistic regression model. We will cover one popular way here - concordance. Counting the number of **concordant**, **discordant**, and **tied** pairs is a way to to assess how well the model fits the data.\n\nTo find concordant, discordant, and tied pairs, we must compare all of the 0's in the target variable to all of the 1's. For our example, we will compare every pair of homes where one home is bonus eligible and one is not (every 0 and 1 pair). A **concordant** pair is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has a higher predicted probability than the non-bonus eligible home (the 0 in our model) - our model successfully ordered these two observations by probability. It does not matter what the actual predicted probability values are as long as the bonus eligible home has a higher predicted probability than the non-bonus eligible home. A **discordant** pair is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has a lower predicted probability than the non-bonus eligible home (the 0 in our model) - our model unsuccessfully ordered the homes. It does not matter what the actual predicted probability values are as long as the bonus eligible home has a lower predicted probability than the non-bonus eligible home. A **tied** pair is a 0 and 1 pair where the bonus eligible home has the same predicted probability as the non-bonus eligible home - the model is confused and sees these two different things as the same. In general, you want a high percentage of concordant pairs and low percentages of discordant and tied pairs.\n\nWe can use the `Concordance` function to obtain these values on our predictions from the `predict` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(InformationValue)\n\nConcordance(train$Bonus, predict(ames_logit, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$Concordance\n[1] 0.862933\n\n$Discordance\n[1] 0.137067\n\n$Tied\n[1] -5.551115e-17\n\n$Pairs\n[1] 1017240\n```\n:::\n:::\n\n\n\nFrom the above output we have a concordance of 86.3% for our model. There is no good or bad value as this can only be compared with another model to see which is better. Let's compare this to our model with the categorical variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nConcordance(train$Bonus, predict(ames_logit2, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$Concordance\n[1] 0.8839182\n\n$Discordance\n[1] 0.1160818\n\n$Tied\n[1] 4.163336e-17\n\n$Pairs\n[1] 1017240\n```\n:::\n:::\n\n\n\nWe can see that the model with categorical predictors added to it has a higher concordance at 88.4%. That implies that our model is correctly able to rank our observations 88.4% of the time. This is **NOT** the same thing as saying our model is 88.4% accurate. Accuracy (which is not covered here) deals with a prediction being correct or incorrect. Concordance is only measuring how often we are able to predict 1's with higher probability than 0's - again, correctly ranking the observations.\n\n### Variable Selection and Regularized Regression\n\nAs with linear regression in Chapters @sec-mlr and @sec-sel, logistic regression uses the same approaches to doing variable selection. In fact, the same function are used as well. Let's use the `step` function to apply a forward and backward selection to the logistic regression model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_sel_log <- train %>% \n  dplyr::select(Bonus, \n         Lot_Area,\n         Street,\n         Bldg_Type,\n         House_Style,\n         Overall_Qual,\n         Roof_Style,\n         Central_Air,\n         First_Flr_SF,\n         Second_Flr_SF,\n         Full_Bath,\n         Half_Bath,\n         Fireplaces,\n         Garage_Area,\n         Gr_Liv_Area, \n         TotRms_AbvGrd) %>%\n  replace(is.na(.), 0)\n\nfull.model <- glm(Bonus ~ . , data = train_sel_log)\n\nempty.model <- glm(Bonus ~ 1, data = train_sel_log)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor.model <- step(empty.model,\n                  scope = list(lower = formula(empty.model),\n                               upper = formula(full.model)),\n                  direction = \"forward\", k = log(dim(train_sel_log)[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=2918.59\nBonus ~ 1\n\n                Df Deviance    AIC\n+ Overall_Qual   9   234.74 1453.0\n+ Full_Bath      1   320.51 2030.8\n+ Gr_Liv_Area    1   335.11 2122.1\n+ Garage_Area    1   367.62 2312.0\n+ First_Flr_SF   1   396.15 2465.3\n+ Fireplaces     1   415.26 2561.9\n+ TotRms_AbvGrd  1   422.53 2597.6\n+ House_Style    7   444.22 2745.9\n+ Half_Bath      1   459.40 2769.1\n+ Second_Flr_SF  1   461.26 2777.4\n+ Central_Air    1   473.64 2831.7\n+ Lot_Area       1   478.42 2852.3\n+ Bldg_Type      4   477.21 2870.0\n<none>               495.97 2918.6\n+ Street         1   495.86 2925.8\n+ Roof_Style     5   491.40 2937.7\n\nStep:  AIC=1452.99\nBonus ~ Overall_Qual\n\n                Df Deviance    AIC\n+ Full_Bath      1   203.87 1171.4\n+ Gr_Liv_Area    1   208.62 1218.7\n+ First_Flr_SF   1   218.70 1315.5\n+ Garage_Area    1   223.10 1356.3\n+ TotRms_AbvGrd  1   223.12 1356.5\n+ Fireplaces     1   223.33 1358.5\n+ Lot_Area       1   225.07 1374.3\n+ Bldg_Type      4   227.31 1417.6\n+ Second_Flr_SF  1   230.98 1427.5\n+ Half_Bath      1   231.81 1434.8\n+ Central_Air    1   233.55 1450.2\n<none>               234.74 1453.0\n+ Street         1   234.30 1456.8\n+ House_Style    7   230.28 1467.1\n+ Roof_Style     5   234.18 1486.2\n\nStep:  AIC=1171.42\nBonus ~ Overall_Qual + Full_Bath\n\n                Df Deviance    AIC\n+ Fireplaces     1   193.70 1074.1\n+ First_Flr_SF   1   195.11 1089.0\n+ Gr_Liv_Area    1   196.26 1101.0\n+ Lot_Area       1   197.22 1111.0\n+ Bldg_Type      4   195.24 1113.2\n+ Garage_Area    1   197.63 1115.3\n+ Half_Bath      1   201.31 1153.1\n+ TotRms_AbvGrd  1   202.51 1165.3\n+ Central_Air    1   202.61 1166.4\n<none>               203.87 1171.4\n+ Street         1   203.38 1174.2\n+ Second_Flr_SF  1   203.85 1178.9\n+ House_Style    7   201.34 1199.2\n+ Roof_Style     5   203.40 1204.8\n\nStep:  AIC=1074.14\nBonus ~ Overall_Qual + Full_Bath + Fireplaces\n\n                Df Deviance    AIC\n+ Garage_Area    1   188.96 1030.9\n+ First_Flr_SF   1   189.34 1035.0\n+ Bldg_Type      4   187.45 1037.3\n+ Lot_Area       1   190.10 1043.3\n+ Gr_Liv_Area    1   190.64 1049.1\n+ Half_Bath      1   192.04 1064.0\n<none>               193.70 1074.1\n+ Central_Air    1   193.02 1074.5\n+ Street         1   193.35 1078.0\n+ TotRms_AbvGrd  1   193.42 1078.7\n+ Second_Flr_SF  1   193.69 1081.7\n+ House_Style    7   190.38 1092.0\n+ Roof_Style     5   193.16 1106.5\n\nStep:  AIC=1030.9\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area\n\n                Df Deviance    AIC\n+ Bldg_Type      4   183.72 1003.8\n+ First_Flr_SF   1   186.24 1008.8\n+ Lot_Area       1   186.46 1011.2\n+ Gr_Liv_Area    1   186.87 1015.7\n+ Half_Bath      1   187.55 1023.2\n<none>               188.96 1030.9\n+ Street         1   188.68 1035.5\n+ Central_Air    1   188.76 1036.3\n+ TotRms_AbvGrd  1   188.78 1036.6\n+ Second_Flr_SF  1   188.96 1038.5\n+ House_Style    7   186.42 1056.5\n+ Roof_Style     5   188.20 1060.8\n\nStep:  AIC=1003.78\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type\n\n                Df Deviance     AIC\n+ First_Flr_SF   1   181.17  982.68\n+ Half_Bath      1   181.99  991.99\n+ Gr_Liv_Area    1   182.00  992.09\n+ Lot_Area       1   182.09  993.12\n<none>               183.72 1003.78\n+ Street         1   183.36 1007.35\n+ Central_Air    1   183.60 1009.98\n+ TotRms_AbvGrd  1   183.66 1010.71\n+ Second_Flr_SF  1   183.72 1011.35\n+ House_Style    7   180.41 1019.87\n+ Roof_Style     5   182.90 1032.69\n\nStep:  AIC=982.68\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type + First_Flr_SF\n\n                Df Deviance     AIC\n+ Half_Bath      1   177.46  947.94\n+ Second_Flr_SF  1   180.10  978.18\n+ Lot_Area       1   180.24  979.73\n+ Gr_Liv_Area    1   180.28  980.18\n<none>               181.17  982.68\n+ Street         1   180.77  985.80\n+ House_Style    7   176.95  987.73\n+ Central_Air    1   181.06  989.10\n+ TotRms_AbvGrd  1   181.16  990.19\n+ Roof_Style     5   179.76 1004.77\n\nStep:  AIC=947.94\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type + First_Flr_SF + Half_Bath\n\n                Df Deviance    AIC\n+ Lot_Area       1   176.75 947.31\n<none>               177.46 947.94\n+ TotRms_AbvGrd  1   177.03 950.49\n+ Street         1   177.09 951.18\n+ Central_Air    1   177.43 955.17\n+ Gr_Liv_Area    1   177.46 955.45\n+ Second_Flr_SF  1   177.46 955.56\n+ Roof_Style     5   175.95 968.54\n+ House_Style    7   174.78 970.01\n\nStep:  AIC=947.31\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type + First_Flr_SF + Half_Bath + Lot_Area\n\n                Df Deviance    AIC\n<none>               176.75 947.31\n+ TotRms_AbvGrd  1   176.32 949.96\n+ Street         1   176.54 952.49\n+ Central_Air    1   176.72 954.56\n+ Gr_Liv_Area    1   176.73 954.71\n+ Second_Flr_SF  1   176.75 954.89\n+ Roof_Style     5   175.25 968.01\n+ House_Style    7   174.09 969.62\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nback.model <- step(full.model,\n                   scope = list(lower = formula(empty.model),\n                                upper = formula(full.model)),\n                   direction = \"backward\", k = log(dim(train_sel_log)[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=1016.59\nBonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + \n    Roof_Style + Central_Air + First_Flr_SF + Second_Flr_SF + \n    Full_Bath + Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + \n    TotRms_AbvGrd\n\n                Df Deviance     AIC\n- House_Style    7   174.31  995.05\n- Roof_Style     5   173.24  997.67\n- Gr_Liv_Area    1   171.63 1009.07\n- Central_Air    1   171.64 1009.12\n- Second_Flr_SF  1   171.71 1009.95\n- First_Flr_SF   1   171.84 1011.48\n- Street         1   171.89 1012.09\n- TotRms_AbvGrd  1   171.94 1012.72\n- Lot_Area       1   172.08 1014.43\n<none>               171.62 1016.59\n- Garage_Area    1   172.50 1019.38\n- Half_Bath      1   173.37 1029.70\n- Fireplaces     1   173.90 1035.95\n- Bldg_Type      4   177.07 1050.13\n- Full_Bath      1   187.25 1187.64\n- Overall_Qual   9   217.74 1436.04\n\nStep:  AIC=995.05\nBonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + Roof_Style + \n    Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath + \n    Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd\n\n                Df Deviance     AIC\n- Roof_Style     5   175.81  974.50\n- Central_Air    1   174.35  987.91\n- Gr_Liv_Area    1   174.43  988.84\n- Second_Flr_SF  1   174.49  989.53\n- Street         1   174.53  989.99\n- TotRms_AbvGrd  1   174.74  992.45\n- Lot_Area       1   174.80  993.19\n- First_Flr_SF   1   174.89  994.22\n<none>               174.31  995.05\n- Garage_Area    1   175.57 1002.23\n- Fireplaces     1   176.38 1011.68\n- Half_Bath      1   176.86 1017.24\n- Bldg_Type      4   179.33 1022.77\n- Full_Bath      1   191.76 1183.08\n- Overall_Qual   9   222.96 1431.28\n\nStep:  AIC=974.5\nBonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + Central_Air + \n    First_Flr_SF + Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + \n    Garage_Area + Gr_Liv_Area + TotRms_AbvGrd\n\n                Df Deviance     AIC\n- Central_Air    1   175.85  967.29\n- Gr_Liv_Area    1   175.91  968.02\n- Second_Flr_SF  1   175.97  968.79\n- Street         1   176.01  969.19\n- TotRms_AbvGrd  1   176.30  972.55\n- Lot_Area       1   176.31  972.65\n- First_Flr_SF   1   176.31  972.66\n<none>               175.81  974.50\n- Garage_Area    1   177.04  981.15\n- Fireplaces     1   177.86  990.64\n- Half_Bath      1   178.19  994.42\n- Bldg_Type      4   180.75 1000.81\n- Full_Bath      1   193.59 1164.44\n- Overall_Qual   9   224.35 1405.92\n\nStep:  AIC=967.29\nBonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + First_Flr_SF + \n    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + \n    Gr_Liv_Area + TotRms_AbvGrd\n\n                Df Deviance     AIC\n- Gr_Liv_Area    1   175.95  960.84\n- Second_Flr_SF  1   176.01  961.59\n- Street         1   176.03  961.83\n- TotRms_AbvGrd  1   176.34  965.38\n- First_Flr_SF   1   176.35  965.52\n- Lot_Area       1   176.35  965.52\n<none>               175.85  967.29\n- Garage_Area    1   177.19  975.25\n- Fireplaces     1   177.95  984.00\n- Half_Bath      1   178.34  988.55\n- Bldg_Type      4   180.88  994.63\n- Full_Bath      1   193.82 1159.28\n- Overall_Qual   9   224.36 1398.33\n\nStep:  AIC=960.84\nBonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + First_Flr_SF + \n    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + \n    TotRms_AbvGrd\n\n                Df Deviance     AIC\n- Street         1   176.13  955.40\n- Second_Flr_SF  1   176.14  955.45\n- Lot_Area       1   176.44  959.00\n- TotRms_AbvGrd  1   176.54  960.09\n<none>               175.95  960.84\n- Garage_Area    1   177.32  969.16\n- Fireplaces     1   178.04  977.46\n- Half_Bath      1   178.50  982.75\n- Bldg_Type      4   180.94  987.72\n- First_Flr_SF   1   179.15  990.17\n- Full_Bath      1   193.95 1153.02\n- Overall_Qual   9   224.57 1392.65\n\nStep:  AIC=955.4\nBonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + \n    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + \n    TotRms_AbvGrd\n\n                Df Deviance     AIC\n- Second_Flr_SF  1   176.32  949.96\n- TotRms_AbvGrd  1   176.75  954.89\n- Lot_Area       1   176.78  955.30\n<none>               176.13  955.40\n- Garage_Area    1   177.53  963.91\n- Fireplaces     1   178.25  972.22\n- Half_Bath      1   178.71  977.53\n- Bldg_Type      4   181.03  981.12\n- First_Flr_SF   1   179.28  984.03\n- Full_Bath      1   194.12 1147.16\n- Overall_Qual   9   224.74 1386.53\n\nStep:  AIC=949.96\nBonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + \n    Full_Bath + Half_Bath + Fireplaces + Garage_Area + TotRms_AbvGrd\n\n                Df Deviance     AIC\n- TotRms_AbvGrd  1   176.75  947.31\n<none>               176.32  949.96\n- Lot_Area       1   177.03  950.49\n- Garage_Area    1   177.78  959.22\n- Fireplaces     1   178.72  970.00\n- Bldg_Type      4   181.35  977.15\n- Half_Bath      1   180.23  987.27\n- First_Flr_SF   1   180.28  987.88\n- Full_Bath      1   198.37 1183.99\n- Overall_Qual   9   225.34 1384.40\n\nStep:  AIC=947.31\nBonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + \n    Full_Bath + Half_Bath + Fireplaces + Garage_Area\n\n               Df Deviance     AIC\n<none>              176.75  947.31\n- Lot_Area      1   177.46  947.94\n- Garage_Area   1   178.30  957.58\n- Fireplaces    1   178.97  965.27\n- Bldg_Type     4   181.72  973.64\n- Half_Bath     1   180.24  979.73\n- First_Flr_SF  1   180.34  980.88\n- Full_Bath     1   199.82 1191.34\n- Overall_Qual  9   225.73 1380.31\n```\n:::\n:::\n\n\n\nIn the above two approaches we used the BIC selection criteria. Here both forward and backward selection actually picked the same model. Let's check the concordance of this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nConcordance(train$Bonus, predict(back.model, type = \"response\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$Concordance\n[1] 0.9612668\n\n$Discordance\n[1] 0.03873324\n\n$Tied\n[1] -2.081668e-17\n\n$Pairs\n[1] 1017240\n```\n:::\n:::\n\n\n\nNot surprisingly, this model outperforms the previous model that we had with a concordance of 96.1%.\n\nAlthough not covered in detail here, regularized regression can also be applied to logistic regression to get a different view. This might be helpful with the multicollinearity present in these predictor variables. Again, we can use the `glmnet` function with the addition of a `family = \"binomial\"` option.\n",
    "supporting": [
      "07-categorical_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}