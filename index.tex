% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Statistical Foundations},
  pdfauthor={IAA Faculty},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Statistical Foundations}
\author{IAA Faculty}
\date{3/2/23}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, frame hidden, boxrule=0pt, breakable, borderline west={3pt}{0pt}{shadecolor}, sharp corners, interior hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{statistical-foundations}{%
\chapter{Statistical Foundations}\label{statistical-foundations}}

\hypertarget{authors}{%
\subsection*{Authors}\label{authors}}
\addcontentsline{toc}{subsection}{Authors}

Aric Labarr, Phd - Institute for Advanced Analytics @ NC State\\
Susan Simmons, PhD - Institute for Advanced Analytics @ NC State\\

with special thanks to the contributions from Dr.~Shaina Race

\hypertarget{structure-of-the-book}{%
\subsection*{Structure of the book}\label{structure-of-the-book}}
\addcontentsline{toc}{subsection}{Structure of the book}

The book is broken down into small sections that aim to demonstrate a
single concept at a time. The companion text,
\href{https://andreavillanes.github.io/R_Data_Manipulation_Techniques/}{Data
Manipulation Techniques with R} introduces the foundations of
programming in R. This book is a work in progress. Submit any issues
\href{https://https://github.com/IAA-Faculty/statistical_foundations/issues}{here}.
Please check back frequently for updates.

\hypertarget{acknowledgements}{%
\subsection*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{subsection}{Acknowledgements}

The authors would like to thank the members of the faculty and the TAs
at the Institute for Advanced Analytics for providing feedback on this
work.

\bookmarksetup{startatroot}

\hypertarget{sec-intro-stat}{%
\chapter{Introduction to Statistics}\label{sec-intro-stat}}

Welcome to your introduction to statistics. You will be learning the
basics of statistics, along with applications of statistics within the R
language. This book will provide fundamentals of the concepts and the
code to apply these concepts in R.\\
This Chapter aims to answer the following questions:

What type of data is being analyzed?

Nominal

Ordinal

Continuous/Discrete

How do we describe distributions of these variables?

Center

Spread

Shape

Graphical Display

How do we create confidence intervals for parameters?

How do we perform hypothesis testing?

One sample t-test

Two sample t-test

Testing Normality

Testing Equality of Variances

Testing Equality of Means

Mann-Whitney-Wilcoxon Test

The following packages will be used in this textbook. Below we install
and add the packages to our libraries so that any version issues can be
dealt with at the beginning of the course. Sometimes packages require
that you update to the latest version of R; if you see an error that
indicates that situation, download the latest version of R from
\href{https://cran.r-project.org}{CRAN} and install it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}AmesHousing\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}tidyverse\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}car\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}DescTools\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}corrplot\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}mosaic\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}modelr\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}plotly\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}ggplot2\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}Hmisc\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}onehot\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}jmuOutlier\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}leaps\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}glmnet\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}nortest\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}lmtest\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}InformationValue\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}gmodels\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}vcdExtra\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}TSA\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}carData\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}epiDisplay\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}gridExtra\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AmesHousing)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{library}\NormalTok{(corrplot)}
\FunctionTok{library}\NormalTok{(mosaic)}
\FunctionTok{library}\NormalTok{(modelr)}
\FunctionTok{library}\NormalTok{(plotly)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(Hmisc)}
\FunctionTok{library}\NormalTok{(onehot)}
\FunctionTok{library}\NormalTok{(jmuOutlier)}
\FunctionTok{library}\NormalTok{(leaps)}
\FunctionTok{library}\NormalTok{(glmnet)}
\FunctionTok{library}\NormalTok{(nortest)}
\FunctionTok{library}\NormalTok{(lmtest)}
\FunctionTok{library}\NormalTok{(InformationValue)}
\FunctionTok{library}\NormalTok{(gmodels)}
\FunctionTok{library}\NormalTok{(vcdExtra)}
\FunctionTok{library}\NormalTok{(TSA)}
\FunctionTok{library}\NormalTok{(carData)}
\FunctionTok{library}\NormalTok{(epiDisplay)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-eda}{%
\section{Exploratory Data Analysis (EDA)}\label{sec-eda}}

The crucial first step to any data science problem is exploratory data
analysis (EDA). Before you attempt to run any models, or jump towards
any formal statistical analysis, you must \textbf{\emph{explore your
data}}. Many unexpected frustrations arise when exploratory analysis is
overlooked; knowing your data is critical to your ability to make
necessary assumptions about it. This preliminary analysis will help
inform our decisions for data manipulation, give us a base-level
understanding of our variables and the relationships between them, and
help determine which statistical analyses might be appropriate for the
questions we are trying to answer. Some of the questions we aim to
answer through exploratory analysis are:

What kind of variables to you have?

Continuous

Nominal

Ordinal

How are the attributes stored?

Strings

Integers

Floats/Numeric

Dates

What do their distributions look like?

Center/Location

Spread

Shape

Are there any anomolies?

Outliers

Leverage points

Missing values

Low-frequency categories

Throughout the textbook, we will continue to use a real-estate data set
that contains the \texttt{sale\_price} and numerous physical attributes
of nearly 3,000 homes in Ames, Iowa in the early 2000s. To access this
data, we first add the \texttt{AmesHousing} package to our library and
create the nicely formatted data with the \texttt{make\_ordinal\_ames()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AmesHousing)}

\NormalTok{ames }\OtherTok{\textless{}{-}} \FunctionTok{make\_ordinal\_ames}\NormalTok{() }
\FunctionTok{str}\NormalTok{(ames)}
\end{Highlighting}
\end{Shaded}

\hypertarget{vartypes}{%
\subsection{Types of Variables}\label{vartypes}}

The columns of a data set are referred to by the following
\textbf{equivalent terms}:

Variables

Features

Attributes

Predictors/Targets

Factors

Inputs/Outputs

This book may use any of these words interchangeably to refer to a
quality or quantity of interest in our data.

\hypertarget{nominal-variables}{%
\subsubsection*{Nominal Variables}\label{nominal-variables}}
\addcontentsline{toc}{subsubsection}{Nominal Variables}

A \textbf{nominal} or \textbf{categorical variable} is a \emph{quality
of interest} whose values have no logical ordering. Color (``blue'',
``red'', ``green''\ldots), ethnicity (``African-American'', ``Asian'',
``Caucasian'',\ldots), and style of house (``ranch'', ``two-story'',
``duplex'', \ldots) are all examples of nominal attributes. The
categories or values that these variables can take on - those words
listed in quotes and parenthesis - are called the \textbf{levels} of the
variable.

In modeling, nominal attributes are commonly transformed into
\textbf{dummy variables}. Dummy variables are binary columns that
indicate the presence or absence of a quality. There is more than one
way to create dummy variables, and the treatment will be different
depending on what type of model you are using. Linear regression models
will use either \textbf{reference-level} or \textbf{effects coding},
whereas other machine learning models are more likely to use one-hot
encoding or a variation thereof.

\textbf{One-hot encoding}

For machine learning applications, it is common to create a binary dummy
column for each level of your categorical variable. This is the most
intuitive representation of categorical information, answering
indicative questions for each level of the variable: \emph{``is it
blue?''}, \emph{``is it red?''} etc. The table below gives an example of
some data, the original nominal variable (color) and the one-hot encoded
color information.

\hypertarget{tbl-onehot}{}
\begin{longtable}[]{@{}llllll@{}}
\caption{\label{tbl-onehot}One-hot dummy variable coding for the
categorical attribute \emph{color}}\tabularnewline
\toprule()
\endhead
Observation & Color & Blue & Red & Yellow & Other \\
1 & Blue & 1 & 0 & 0 & 0 \\
2 & Yellow & 0 & 0 & 1 & 0 \\
3 & Blue & 1 & 0 & 0 & 0 \\
4 & Red & 0 & 1 & 0 & 0 \\
5 & Red & 0 & 1 & 0 & 0 \\
6 & Blue & 1 & 0 & 0 & 0 \\
7 & Yellow & 0 & 0 & 1 & 0 \\
8 & Other & 0 & 0 & 0 & 1 \\
\bottomrule()
\end{longtable}

We will demonstrate the creation of this data using some simple random
categorical data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{41}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{2}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{),}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{0}\NormalTok{)),}
                \AttributeTok{x1 =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{), }\AttributeTok{each =} \DecValTok{10}\NormalTok{)),}
                \AttributeTok{x2 =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Z"}\NormalTok{, }\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{,}\StringTok{"W"}\NormalTok{,}\StringTok{"V"}\NormalTok{,}\StringTok{"U"}\NormalTok{), }\AttributeTok{each =} \DecValTok{5}\NormalTok{)))}
\FunctionTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            y x1 x2
1   1.2056317  A  Z
2   2.1972575  A  Z
3   3.0017043  A  Z
4   3.2888254  A  Z
5   2.9057534  A  Z
6   2.4936675  A  X
7   2.5992858  A  X
8   0.4203930  A  X
9   3.0006207  A  X
10  4.1880077  A  X
11 -0.2093244  B  Y
12  0.4126881  B  Y
13  2.0561206  B  Y
14  0.6834151  B  Y
15  0.9454590  B  Y
16  1.3297513  B  W
17  1.6630951  B  W
18  1.8783282  B  W
19  1.2028743  B  W
20  3.2744025  B  W
21 -0.8992970  C  V
22  2.1394903  C  V
23 -1.1659510  C  V
24 -0.0471304  C  V
25  0.4158763  C  V
26  1.7200805  C  U
27 -0.7843607  C  U
28 -1.3039296  C  U
29 -0.4520359  C  U
30 -1.7739919  C  U
\end{verbatim}

Unlike reference and effects coding, which are typically specified
within the \texttt{lm()} function as we will see in Chapter
Chapter~\ref{sec-slr}, one-hot encoding is most quickly achieved through
use of the \texttt{onehot} package in R, which first creates an
``encoder'' to do the job quickly.

The speed of this function has been tested against both the base R
\texttt{model.matrix()} function and the \texttt{dummyVars()} function
in the \texttt{caret} package and is \emph{substantially} faster than
either.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(onehot)}

\NormalTok{encoder  }\OtherTok{=} \FunctionTok{onehot}\NormalTok{(dat)}
\NormalTok{dummies }\OtherTok{=} \FunctionTok{predict}\NormalTok{(encoder,dat)}
\FunctionTok{head}\NormalTok{(dummies)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            y x1=A x1=B x1=C x2=U x2=V x2=W x2=X x2=Y x2=Z
[1,] 1.205632    1    0    0    0    0    0    0    0    1
[2,] 2.197258    1    0    0    0    0    0    0    0    1
[3,] 3.001704    1    0    0    0    0    0    0    0    1
[4,] 3.288825    1    0    0    0    0    0    0    0    1
[5,] 2.905753    1    0    0    0    0    0    0    0    1
[6,] 2.493667    1    0    0    0    0    0    1    0    0
\end{verbatim}

\textbf{Reference-level coding}

Reference-level coding is similar to one-hot encoding except one of the
levels of the attribute, called the \textbf{reference level}, is
omitted. Notice that the 4 dummy columns from Table
Table~\ref{tbl-onehot} collectively form a linearly dependent set; that
is, if you know the values of 3 of the 4 dummy variables you can
determine the \(4^{th}\) with complete certainty. This would be a
problem for linear regression, where we assume our input attributes are
not linearly dependent as we will discuss in Chapter
Chapter~\ref{sec-slr}.

A reference level of the attribute is often specified by the user to be
a particular level worthy of comparison (a baseline), as the estimates
in the regression output will be interpreted in a way that compares each
non-reference level to the reference level. If a reference level is not
specified by the user, one will be picked by the software by default
either using the order in which the levels were encountered in the data,
or their alphabetical ordering. Users should check the documentation of
the associated function to understand what to expect.

Table Table~\ref{tbl-refcoding} transforms the one-hot encoding from
Table Table~\ref{tbl-onehot} into reference-level coding with the color
``blue'' as the reference level. Notice the absence of the column
indicating ``blue'' and how each blue observation exists as a row of
zeros.

\hypertarget{tbl-refcoding}{}
\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tbl-refcoding}Reference-level dummy variable coding for
the categorical attribute \emph{color} and the reference level of
``blue''}\tabularnewline
\toprule()
\endhead
Observation & Color & Red & Yellow & Other \\
1 & Blue & 0 & 0 & 0 \\
2 & Yellow & 0 & 1 & 0 \\
3 & Blue & 0 & 0 & 0 \\
4 & Red & 1 & 0 & 0 \\
5 & Red & 1 & 0 & 0 \\
6 & Blue & 0 & 0 & 0 \\
7 & Yellow & 0 & 1 & 0 \\
8 & Other & 0 & 0 & 1 \\
\bottomrule()
\end{longtable}

\textbf{Effects coding}

Effects coding is useful for obtaining a more general comparative
interpretation when you have approximately equal sample sizes across
each level of your categorical attribute. Effects coding is designed to
allow the user to compare each level to \emph{all} of the other levels.
More specifically the mean of each level is compared to the
\textbf{overall mean} of your data. However, the comparison is actually
to the so-called \emph{grand mean}, which is the mean of the means of
each group. When sample sizes are equal, the grand mean and the overall
sample mean are equivalent. When sample sizes are \emph{not} equal, the
parameter estimates for effects coding should not be used for
interpretation or explanation.

Effects coding still requires a \emph{reference level}, however the
purpose of the reference level is not the same as it was in
reference-level coding. Here, the reference level is left out in the
sense that no comparison is made between it and the overall mean. Table
Table~\ref{tbl-effcoding} shows our same example with effects coding.
Again we notice the absence of the column indicating ``blue'' but now
the reference level receives values of \texttt{-1} rather than
\texttt{0} for all 3 dummy columns. We will revisit the interpretation
of linear regression coefficients under this coding scheme in Chapter
Chapter~\ref{sec-slr}.

\hypertarget{tbl-effcoding}{}
\begin{longtable}[]{@{}lllll@{}}
\caption{\label{tbl-effcoding}Effects coding for the categorical
attribute \emph{color} and the reference level of
``blue''}\tabularnewline
\toprule()
\endhead
Observation & Color & Red & Yellow & Other \\
1 & Blue & -1 & -1 & -1 \\
2 & Yellow & 0 & 1 & 0 \\
3 & Blue & -1 & -1 & -1 \\
4 & Red & 1 & 0 & 0 \\
5 & Red & 1 & 0 & 0 \\
6 & Blue & -1 & -1 & -1 \\
7 & Yellow & 0 & 1 & 0 \\
8 & Other & 0 & 0 & 1 \\
\bottomrule()
\end{longtable}

\hypertarget{interval-variables}{%
\subsubsection*{Interval Variables}\label{interval-variables}}
\addcontentsline{toc}{subsubsection}{Interval Variables}

An interval variable is a \emph{quantity of interest} on which the
mathematical operations of addition, subtraction, multiplication and
division can be performed. Time, temperature and age are all examples of
interval attributes. To illustrate the definition, note that ``15
minutes'' divided by ``5 minutes'' is 3, which indicates that 15 minutes
is 3 times as long as 5 minutes. The sensible interpretation of this
simple arithmetic sentence demonstrates the nature of interval
attributes. One should note that such arithmetic would not make sense in
the treatment of nominal variables.

\hypertarget{ordinal-variables}{%
\subsubsection*{Ordinal Variables}\label{ordinal-variables}}
\addcontentsline{toc}{subsubsection}{Ordinal Variables}

\textbf{Ordinal variables} are attributes that are qualitative in nature
but have some natural ordering. \emph{Level of education} is a common
example, with a level of `PhD' indicating \emph{more} education than
`Bachelors' but lacking a numerical framework to quantify \emph{how much
more}. The treatment of ordinal variables will depend on the
application. Survey responses on a Likert scale are also ordinal - a
response of 4=``somewhat agree'' on a 1-to-5 scale of agreement cannot
reliably be said to be twice as enthusiastic as a response of
2=``somewhat disagree''. These are not interval measurements, though
they are often treated as such in a trade-off for computational
efficiency.

\textbf{Ordinal variables will either be given some numeric value and
treated as interval variables or they will be treated as categorical
variables and dummy variables will be created. The choice of solution is
up to the analyst.} When numeric values are assigned to ordinal
variables, the possibilities are many. For example, consider \emph{level
of education}. The simplest ordinal treatment for such an attribute
might be something like Table Table~\ref{tbl-educationint}.

\hypertarget{tbl-educationint}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-educationint}One potential approach to scaling the
ordinal attribute level of education}\tabularnewline
\toprule()
\endhead
Level of Education & Numeric Value \\
No H.S. Diploma & 1 \\
H.S. Diploma or GED & 2 \\
Associates or Certificate & 3 \\
Bachelors & 4 \\
Graduate Certificate & 5 \\
Masters & 6 \\
PhD & 7 \\
\bottomrule()
\end{longtable}

While numeric values have been assigned and this data \emph{could} be
used like an interval attribute, it's important to realize that the
notion of a ``one-unit increase'' is qualitative in nature rather than
quantitative. However, if we're interested in learning whether there is
a \emph{linear} type of relationship between education and another
attribute (meaning as education level increases, the value of another
attribute increases or decreases), this would be the path to get us
there. However we're making an assumption in this model that the
difference between a H.S. Diploma and an Associates degree (a difference
of ``1 unit'') is the same as the difference between a Master's degree
and a PhD (also a difference of ``1 unit''). These types of assumptions
can be flawed, and it is often desirable to develop an alternative
system of measurement based either on domain expertise or the target
variable of interest. This is the notion behind \textbf{optimal scaling}
and \textbf{target-level encoding}.

\textbf{Optimal Scaling}

The primary idea behind optimal scaling is to transform an ordinal
attribute into an interval one in a way that doesn't restrict the
numeric values to simply the integers \(1,2,3, \dots\). It's reasonable
for a data scientist to use domain expertise to develop an alternative
scheme.

For example, if analyzing movie theater concessions with ordinal drink
sizes \{small, medium, large\}, one is not restricted to the numeric
valuation of 1=small, 2=medium, and 3=large just because it's an ordinal
variable with 3 levels. Perhaps it would make more sense to use the
drink size in fluid ounces to represent the ordinality. If the small
drink is 12 ounces, the medium is 20 ounces, and the large is 48 ounces,
then using those values as the numerical representation would be every
bit as (if not more) reasonable than using the standard integers 1, 2,
and 3.

If we re-consider the ordinal attribute level of education, we might
decide to represent the approximate years of post-secondary schooling
required to obtain a given level. This might lead us to something like
the attribute values in Table Table~\ref{tbl-educationscaled}

\hypertarget{tbl-educationscaled}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-educationscaled}One potential approach to scaling
the ordinal attribute level of education}\tabularnewline
\toprule()
\endhead
Level of Education & Numeric Value \\
No H.S. Diploma & -1 \\
H.S. Diploma or GED & 0 \\
Associate's or Certificate & 2 \\
Bachelor's & 4 \\
Graduate Certificate & 5 \\
Master's & 6 \\
PhD & 8 \\
\bottomrule()
\end{longtable}

If we were modeling the effect of education on something like salary, it
seems reasonable to assume that the jumps between levels should not have
equal distance like they did in Table~\ref{tbl-educationint}. It seems
reasonable to assume that one would experience a larger salary lift from
Associate's to Bachelor's degree than they would from No H.S. Diploma to
GED. The most common way to determine the numeric values for categories
is to use information from the response variable. This is commonly
referred to as \textbf{target level encoding}.

\textbf{Target Level Encoding}

The values in Table Table~\ref{tbl-educationscaled} might have struck
the reader as logical but arbitrary. To be more scientific about the
determination of those numeric values, one might wish to use information
from the response variable to obtain a more precise expected change in
salary for each level increase in education. At first hearing this, one
might question the validity of the technique; isn't the goal to
\emph{predict} salary? This line of thought is natural, which is why
having a holdout sample is extremely important in this situation. To
implement Target level encoding, we can simply create a look-up table
that matches each level of education to the average or median salary
obtained for that level. These values can be used just as readily as the
arbitrary levels created in Table Table~\ref{tbl-educationscaled} to
encode the ordinal attribute!

\hypertarget{sec-dist}{%
\subsection{Distributions}\label{sec-dist}}

After reviewing the types and formats of the data inputs, we move on to
some basic \textbf{univariate} (one variable at a time) analysis. We
start by describing the distribution of values that each variable takes
on. For nominal variables, this amounts to frequency tables and bar
charts of how often each level of the variable appears in the data set.

We'll begin by exploring one of our nominal features,
\texttt{Heating\_QC} which categorizes the quality and condition of a
home's heating system. To create plots in R, we will use the popular
\texttt{ggplot2} library. At the same time, we will load the
\texttt{tidyverse} library which we will use in the next chunk of code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ames) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Heating\_QC),}\AttributeTok{fill=}\StringTok{"orange"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Heating System Quality"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Frequency"}\NormalTok{,}\AttributeTok{title=}\StringTok{"Bar Graph of Heating System"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/barchart-1.pdf}

}

\caption{Distribution of Nominal Variable Heating\_QC}

\end{figure}

To summon the same information in tabular form, we can use the
\texttt{count()} function to create a table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(Heating\_QC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  Heating_QC     n
  <ord>      <int>
1 Poor           3
2 Fair          92
3 Typical      864
4 Good         476
5 Excellent   1495
\end{verbatim}

You'll notice that very few houses (3) have heating systems in
\texttt{Poor} condition, and the majority of houses have systems rated
\texttt{Excellent}. \textbf{It will likely make sense to combine the
categories of \texttt{Fair} and \texttt{Poor} in our eventual analysis,
a decision we will later revisit.}

Next we create a \textbf{histogram} for an interval attribute like
\texttt{Sale\_Price}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ames) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{),}\AttributeTok{fill=}\StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Frequency"}\NormalTok{,}\AttributeTok{title=}\StringTok{"Histogram of Sales Price in Thousands of Dollars"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-histogram-1.pdf}

}

\caption{\label{fig-histogram}Distribution of Interval Variable
Sale\_Price}

\end{figure}

From this initial inspection, we can conclude that most of the houses
sell for less than \$200,000 and there are a number of expensive
anomalies. To more concretely describe and quantify a statistical
distribution, we use statistics that describe the \emph{location,
spread, and shape} of the data.

\hypertarget{location}{%
\subsubsection*{Location}\label{location}}
\addcontentsline{toc}{subsubsection}{Location}

The \emph{location} is a measure of central tendency for a distribution.
Most common measures of central tendency are \textbf{mean},
\textbf{median}, and \textbf{mode}.

We define each of these terms below for a variable \(\mathbf{x}\) having
n observations with values \(\{x_i\}_{i=1}^n\), sorted in order of
magnitude such that \(x_1 \leq x_2 \leq \dots \leq x_n\):

Mean: The \textbf{average} of the observations,
\(\bar{\mathbf{x}}= \frac{1}{n}\sum_{i=1}^n x_i\)

Median: The ``middle value'' of the data. Formally, when \(n\) is odd,
the median is the observation value, \(x_m = x_{\frac{(n+1)}{2}}\) for
which \(x_i < x_m\) for 50\% of the observations (excluding \(x_m\)).
When \(n\) is even, \(x_m\) is the average of \(x_\frac{n}{2}\) and
\(x_{(\frac{n}{2}+1)}\). The median is also known as the \(2^{nd}\)
\textbf{quartile} (see next section).

Mode: The most commonly occurring value in the data. Most commonly used
to describe nominal attributes.

\textbf{Example}

The following table contains the heights of 10 students randomly sampled
from NC State's campus. Compute the mean, median, mode and quartiles of
this variable.

\begin{longtable}[]{@{}ccccccccccc@{}}
\toprule()
\endhead
height & 60 & 62 & 63 & 65 & 67 & 67 & 67 & 68 & 68 & 69 \\
\bottomrule()
\end{longtable}

\textbf{Solution:}

The mean is \texttt{(60+62+63+65+67+67+67+68+68+69)/10} = 65.6.

The median (second quartile) is \texttt{(67+67)/2} = 67.

The mode is 67.

\hypertarget{spread}{%
\subsubsection*{Spread}\label{spread}}
\addcontentsline{toc}{subsubsection}{Spread}

Once we have an understanding of the central tendency of a data set, we
move on to describing the spread (the dispersion or variation) of the
data. \textbf{Range}, \textbf{interquartile range}, \textbf{variance},
and \textbf{standard deviation} are all statistics that describe spread.

Range: The difference between the maximum and minimum data values.

Sample variance: The sum of squared differences between each data point
and the mean, divided by (n-1).
\(\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2\)

Standard deviation: The square root of the sample variance.\\

In order to define \textbf{interquartile range}, we must first define
\textbf{percentiles} and \textbf{quartiles}.

Percentiles: The 99 intermediate values of the data which divide the
observations into 100 equally-sized groups. The \(r^{th}\) percentile of
the data, \(P_{r}\) is the number for which \(r\)\% of the data is less
than \(P_{r}\).

Quartiles: The quartiles of the data are the \(25^{th}\), \(50^{th}\)
and \(75^{th}\) percentiles. They are denoted as \(Q_{1}\) (\(1^{st}\)
quartile), \(Q_{2}\) (\(2^{nd}\) quartile = median) and \(Q_{3}\)
(\(3^{rd}\) quartile), respectively.

Interquartile range (IQR): The difference between the \(25^{th}\) and
\(75^{th}\) percentiles.

One should note that standard deviation is more frequently reported than
variance because it shares the same units as the original data, and
because of the guidance provided by the empirical rule. If we're
exploring something like \texttt{Sale\_Price}, which has the unit
``dollars'', then the variance would be measured in ``square-dollars'',
which hampers the intuition. Standard deviation, on the other hand,
would share the unit ``dollars'', aiding our fundamental understanding.

\textbf{Example} Let's again use the table of heights from the previous
example, this time computing the range, IQR, sample variance and
standard deviation.

\begin{longtable}[]{@{}ccccccccccc@{}}
\toprule()
\endhead
height & 60 & 62 & 63 & 65 & 67 & 67 & 67 & 68 & 68 & 69 \\
\bottomrule()
\end{longtable}

\textbf{Solution:}

The range \texttt{69-60} = 9.

The variance is
\texttt{((60-65.6)\^{}2+(62-65.6)\^{}2+(63-65.6)\^{}2+(65-65.6)\^{}2+(67-65.6)\^{}2+(67-65.6)\^{}2+(67-65.6)\^{}2+(68-65.6)\^{}2+(68-65.6)\^{}2+(69-65.6)\^{}2)/9}
= 8.933

The standard deviation is \texttt{sqrt(8.933)} = 2.989

The first quartile is \texttt{(62+63)/2} = 62.5

The third quartile is \texttt{(68+68)/2} = 68

The IQR is \texttt{68\ -\ 62.5} = 5.5.

\hypertarget{shape}{%
\subsubsection*{Shape}\label{shape}}
\addcontentsline{toc}{subsubsection}{Shape}

The final description we will want to give to distributions regards
their shape. Is the histogram \emph{symmetric}? Is it \emph{unimodal}
(having a single large ``heap'' of data) or \emph{multimodal} (having
multiple heaps'')? Does it have a longer tail on one side than the other
(\emph{skew})? Is there a lot more or less data in the tails than you
might expect?

We'll formalize these ideas with some illustrations. A distribution is
right (left) skewed if it has a longer tail on its right (left) side, as
shown in Figure Figure~\ref{fig-skewdiagram}.

\begin{figure}

{\centering \includegraphics{./img/skewdiagrams.png}

}

\caption{\label{fig-skewdiagram}Examples of Left-Skewed (Negative Skew)
and Right-skewed (Positive Skew) distributions respectively}

\end{figure}

A distribution is called \emph{bimodal} if it has two ``heaps'', as
shown in Figure Figure~\ref{fig-bimodal}.

\begin{figure}

{\centering \includegraphics{./img/bimodal.png}

}

\caption{\label{fig-bimodal}Example of a Bimodal Distribution}

\end{figure}

\hypertarget{summary-functions-in-r}{%
\subsubsection*{Summary Functions in R}\label{summary-functions-in-r}}
\addcontentsline{toc}{subsubsection}{Summary Functions in R}

There are many ways to obtain all of the statistics described in the
preceding sections, below we highlight 3:

The \texttt{describe} function from the \texttt{Hmisc} package which can
work on the entire dataset or a subset of columns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ames$Sale_Price 
       n  missing distinct     Info     Mean      Gmd      .05      .10 
    2930        0     1032        1   180796    81960    87500   105450 
     .25      .50      .75      .90      .95 
  129500   160000   213500   281242   335000 

lowest :  12789  13100  34900  35000  35311, highest: 611657 615000 625000 745000 755000
\end{verbatim}

The tidyverse \texttt{summarise} function, in this case obtaining
statistics for each \texttt{Exter\_Qual} separately.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Exter\_Qual) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{average =} \FunctionTok{mean}\NormalTok{(Sale\_Price), }\AttributeTok{st.dev =} \FunctionTok{sd}\NormalTok{(Sale\_Price), }\AttributeTok{maximum =} \FunctionTok{max}\NormalTok{(Sale\_Price), }\AttributeTok{minimum =} \FunctionTok{min}\NormalTok{(Sale\_Price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
  Exter_Qual average  st.dev maximum minimum
  <ord>        <dbl>   <dbl>   <int>   <int>
1 Fair        89924.  38014.  200000   13100
2 Typical    143374.  41504.  415000   12789
3 Good       230756.  70411.  745000   52000
4 Excellent  377919. 106988.  755000  160000
\end{verbatim}

The base R \texttt{summary} function, which can work on the entire
dataset or an individual variable

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  12789  129500  160000  180796  213500  755000 
\end{verbatim}

\hypertarget{normal}{%
\subsection{The Normal Distribution}\label{normal}}

The normal distribution, also known as the Gaussian distribution, is one
of the most fundamental concepts in statistics. It is one that arises
naturally out of many applications and settings. The normal distribution
has the following characteristics:

Symmetric

Fully defined by mean and standard deviation (equivalently, variance)

Bell-shaped/Unimodal

Mean = Median = Mode

Assymptotic to the x-axis (theoretical bounds are \(-\infty\) to
\(\infty\))

Much of the normal distributions utility can be summarized in the
\textbf{empirical rule}, which states that:

\(\approx\) 68\% of data in normal distribution lies within 1 standard
deviation of the mean.

\(\approx\) 95\% of data in normal distribution lies within 2 standard
deviations of the mean.

\(\approx\) 99.7\% of data in normal distribution lies within 3 standard
deviations of the mean.

We can thus conclude that observations found outside of 3 standard
deviations from the mean are quite rare, expected less than 1\% of the
time.

\hypertarget{skew}{%
\subsection{Skewness}\label{skew}}

Skewness is a statistic that describes the symmetry (or lack thereof) of
a distribution. A normal distribution is perfectly symmetric and has a
skewness of 0. Distributions that are more right skewed will have
positive values of skewness whereas distributions that are more left
skewed will have negative values of skewness.

\hypertarget{kurt}{%
\subsection{Kurtosis}\label{kurt}}

Kurtosis is a statistic that describes the \emph{tailedness} of a
distribution. The normal distribution has a kurtosis of 3. Distributions
that are more tailed (leptokurtic or heavy-tailed) will have kurtosis
values greater than 3 whereas distributions that are more less tailed
(platykurtic or thin-tailed) will have values of kurtosis less than 3.
For this reason, kurtosis is often reported in the form of \emph{excess
kurtosis} which is the raw kurtosis value minus 3. This is meant as a
comparison to the normal distribution so that positive values indicate
thicker tails and negative values indicate thinner tails than the
normal.

In Figure Figure~\ref{fig-kurtosis} below, we compare classical examples
of leptokurtic and platykurtic distributions to a normal distribution
with the same mean and variance.

\begin{figure}

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-kurtosis-1.pdf}

}

\caption{\label{fig-kurtosis}The Laplace distribution (top left) is
leptokurtic because it has more data in its tails than the normal
distribution with the same mean and variance. The uniform distribution
(top right) is platykurtic because it has less data in its tails than
the normal distribution with the same mean and variance (it effectively
has no tails).}

\end{figure}

\hypertarget{graphdist}{%
\subsection{Graphical Displays of Distributions}\label{graphdist}}

There are three types of plots for examining the distribution of your
data values:

Histograms

Normal Probability Plots (QQ-plots)

Box Plots

\hypertarget{histograms}{%
\subsubsection*{Histograms}\label{histograms}}
\addcontentsline{toc}{subsubsection}{Histograms}

A histogram shows the shape of a univariate distribution. Each bar in
the histogram represents a group of values (a \emph{bin}). The height of
the bar represents the either the frequency of or the percent of values
in the bin. The width and number of bins is determined automatically,
but the user can adjust them to see more or less detail in the
histogram. Figure Figure~\ref{fig-histogram} demonstrated a histogram of
sale price. Sometimes it's nice to overlay a continuous approximation to
the underlying distribution using a \emph{kernal density estimator} with
the \texttt{geom\_density} plot, demonstrated in Figure
Figure~\ref{fig-histwithkernal}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ames,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{..density..), }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_density}\NormalTok{( }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.
i Please use `after_stat(density)` instead.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-histwithkernal-1.pdf}

}

\caption{\label{fig-histwithkernal}Histogram of Sale\_Price with kernal
density estimator}

\end{figure}

In our next example, Figure Figure~\ref{fig-overhistogramAC}, we'll
complicate the previous example by showing \emph{two} distributions of
sale price, one for each level of the binary variable
\texttt{Central\_Air}, overlaid on the same axes.

We can immediately see that there are many more houses that have central
air than do not in this data. It appears as though the two distributions
have different locations, with the purple distribution centered at a
larger sale price. To normalize that quantity and compare the raw
probability densities, we can change our axes to density as in Figure
Figure~\ref{fig-overhistogramAC}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ames,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(ames,Central\_Air }\SpecialCharTok{==} \StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{),}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Central\_Air), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(ames,Central\_Air }\SpecialCharTok{==} \StringTok{\textquotesingle{}N\textquotesingle{}}\NormalTok{),}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Central\_Air), }\AttributeTok{alpha  =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{)  }\SpecialCharTok{+} \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{name=}\StringTok{"Central\_Air"}\NormalTok{,}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{),}\AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"No"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-overhistogramAC-1.pdf}

}

\caption{\label{fig-overhistogramAC}Histogram: Frequency of Sale\_Price
for Each value of Central\_Air}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ames,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(ames,Central\_Air }\SpecialCharTok{==} \StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{),}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{..density..,}\AttributeTok{fill=}\NormalTok{Central\_Air), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(ames,Central\_Air }\SpecialCharTok{==} \StringTok{\textquotesingle{}N\textquotesingle{}}\NormalTok{),}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{..density..,}\AttributeTok{fill=}\NormalTok{Central\_Air), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-overhistogramdensityAC-1.pdf}

}

\caption{\label{fig-overhistogramdensityAC}Histogram: Density of
Sale\_Price for varying qualities of Central\_Air}

\end{figure}

To ease our differentiation of the histograms even further, we again
employ a kernel density estimator as shown in Figure
Figure~\ref{fig-overhistogramdensitykernelAC}. This is an appealing
alternative to the histogram for continuous data that is assumed to
originate from some smooth underlying distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ames,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(ames,Central\_Air }\SpecialCharTok{==} \StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{),}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Central\_Air), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{data=}\FunctionTok{subset}\NormalTok{(ames,Central\_Air }\SpecialCharTok{==} \StringTok{\textquotesingle{}N\textquotesingle{}}\NormalTok{),}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Central\_Air), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-overhistogramdensitykernelAC-1.pdf}

}

\caption{\label{fig-overhistogramdensitykernelAC}Histogram: Density of
Sale\_Price for varying qualities of Central\_Air}

\end{figure}

\hypertarget{normal-probability-plots-qq-plots}{%
\subsubsection*{Normal probability plots (QQ
Plots)}\label{normal-probability-plots-qq-plots}}
\addcontentsline{toc}{subsubsection}{Normal probability plots (QQ
Plots)}

A \textbf{normal probability plot} graphs the sorted data values against
the values that one would expect if the same number of observations came
from a theoretical normal distribution. The resulting image would look
close to a straight line if the data was generated by a normal
distribution. Strong deviations from a straight line indicate that the
data distribution is not normal.

Figure Figure~\ref{fig-qqplot} shows a QQ plot for \texttt{Sale\_Price},
and we can conclude that the variable is not normally distributed (in
fact it is right skewed).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ames, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-qqplot-1.pdf}

}

\caption{\label{fig-qqplot}QQ-Plot: Quantiles of Sale\_Price
vs.~quantiles of a theoretical normal distribution with same mean and
standard deviation. Conclusion: Sale\_Price is \emph{not} normally
distributed due to a problem with skew.}

\end{figure}

There are two main patterns that we expect to find when examining
QQ-plots:

A quadratic shape, as seen in Figure Figure~\ref{fig-qqplot}. This
pattern indicates a deviation from normality due to skewness to the
data.

An S-shape (or cubic shape), as seen in Figure
Figure~\ref{fig-qqplotKurt}. This pattern indicates deviation from
normality due to kurtosis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{j1 =} \FunctionTok{rlaplace}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample=}\NormalTok{j1)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-qqplotKurt-1.pdf}

}

\caption{\label{fig-qqplotKurt}QQ-Plot: Quantiles of the Laplace
distribution vs.~quantiles of a theoretical normal distribution with
same mean and standard deviation. Conclusion: Data is \emph{not}
normally distributed (in fact it is leptokurtic), due to a problem with
kurtosis.}

\end{figure}

\hypertarget{box-plots}{%
\subsubsection*{Box Plots}\label{box-plots}}
\addcontentsline{toc}{subsubsection}{Box Plots}

Box plots (sometimes called box-and-whisker plots) will not necessarily
tell you about the \emph{shape} of your distribution (for instance a
bimodal distribution could have a similar box plot to a unimodal one),
but it will give you a sense of the distribution's location and spread
and potential skewness.

Many of us have become familiar with the \emph{idea} of a box plot, but
when pressed for the specific steps to create one, we realize our
familiarity fades. The diagram in Figure Figure~\ref{fig-boxplot}) will
remind the reader the precise information conveyed by a box plot.

\begin{figure}

{\centering \includegraphics{./img/boxplot.png}

}

\caption{\label{fig-boxplot}Anatomy of a Box Plot.}

\end{figure}

Figure Figure~\ref{fig-rboxplot} shows the boxplot of
\texttt{Sale\_Price}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ames, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-rboxplot-1.pdf}

}

\caption{\label{fig-rboxplot}Box Plot of Sales Price}

\end{figure}

Furthermore, we might want to compare the boxplots of
\texttt{Sale\_Price} for different levels of a categorical variable,
like \texttt{Central\_Air} as we did with histograms and densities in
Figures Figure~\ref{fig-overhistogramAC} and
Figure~\ref{fig-overhistogramdensityAC}.

The following code achieves this goal in Figure
Figure~\ref{fig-multiboxplotAC}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ames, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{x =}\NormalTok{ Central\_Air, }\AttributeTok{fill =}\NormalTok{ Central\_Air)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Central Air"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Accent"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-multiboxplotAC-1.pdf}

}

\caption{\label{fig-multiboxplotAC}Box Plots of Sale\_Price for each
level of Exter\_Qual}

\end{figure}

\hypertarget{pointest}{%
\section{Point Estimates}\label{pointest}}

All the statistics discussed so far have been \emph{point estimates}.
They are our \emph{best} estimate at what the population parameter might
be, but since we've taken a random sample of data from that population,
there must be some uncertainty surrounding that estimate. In statistics,
our real interest lies in drawing inferences about an entire population
(which we couldn't possibly observe due to time, cost, and/or
feasibility constraints) and our approach is to take a representative
sample and try to understand what it might tell us about the population.

For the remainder of this text, we will assume our sample is
representative of the population. Let's review some common statistical
notation of \emph{population parameters} (the true values we are unable
to observe) and \emph{sample statistics} (those values we calculate
based on our sample)

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4932}}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5068}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Population Parmeter
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Sample Statistic
\end{minipage} \\
\midrule()
\endhead
Mean (\(\mu\)) & Sample Average(\(\bar{x}\)) \\
Variance (\(\sigma^{2}\)) & Sample Variance(\(s^{2}_{x}\)) \\
Standard deviation (\(\sigma^{2}\)) & Sample standard
deviation(\(s_{x}\)) \\
\bottomrule()
\end{longtable}

Calculating point estimates should lead us to a natural question, one
that embodies the field of statistics which aims to quantify
uncertainty: \emph{What's the margin of error for this estimate?} This
will be the subject of interest in the next section.

\hypertarget{sec-ci}{%
\section{Confidence Intervals}\label{sec-ci}}

Let's imagine that we want to calculate the average gas mileage of
American cars on the road today in order to analyze the country's carbon
footprint. It should be clear to the reader that the calculation of the
\emph{population mean} would not be possible. The best we could do is
take a large representative sample and calculate the sample average.
Again, the next question should be: What is the margin of error for this
estimate? If our sample average is 21.1 mpg, could the population mean
reasonably be 21.2 mpg? how about 25 mpg? 42 mpg?

To answer this question, we reach for the notion of \emph{confidence
intervals}. A \textbf{confidence interval} is an interval that we
believe contains the population mean with some degree of confidence. A
confidence interval is associated with a \emph{confidence level}, a
percentage, which indicates the strength of our confidence that the
interval created actually captured the true parameter.

It's an important nuance to remember that the population mean is a fixed
number. The source of randomness in our estimation is our sample. When
we construct a 95\% confidence interval, we are claiming that, upon
repetition of the sampling and interval calculation process, we expect
95\% of our created intervals to contain the population mean.

To obtain a confidence interval for a mean in R, we can use the
\texttt{t.test()} function, as shown below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  ames$Sale_Price
t = 122.5, df = 2929, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 177902.3 183689.9
sample estimates:
mean of x 
 180796.1 
\end{verbatim}

We can gather based on the output that our 95\% confidence interval for
the mean of \texttt{Sale\_Price} is {[}177902.3, 183689.9{]}. This
function also outputs some extra information that relates to hypothesis
testing which we will discuss in Section Section~\ref{sec-hypotest}).
For now, if we only want to pull the output containing the confidence
interval information, we can specify \texttt{\$conf.int} to the object
output from \texttt{t.test}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}\SpecialCharTok{$}\NormalTok{conf.int}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 177902.3 183689.9
attr(,"conf.level")
[1] 0.95
\end{verbatim}

To learn the labels of the various pieces of output, you can list them
with the \texttt{ls()} function, or by saving the output as an object
(below, \texttt{results} is the object that stores the output) and
exploring it in your environment (upper right panel in RStudio):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ls}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "alternative" "conf.int"    "data.name"   "estimate"    "method"     
 [6] "null.value"  "p.value"     "parameter"   "statistic"   "stderr"     
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-hypotest}{%
\section{Hypothesis Testing}\label{sec-hypotest}}

A confidence interval can help us test a hypothesis about the population
mean. A \textbf{hypothesis} is merely a statement that we wish to
investigate scientifically through the process of statistical inference.
In Section Section~\ref{sec-ci} we proposed some potential hypotheses in
the form of questions: If the sample average gas mileage is 21.1, is it
possible that the population mean is 21.2? How about 42? The statistical
\textbf{hypothesis test} can help us answer these questions.

To conduct a hypothesis test, we make an initial assumption. This
initial assumption is called the \textbf{null hypothesis} and typically
denoted as \(H_0\). We then analyze the data and determine whether our
observations are likely, given our assumption of the null hypothesis. If
we determine that our observed data was unlikely \emph{enough} (beyond
some threshold that we set before hand - or beyond a ``reasonable
doubt'' in the justice system) then we \emph{reject} our initial
assumption in favor of the opposite statement, known as the
\textbf{alternative hypothesis} denoted \(H_a\). The threshold or
\textbf{significance level} that we use to determine how much evidence
is required to reject the null hypothesis is a proportion, \(\alpha\),
which specifies how often we're willing to incorrectly reject the null
hypothesis (this means that we are assuming the null hypothesis is
true). Remember, in applied statistics there are no proofs. Every
decision we make comes with some degree of uncertainty. \(\alpha\)
quantifies our allowance for that uncertainty. In statistical textbooks
of years past, \(\alpha = 0.05\) was the norm. Later in this text we
will propose much smaller values for \(\alpha\) depending on your sample
size.

In order to quantify how unlikely it was that we observed a statistic as
extreme or more extreme than we did, we calculate a \textbf{p-value}.
The p-value is the area under the \textbf{null distribution} that
represents the probability that we observed something as extreme or more
extreme than we did (assuming the truth of the null hypothesis). If our
p-value is less than our confidence level, \(\alpha\), we have enough
evidence to reject the null hypothesis in favor of the alternative.

Let's take an example and actually \emph{create} a null distribution.
Suppose we flip a fair coin, having equal probability of landing on
heads or tails. We can actually simulate this experience with code! The
following line of code does just that. Go ahead and run it a few times
until you observe a coin flip of each type.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Heads\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Tails\textquotesingle{}}\NormalTok{), }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Heads"
\end{verbatim}

Now, let's suppose we do that \emph{many} times and count the number of
times we observe one outcome, say \texttt{Heads}. This can be done by
sampling the values directly into a vector. Let \texttt{n} be the number
of coin tosses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n}\OtherTok{=}\DecValTok{100}
\NormalTok{outcomes }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Heads\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Tails\textquotesingle{}}\NormalTok{), n, }\AttributeTok{replace=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

We can count the number of \texttt{Heads} we obtained as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(outcomes}\SpecialCharTok{==}\StringTok{"Heads"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 58
\end{verbatim}

Every time you run the lines of code above you will find a different set
of coin flips and a varying number of \texttt{Heads}; \emph{however} the
number of \texttt{Heads} will revolve around 50, because that is what
we'd expect for a fair coin whose probability of \texttt{Heads} is 50\%
(Indeed, this simulates a draw from a binomial distribution with n=100
and p=0.5; the expected value of that distribution is \(np=50\) and the
variance is \(np(1-p)=25\)).

Thus, if were to do the above experiment thousands of times, we could
map out a distribution of how many \texttt{Heads} one might reasonably
receive by tossing a fair coin 100 times. Let's do that, using a
\texttt{for} loop. Let \texttt{T} be the number of simulated experiments
(each experiment tosses the coin 100 times), and let
\texttt{number\_heads} be a vector that stores the number of heads for
each experiment. We can initialize \texttt{number\_heads} with an empty
vector. Notice that our loop overwrites the coin toss data in each step,
after recording the number of heads.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T}\OtherTok{=}\DecValTok{10000}
\NormalTok{n}\OtherTok{=}\DecValTok{100}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11}\NormalTok{)}
\NormalTok{number\_heads }\OtherTok{=} \FunctionTok{vector}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{outcomes }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Heads\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Tails\textquotesingle{}}\NormalTok{), n, }\AttributeTok{replace=}\NormalTok{T)}
\NormalTok{number\_heads[i] }\OtherTok{=} \FunctionTok{sum}\NormalTok{(outcomes}\SpecialCharTok{==}\StringTok{"Heads"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{df }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(number\_heads)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ number\_heads)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of heads in 100 tosses"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-cointoss-1.pdf}

}

\caption{\label{fig-cointoss}Null Distribution: Number of heads for fair
coin tossed 100 times}

\end{figure}

Figure Figure~\ref{fig-cointoss} represents our null distribution of the
number of heads from a fair coin tossed 100 times. What are the minimum
and maximum values of this observed distribution?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{number\_heads)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  33.00   47.00   50.00   49.99   53.00   71.00 
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example}

Suppose now we obtain a new coin from a friend and our hypothesis is
that it ``feels unfair''. We decide that we want a conservative
signficance level of 0.01 before we accuse our friend of cheating, so we
conduct a hypothesis test. Our null hypothesis must generate a known
distribution to which we can compare. Thus our null hypothesis is that
the coin is fair: \[H_0 = \text{The coin is fair:}\quad P(Heads) = 0.5\]
Our alternative hypothesis is the opposite of this:
\[H_0 = \text{The coin is not fair:}\quad P(Heads) \neq 0.5\] Suppose we
flip the coin 100 times and count 65 heads. How likely is it that we
would have obtained a result as extreme or more extreme than this if the
coin was fair? Here we introduce the notion of a \textbf{two-tailed
hypothesis test}. Since our hypothesis is that the coin is simply
unfair, we want to know how likely it is that we obtained a result
\emph{so different from 50}. This is quantified by the \emph{absolute}
difference between what we observed and what we expected. Thus, when
considering our null distribution, we want to look at the probability
we'd obtain something greater than or equal to 65 (\(=50+15\)) heads,
\emph{or} less than or equal to 35 (\(=50-15\)) heads.

Let's take a look at this graphically through our simulated data:

\begin{verbatim}
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
i Please use `linewidth` instead.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-cointossht-1.pdf}

}

\caption{\label{fig-cointossht}Null Distribution: Number of heads for
fair coin tossed 100 times}

\end{figure}

We can use this simulated distribution to estimate the p-value
associated with obtaining 65 heads (the red area highlighted in Figure
Figure~\ref{fig-cointossht}. We'd simply calculate the proportion of
times we observed values equal to or more extreme than 65 - this is the
very definition of a p-value. In the following line of code,
\texttt{\textbar{}} represents the logical ``or'' operator.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(number\_heads}\SpecialCharTok{\textgreater{}=}\DecValTok{65} \SpecialCharTok{|}\NormalTok{ number\_heads}\SpecialCharTok{\textless{}=}\DecValTok{35}\NormalTok{)}\SpecialCharTok{/}\NormalTok{T}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0041
\end{verbatim}

\textbf{Conclusion}: We said at the outset that we wanted a significance
level of 0.01, or 1\%, for our test before we accused our friend of
cheating. Based on our simulations, there is a 0.4\% chance that we'd
obtain the result we did, or something more extreme, if the coin was
fair. Therefore, we have no choice but to \emph{reject our null
hypothesis in favor of the alternative}. Our friend has some explaining
to do!

Before we move on, we can compare the simulated result we just developed
to one based on a theoretical distribution. This can be done using the
\texttt{prop.test()} function to test a proportion. The formal test
confirms our conclusion.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\DecValTok{65}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
           \AttributeTok{alternative =} \FunctionTok{c}\NormalTok{(}\StringTok{"two.sided"}\NormalTok{),}
           \AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    1-sample proportions test with continuity correction

data:  65 out of 100
X-squared = 8.41, df = 1, p-value = 0.003732
alternative hypothesis: true p is not equal to 0.5
99 percent confidence interval:
 0.5162768 0.7643236
sample estimates:
   p 
0.65 
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The nice thing about a simulation study like the one above is that it
allows the user to explore how changes in the underlying procedure might
affect the outcome. We'll next consider two pieces that of the
simulation study and how they affect the p-value: the \emph{sample size}
(the number of coin flips) and the \emph{effect size} (the observed
deviation from 50\% heads).

What happens if we increase/decrease the number of coin flips in our
experiment, but keep the effect size the same, fixed at 65\% heads? If
we only flipped the coin 10 times, would 6-7 heads be improbable to
witness from a fair coin? If we flipped the coin 1000 times, would 650
heads be \emph{more} or \emph{less} improbable than that same ratio in
10 tosses? In other words, which of these situations would entail a
smaller p-value? We hope that the reader now has some intuition to
answer this question. If not, we encourage them to answer it by altering
the value of \texttt{n} in the simulation code, and seeing how the
changes affect the distribution of the null hypothesis.

What happens if we fix the sample size at 100 tosses and decrease the
effect size from 65 heads to 60 heads? We've already generated the data
to answer this question - our p-value would \emph{increase} because it
would be \emph{more} probable to obtain a smaller effect size from a
fair coin. On the flip side (pun intended) the p-value would
\emph{decrease} for a larger effect size.

\hypertarget{onesample}{%
\subsection{One-Sample T-Test}\label{onesample}}

If we want to test whether the mean of continuous variable is equal to
hypothesized value, we can use the \texttt{t.test()} function. The
following code tests whether the average sale price of homes from Ames,
Iowa over the data time period is \$178,000. For now, we'll use the
classic \(\alpha=0.05\) as our significance level. If we have enough
evidence to reject this null hypothesis, we will conclude that the mean
sale price is significantly different than \$178,000 for a two-tailed
test (the default):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{mu =} \DecValTok{178000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  ames$Sale_Price
t = 1.8945, df = 2929, p-value = 0.05825
alternative hypothesis: true mean is not equal to 178000
95 percent confidence interval:
 177902.3 183689.9
sample estimates:
mean of x 
 180796.1 
\end{verbatim}

Because our \emph{p-value is greater than our alpha} level of 0.05, we
\emph{fail to reject} the null hypothesis. We do not quite have
sufficient evidence to say the mean is different from 178,000.

If we're instead interested in testing whether the \texttt{Sale\_Price}
is \emph{higher} than \$178,000, we can specify this in the
\texttt{alternative=} option.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{mu =} \DecValTok{178000}\NormalTok{, }\AttributeTok{alternative =} \StringTok{\textquotesingle{}greater\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  ames$Sale_Price
t = 1.8945, df = 2929, p-value = 0.02913
alternative hypothesis: true mean is greater than 178000
95 percent confidence interval:
 178367.7      Inf
sample estimates:
mean of x 
 180796.1 
\end{verbatim}

In this second test, we see that we actually \emph{do have enough
evidence} to claim that the true mean is greater than \$178,000 at the
\(\alpha=0.05\) level.

\hypertarget{two-sample-t-tests}{%
\section{Two-Sample t-tests}\label{two-sample-t-tests}}

If we have a hypothesis about a difference in the means of two groups of
observations, a \textbf{two-sample t-test} can tell us whether that
difference is statistically significant. By \emph{statistically
significant}, we mean the observed difference in sample means is greater
than what we would expect to find if the population means were truly
equal. In other words, statistical significance is a phrase that
describes when our p-value falls below our significance level,
\(\alpha\). Typically, the groups of interest are formed by levels of a
binary variable, and the t-test is a way of testing whether there is a
relationship between that binary variable and the continuous variable.

To conduct a two-sample t-test, our data should satisfy 3 fundamental
assumptions:

The observations are independent

The data from each group is normally distributed

The variances for each group are equal

If our data does not satisfy these assumptions, we must adapt our test
to the situation. If the \(3^{rd}\) assumption of equal variances is not
met, we simply add the option \texttt{var.equal=F} to the
\texttt{t.test()} function to use the Welch or Satterthwaite
approximation to degrees of freedom (it's becoming increasingly common
for practitioners to use this option even when variances are equal).

If the \(2^{nd}\) assumption is not met, one must opt for a
\emph{nonparametric} test like the Mann-Whitney-U test (also called the
Mann--Whitney--Wilcoxon or the Wilcoxon rank-sum test).

The \(1^{st}\) assumption is not easily checked unless the data is
generated over time (time-series) and is instead generally implied by
careful data collect and the application of domain expertise.

\hypertarget{sec-testnorm}{%
\subsection{Testing Normality of Groups}\label{sec-testnorm}}

We can test the normality assumption either graphically or through
formal statistical tests. The best graphical test of normality is a
QQ-Plot, though histograms are often used as well. We saw in Figures
Figure~\ref{fig-qqplot} and Figure~\ref{fig-histogram} that the
distribution of \texttt{Sale\_Price} was not perfectly normal, however
the deviations from normality were not egregious. In Figure
Figure~\ref{fig-qqplotcentralair} we again examine the normality of Sale
Price, this time for each value of \texttt{Central\_Air}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ames, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Sale\_Price, }\AttributeTok{color =}\NormalTok{ Central\_Air)) }\SpecialCharTok{+}
     \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+}
     \FunctionTok{stat\_qq\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./01-introduction_statistics_files/figure-pdf/fig-qqplotcentralair-1.pdf}

}

\caption{\label{fig-qqplotcentralair}QQ-Plot: Quantiles of Sale\_Price
vs.~quantiles of a theoretical normal distribution with same mean and
standard deviation, for each level of Central\_Air. Conclusion:
Sale\_Price is \emph{not} normally distributed for Central\_Air=`Yes'
and is \emph{more} normally distributed for Central\_Air = `No'.}

\end{figure}

For formal tests of normality, we most often use the Shapiro-Wilk test,
although many other formal tests exist, each with their own advantages
and disadvantages. All of these tests have the null hypothesis of
normality: \[H_0: \text{ the data is normally distributed}\]
\[H_a: \text{ the data is NOT normally distributed}\] We conduct formal
tests as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price[ames}\SpecialCharTok{$}\NormalTok{Central\_Air}\SpecialCharTok{==}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  ames$Sale_Price[ames$Central_Air == "Y"]
W = 0.86295, p-value < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(ames}\SpecialCharTok{$}\NormalTok{Sale\_Price[ames}\SpecialCharTok{$}\NormalTok{Central\_Air}\SpecialCharTok{==}\StringTok{\textquotesingle{}N\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  ames$Sale_Price[ames$Central_Air == "N"]
W = 0.95315, p-value = 4.739e-06
\end{verbatim}

The formal test rejects the null hypotheses of normality and confirms
what we saw in our visual analysis.

For the sake of illustration, we will proceed in this example under the
assumption of normality (JUST for illustration), though a non-parametric
approach will also be demonstrated later in our analysis in Section
Section~\ref{sec-wilcoxon}.

\hypertarget{sec-ftest}{%
\subsection{Testing Equality of Variances}\label{sec-ftest}}

We can conduct a formal test to confirm that the \(3^{rd}\) assumption
is met. This test for equal variances is known as an F-Test. The null
hypothesis is that the variances are equal, the alternative being that
they are not: \[H_0: \sigma_1^2 = \sigma_2^2\]
\[H_a: \sigma_1^2 \neq \sigma_2^2\]

The F-Test is invoked with the \texttt{var.test} function, which takes
as input a \texttt{formula}. A formula is an R object most often created
using the \texttt{\textasciitilde{}} operator, for example
\texttt{y\ \textasciitilde{}\ x\ +\ z}, interpreted as a specification
that the response \texttt{y} is to be predicted with the inputs
\texttt{x} and \texttt{z}. For our present discussion on two-sample
t-tests, we might think of predicting our continuous attribute with our
binary attribute, provided the means are different between the two
groups.

The following code checks whether the distributions of Sale\_Price for
houses with and without central air have the same variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var.test}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Central\_Air, }\AttributeTok{data =}\NormalTok{ ames)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    F test to compare two variances

data:  Sale_Price by Central_Air
F = 0.2258, num df = 195, denom df = 2733, p-value < 2.2e-16
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
 0.1854873 0.2800271
sample estimates:
ratio of variances 
         0.2257977 
\end{verbatim}

They do not. Thus, we must opt for the \texttt{var.equal=FALSE} option
in the \texttt{t.test()} procedure.

\hypertarget{sec-tsttest}{%
\subsection{Testing Equality of Means}\label{sec-tsttest}}

Assuming the first two assumptions are met, the two-sample t-test is
performed by including a binary grouping variable with into the
\texttt{t.test()} function. Below we test whether the mean sales price
is different for houses with and without \texttt{Central\_Air}, and we
include the \texttt{var.equal=FALSE} option because we determined that
the variances of the two groups were unequal in Section
Section~\ref{sec-ftest}. The null hypothesis for the t-test is that the
means of the two groups are equal: \[H_0: \mu_1 = \mu_2\]
\[H_a: \mu_1 \neq \mu_2\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Central\_Air, }\AttributeTok{data =}\NormalTok{ ames, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  Sale_Price by Central_Air
t = -27.433, df = 336.06, p-value < 2.2e-16
alternative hypothesis: true difference in means between group N and group Y is not equal to 0
95 percent confidence interval:
 -90625.69 -78498.92
sample estimates:
mean in group N mean in group Y 
       101890.5        186452.8 
\end{verbatim}

Our final conclusion from the t-test is the \emph{rejection of the null
hypothesis} and the conclusion that \emph{houses with and without
central air should be expected to sell for different prices.}

\hypertarget{sec-wilcoxon}{%
\subsection{Mann-Whitney-Wilcoxon Test}\label{sec-wilcoxon}}

As we pointed out in Section Section~\ref{sec-testnorm}, the
distribution of \texttt{Sale\_Price} was not precisely normal. The most
principled way to proceed in this case would be with a non-parametric
test. The Mann-Whitney-Wilcoxon test is not identical to the t-test in
its null and alternative hypotheses, but it aims to answer the same
question about an association between the binary attribute and the
continuous attribute.

The null and alternative hypotheses for this test are typically defined
as follows, so long as the two groups \emph{are} identically distributed
(having the same \emph{shape} and variance):
\[H_0: \text{ the medians of the two groups are equal }  \]
\[H_a: \text{ the medians of the two groups are NOT equal }   \] If
those identical distributions are also symmetric, then
Mann-Whitney-Wilcoxon can be interpretted as testing for a difference in
means. When the data is not identically distributed, or when the
distributions are not symmetric, Mann-Whitney-Wilcoxon is a test of
\textbf{dominance} between distributions. \textbf{Distributional
dominance} is the notion that one group's distribution is located at
larger values than another, probabilistically speaking. Formally, a
random variable A has dominance over random variable B if
\(P(A x) \geq P(B\geq x)\) for all \(x\), and for some \(x\),
\(P(A\geq x) > P(B\geq x)\).

We summarize this information in the following table:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5875}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4125}}@{}}
\toprule()
\endhead
Conditions & \begin{minipage}[t]{\linewidth}\raggedright
Interpretation of Significant\\
Mann-Whitney-Wilcoxon Test\strut
\end{minipage} \\
\begin{minipage}[t]{\linewidth}\raggedright
Group distributions are identical in shape,\\
variance, and symmetric\strut
\end{minipage} & Difference in means \\
\begin{minipage}[t]{\linewidth}\raggedright
Group distributions are identical in shape,\\
variance, but not symmetric\strut
\end{minipage} & Difference in medians \\
\begin{minipage}[t]{\linewidth}\raggedright
Group distributions are identical in shape,\\
variance, but not symmetric\strut
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
Difference in location.\\
(distributional dominance)\strut
\end{minipage} \\
\bottomrule()
\end{longtable}

To perform this test, we use the \texttt{wilcox.test()} function, whose
inputs are identical to the \texttt{t.test()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Central\_Air, }\AttributeTok{data =}\NormalTok{ ames)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Wilcoxon rank sum test with continuity correction

data:  Sale_Price by Central_Air
W = 63164, p-value < 2.2e-16
alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

Thus, we make the same conclusion with our non-parametric test. Houses
with and without central air should be expected to sell for different
prices.

\bookmarksetup{startatroot}

\hypertarget{sec-slr}{%
\chapter{Introduction to ANOVA and Linear Regression}\label{sec-slr}}

This Chapter aims to answer the following questions:

What is a predictive model versus an explanatory model?

How to perform an honest assessment of a model.

How to estimate associations.

Continuous-Continuous

Continuous-Categorical

Pearson's correlation

Test of Hypothesis

Effect of outliers

Correlation Matrix

How to perform ANOVA.

Testing assumptions

Kruskal-Wallis

Post-hoc tests

How to perform Simple Linear Regression.

Assumptions

Inference

In this chapter, we introduce one of the most commonly used tools in
data science: the linear model. We'll start with some basic terminology.
A \textbf{linear model} is an equation that typically takes the form
\begin{equation}\protect\hypertarget{eq-linmod}{}{
\mathbf{y} = \beta_0 + \beta_1\mathbf{x_1} + \dots + \beta_k\mathbf{x_k} + \boldsymbol \varepsilon 
}\label{eq-linmod}\end{equation}

The left-hand side of this equation, \(\mathbf{y}\) is equivalently
called the \textbf{target}, \textbf{response}, or \textbf{dependent}
variable. The right-hand side is a linear combination of the columns
\(\{\mathbf{x_i}\}_{i=1}^{k}\) which are commonly referred to as
\textbf{explanatory}, \textbf{input}, \textbf{predictor}, or
\textbf{independent} variables.

\hypertarget{sec-evp}{%
\section{Prediction vs.~Explanation}\label{sec-evp}}

The purpose of a linear model like Equation (Equation~\ref{eq-linmod})
is generally two-fold:

The model is \textbf{predictive} in that it can estimate the value of
\(y\) for a given combination of the \(x\) attributes.

The model is \textbf{explanatory} in that it can estimate how \(y\)
changes for a unit increase in a given \(x\) attribute, holding all else
constant (via the slope parameters \(\beta\)).

However, it's common for \emph{one} of these purposes to be more aligned
with the specific goals of your project, and it is common to approach
the building of such a model differently for each purpose.

In predictive modeling, you are most interested in how much error your
model has on \emph{holdout data}, that is, validation or test data. This
is a notion that we introduce next in Section
Section~\ref{sec-trainvalidtest}. If good predictions are all you want
from your model, you are unlikely to care how many variables (including
polynomial and interaction terms) are included in the final model.

In explanatory modeling, you foremost want a model that is simple to
interpret and doesn't have too many input variables. It's common to
avoid many polynomial and interaction terms for explanatory models.
While the error rates on holdout data will still be useful reporting
metrics for explanatory models, it will be more important to craft the
model for ease of interpretation.

\hypertarget{sec-trainvalidtest}{%
\section{Honest Assessment}\label{sec-trainvalidtest}}

When performing predictive or explanatory modeling, we \emph{always}
divide our data into subsets for training, validation, and/or final
testing. Because models are prone to discovering small, spurious
patterns on the data that is used to create them (the \emph{training
data}), we set aside the \emph{validation} and \emph{testing data} to
get a clear view of how they might perform on new data that they've
never seen before. This is a concept that will be revisited several
times throughout this text, highlighting its importance to honest
assessment of models.

There is no single right answer for how this division should occur for
every data set - the answer depends on a multitude of factors that are
beyond the scope of our present discussion. Generally speaking, one
expects to keep about 70\% of the data for model training purposes, and
the remaining 30\% for validation and testing. These proportions may
change depending on the amount of data available. If one has millions of
observations, they can often get away with a much smaller proportion of
training data to reduce computation time and increase confidence in
validation. If one has substantially fewer observations, it may be
necessary to increase the training proportion in order to build a sound
model - trading validation confidence for proper training.

Below we demonstrate one technique for separating the data into just two
subsets: training and test. These two subsets will suffice for our
analyses in this text. We'll use 70\% of our data for the training set
and the remainder for testing.

Since we are taking a random sample, each time you run this functions
you will get a different result. This can be difficult for team members
who wish to keep their analyses in sync. To avoid that variation of
results, we can provide a ``seed'' to the internal random number
generation process, which ensures that the randomly generated output is
the same to all who use that seed.

The following code demonstrates sampling via the \texttt{tidyverse}.
This method requires the use of an id variable. If your data set has a
unique identifier built in, you may omit the first line of code (after
\texttt{set.seed()}) and use that unique identifier in the third line.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{ames }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \FunctionTok{row\_number}\NormalTok{())}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_frac}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}

\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{anti\_join}\NormalTok{(ames, train, }\AttributeTok{by =} \StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{)}

\FunctionTok{dim}\NormalTok{(train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2051   82
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 879  82
\end{verbatim}

\hypertarget{bivariate-eda}{%
\section{Bivariate EDA}\label{bivariate-eda}}

As stated in Chapter Chapter~\ref{sec-intro-stat}, exploratory data
analysis is the foundation of any successful data science project. As we
move on to the discussion of modeling, we begin to explore
\emph{bivariate} relationships in our data. In doing so, we will often
explore the input variables' relationships with the target. \textbf{Such
exploration should only be done on the training data; we should never
let insights from the validation or test data inform our decisions about
modeling.}

Bivariate exploratory analysis is often used to assess relationships
between two variables. An \textbf{association} or \textbf{relationship}
exists when the expected value of one variable changes at different
levels of the other variable. A \textbf{linear relationship} between two
continuous variables can be inferred when the general shape of a scatter
plot of the two variables is a straight line.

\hypertarget{continuous-continuous-associations}{%
\subsection{Continuous-Continuous
Associations}\label{continuous-continuous-associations}}

Let's conduct a preliminary assessment of the relationship between the
size of the house in square feet (via \texttt{Gr\_Liv\_Area}) and the
\texttt{Sale\_Price} by creating a scatter plot (only on the training
data). Note that we call this a preliminary assessment because we should
not declare a statistical relationship without a formal hypothesis test
(see Section Section~\ref{sec-cor}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Gr\_Liv\_Area, }\AttributeTok{y =}\NormalTok{ Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Greater Living Area (Sqft)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/scatterplot-1.pdf}

}

\caption{Scatter plot demonstrating a positive linear relationship}

\end{figure}

\hypertarget{continuous-categorical-associations}{%
\subsection{Continuous-Categorical
Associations}\label{continuous-categorical-associations}}

We'll also revisit the plots that we created in Section
Section~\ref{sec-eda}, this time being careful to use only our training
data since our goal is eventually to use a linear model to predict
\texttt{Sale\_Price}.

We start by exploring the relationship between the external quality
rating of the home (via the ordinal variable \texttt{Exter\_Qual} and
the \texttt{Sale\_Price}).

The simplest graphic we may wish to create is a bar chart like Figure
Figure~\ref{fig-barsale} that shows the average sale price of homes with
each value of exterior quality.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(train) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Exter\_Qual,}\AttributeTok{y=}\NormalTok{ Sale\_Price), }
           \AttributeTok{position =} \StringTok{"dodge"}\NormalTok{, }\AttributeTok{stat =} \StringTok{"summary"}\NormalTok{, }\AttributeTok{fun =} \StringTok{"mean"}\NormalTok{) }\SpecialCharTok{+}                                      
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{format}\NormalTok{(x, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)) }\CommentTok{\# Modify formatting of axis}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-barsale-1.pdf}

}

\caption{\label{fig-barsale}Bar Chart Comparing Average Sale Price of
Homes with each Level of Exterior Quality}

\end{figure}

This gives us the idea that there may be an association between these
two attributes, but it can be tricky to rely solely on this graph
without exploring the overall distribution in sale price for each group.
While this chart is great for the purposes of \emph{reporting} (once
we've verified the relationship), it's not the best one for exploratory
analysis. The next two charts allow us to have much more information on
one graphic.

The frequency histogram in Figure Figure~\ref{fig-overhistogram} allows
us to see that much fewer of the homes have a rating of
\texttt{Excellent} versus the other tiers, a fact that makes it
difficult to compare the distributions. To normalize that quantity and
compare the raw probability densities, we can change our axes to density
(which is analogous to percentage) and employ a kernel density estimator
with the \texttt{geom\_density} plot as shown in Figure
@ref(fig:overhistogramdensitykernel). We can then clearly see that as
the exterior quality of the home ``goes up'' (in the ordinal sense, not
in the linear sense), the sale price of the home also increases.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(train,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{fill=}\NormalTok{Exter\_Qual)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{alpha=}\FloatTok{0.2}\NormalTok{, }\AttributeTok{position=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-overhistogram-1.pdf}

}

\caption{\label{fig-overhistogram}Histogram: Frequency of Sale\_Price
for varying qualities of home exterior}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ames,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{fill=}\NormalTok{Exter\_Qual)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha=}\FloatTok{0.2}\NormalTok{, }\AttributeTok{position=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-overhistogramdensitykernel-1.pdf}

}

\caption{\label{fig-overhistogramdensitykernel}Histogram: Density of
Sale\_Price for varying qualities of home exterior}

\end{figure}

To further explore the location and spread of the data, we can create
box-plots for each group using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ Sale\_Price}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{x =} \StringTok{\textasciigrave{}}\AttributeTok{Exter\_Qual}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{fill =} \StringTok{\textasciigrave{}}\AttributeTok{Exter\_Qual}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Exterior Quality Category"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ mean, }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{shape =} \DecValTok{20}\NormalTok{, }\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette=}\StringTok{"Blues"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-multiboxplot-1.pdf}

}

\caption{\label{fig-multiboxplot}Box Plots of Sale\_Price for each level
of Exter\_Qual}

\end{figure}

Notice that we've highlighted the mean on each box-plot for the purposes
of comparison. We now have a hypothesis that we may want to formally
test. After all, it is not good practice to look at Figures
Figure~\ref{fig-overhistogramdensitykernel}) and
Figure~\ref{fig-multiboxplot}) and \emph{declare} that a statistical
difference exists. While we do, over time, get a feel for which visually
apparent relationships turn out to be statistically significant, it's
\emph{imperative} that we conduct formal testing before declaring such
insights to a colleague or stakeholder.

If we want to test whether the \texttt{Sale\_Price} is different for the
different values of \texttt{Exter\_Qual}, we have to reach for the
multi-group alternative to the 2-sample t-test. This is called
\textbf{Analysis of Variance}, or \textbf{ANOVA} for short.

\hypertarget{sec-oneway}{%
\section{One-Way ANOVA}\label{sec-oneway}}

\textbf{One-way ANOVA} aims to determine whether there is a difference
in the mean of a continuous attribute across levels of a categorical
attribute. Sound like a two-sample t-test? Indeed, it's the extension of
that test to more than two groups. Performing ANOVA with a binary input
variable is mathematically identical to the two-sample t-test, as are
it's \textbf{assumptions}:

The observations are independent

The model residuals are normally distributed

The variances for each group are equal

A one-way ANOVA refers to a single hypothesis test, which is
\(H_{0}: \mu_{1}=\mu_{2}=...\mu_{k}\) for a predictor variable with
\(k\) levels against the alternative of at least one difference.
Although a one-way ANOVA is designed to assess whether or not there is a
significant difference within the mean values of the response with
respect to the different levels of the predictor variable, we can draw
some parallel to the regression model. For example, if we have \(k\)=4,
then we can let (x\_a), (x\_b), and (x\_c) be 3 reference-coded dummy
variables for the levels: \texttt{a}, \texttt{b}, \texttt{c}, and
\texttt{d}. Note that we only have 3 dummy variables because one gets
left out for the reference level, in this case it is \texttt{d}. The
linear model is of the following form:

\begin{equation}\protect\hypertarget{eq-anova1}{}{
y=\beta\_0 + \beta\_ax_a+\beta\_bx_b+\beta\_cx_c + \varepsilon  
}\label{eq-anova1}\end{equation}

If we define \(x_a\) as 1 if the observation belongs to level \texttt{a}
and 0 otherwise, and the same definition for \(x_b\) and \(x_c\), then
this is called \emph{reference-level coding} (this will change for
effects-level coding). The predicted values in Equation~\ref{eq-anova1}
is basically the predicted mean of the response within the 4 levels of
the predictor variable.\\

\(\beta_0\) represents the mean of reference group, group \texttt{d}.

\(\beta_a, \beta_b, \beta_c\) all represent the \emph{difference} in the
respective group means compared to the reference level. Positive values
thus reflect a group mean that is higher than the reference group, and
negative values reflect a group mean lower than the reference group.

\(\varepsilon\) is called the \textbf{error}.

A \emph{one-way} ANOVA model only contains a single input variable of
interest. Equation Equation~\ref{eq-anova1}, while it has 3 dummy
variable inputs, only contains a single nominal attribute. In
Chapter~\ref{sec-mlr}, we will add more inputs to the equation via
two-way ANOVA and multivariate regression models.

ANOVA is used to test the following hypothesis:
\[H_0: \beta_a=\beta_b=\beta_c = 0 \quad\text{(i.e. all group means are equal)}\]
\[H_0: \beta_a\neq0\mbox{ or }\beta_b\neq0 \mbox{ or } \beta_c \neq 0 \quad\text{(i.e. at least one is different)}\]
Both the \texttt{lm()} function and the \texttt{aov()} function will
provide the p-values to test the hypothesis above, the only difference
between the two functions is that \texttt{lm()} will also provide the
user with the coefficient of determinination, \(R^2\), which tells you
how much of the variation in \(y\) is accounted for by your categorical
input.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Exter\_Qual, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{anova}\NormalTok{(ames\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: Sale_Price
             Df     Sum Sq    Mean Sq F value    Pr(>F)    
Exter_Qual    3 6.6913e+12 2.2304e+12  701.83 < 2.2e-16 ***
Residuals  2047 6.5054e+12 3.1780e+09                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ames\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Sale_Price ~ Exter_Qual, data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-215904  -32910   -6147   24793  516090 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    207785       3176  65.416  < 2e-16 ***
Exter_Qual.L   215078       8353  25.749  < 2e-16 ***
Exter_Qual.Q    44553       6353   7.013 3.15e-12 ***
Exter_Qual.C     6994       3308   2.114   0.0346 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 56370 on 2047 degrees of freedom
Multiple R-squared:  0.507, Adjusted R-squared:  0.5063 
F-statistic: 701.8 on 3 and 2047 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ames\_aov \textless{}{-} aov(Sale\_Price \textasciitilde{} Exter\_Qual, data = train) \# Same thing with aov() function instead}
\CommentTok{\# summary(ames\_aov) \# R{-}squared not reported here. }
\end{Highlighting}
\end{Shaded}

The p-value for the ANOVA hypothesis that all the groups have the same
mean sale price is incredibly small, at \(2.2\times10^{-16}\). This
means it is extraordinarily improbable that we would have observed these
differences in means, or a more extreme difference, if the population
group means were equal. Thus, we reject our null hypothesis and conclude
that there is an association between the exterior quality of a home and
the price of the home.

We note, based on the \(R^2\) statistics, that the exterior quality
rating can account for almost half the variation in sales price!
Adjusted \(R^2\) is a statistic that takes into account the number of
variables in the model. The difference between \(R^2\) and adjusted
\(R^2\) will be more thoroughly discussed in Chapter
Chapter~\ref{sec-mlr}.

We can also confirm what we know about the predictions from ANOVA, that
there are only \(k\) unique predictions from an ANOVA with \(k\) groups
(the predictions being the group means), using the \texttt{predict}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train}\SpecialCharTok{$}\NormalTok{pred\_anova }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ames\_lm, }\AttributeTok{data =}\NormalTok{ train)}

\NormalTok{train}\SpecialCharTok{$}\NormalTok{resid\_anova }\OtherTok{\textless{}{-}} \FunctionTok{resid}\NormalTok{(ames\_lm, }\AttributeTok{data =}\NormalTok{ train)}

\NormalTok{(}\AttributeTok{model\_output =}\NormalTok{ train }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Sale\_Price, pred\_anova, resid\_anova))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2,051 x 3
   Sale_Price pred_anova resid_anova
        <int>      <dbl>       <dbl>
 1     232600    228910.       3690.
 2     166000    228910.     -62910.
 3     170000    142107.      27893.
 4     252000    228910.      23090.
 5     134000    142107.      -8107.
 6     164700    228910.     -64210.
 7     193500    142107.      51393.
 8     118500    142107.     -23607.
 9      94000    142107.     -48107.
10     111250    142107.     -30857.
# ... with 2,041 more rows
\end{verbatim}

\hypertarget{testing-assumptions}{%
\subsection{Testing Assumptions}\label{testing-assumptions}}

We can use the default plots from the \texttt{lm()} function to check
the normality assumption.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ames\_lm)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-anovaplots-1.pdf}

}

\caption{\label{fig-anovaplots}Of the 4 default plots from lm(), we are
presently interested in the top-right QQ plot that tests our assumption
of normally distributed residuals.}

\end{figure}

In the top-right plot in Figure Figure~\ref{fig-anovaplots} we verify
again the \emph{approximate} normality of sale price. To test for the
third assumption of equal variances, we opt for a formal test like
Levene's (which depends on normality and can be found in the
\texttt{car} package) or Fligner's (which does not depend on normality
and exists in the \texttt{stats} package). In both cases, the null
hypothesis is equal variances:
\[H_0: \sigma_a^2 =\sigma_b^2 =\sigma_c^2=\sigma_d^2 \quad  \text{i.e., the groups have equal variance}\]
\[H_a: \text{at least one group's variance is different}\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(stats)}

\FunctionTok{leveneTest}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Exter\_Qual, }\AttributeTok{data =}\NormalTok{ train) }\CommentTok{\# Most popular, but depends on Normality}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Levene's Test for Homogeneity of Variance (center = median)
        Df F value    Pr(>F)    
group    3  76.879 < 2.2e-16 ***
      2047                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fligner.test}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Exter\_Qual, }\AttributeTok{data =}\NormalTok{ train) }\CommentTok{\# DOES NOT depend on Normality}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Fligner-Killeen test of homogeneity of variances

data:  Sale_Price by Exter_Qual
Fligner-Killeen:med chi-squared = 206.26, df = 3, p-value < 2.2e-16
\end{verbatim}

And in both cases, we're forced to reject the null hypothesis of equal
variances. A non-parametric version of the ANOVA test, the
Kruskal-Wallis test, is more suitable to this particular case.
Non-parametric tests do not have the same \emph{statistical power} to
detect differences between groups. \textbf{Statistical power} is the
probability of detecting an effect, if there is a true effect present to
detect. We should opt for these tests in situations where our data is
ordinal or otherwise violates the assumptions of normality or equal
variances in ways that cannot be fixed by logarithmic or other similar
transformation.

\hypertarget{kruskal}{%
\subsection{Kruskal-Wallis}\label{kruskal}}

The Kruskal-Wallis test, proposed in 1952, is equivalent to a parametric
one-way ANOVA where the data values have been replaced with their ranks
(i.e.~largest value = 1, second largest value = 2, etc.). When the data
\emph{is not} normally distributed but \emph{is} identically distributed
(having the same shape and variance), the Kruskal-Wallis test can be
considered a test for differences in medians. If those identical
distributions are also symmetric, then Kruskal-Wallis can be
interpretted as testing for a difference in means. When the data is not
identically distributed, or when the distributions are not symmetric,
Kruskal-Wallis is a test of \textbf{dominance} between distributions.
\textbf{Distributional dominance} is the notion that one group's
distribution is located at larger values than another, probabilistically
speaking. Formally, a random variable A has dominance over random
variable B if \(P(A\geq x) \geq P(B\geq x)\) for all \(x\), and for some
\(x\), \(P(A\geq x) > P(B\geq x)\).

We summarize this information in the following table:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6071}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3929}}@{}}
\toprule()
\endhead
Conditions & \begin{minipage}[t]{\linewidth}\raggedright
Interpretation of Significant\\
Kruskal-Wallis Test\strut
\end{minipage} \\
\begin{minipage}[t]{\linewidth}\raggedright
Group distributions are identical in shape,\\
variance, and symmetric\strut
\end{minipage} & Difference in means \\
\begin{minipage}[t]{\linewidth}\raggedright
Group distributions are identical in shape,\\
variance, but not symmetric\strut
\end{minipage} & Difference in medians \\
\begin{minipage}[t]{\linewidth}\raggedright
Group distributions are not identical in shape,\\
variance, and are not symmetric\strut
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
Difference in location.\\
(distributional dominance)\strut
\end{minipage} \\
\bottomrule()
\end{longtable}

Implementing the Kruskal-Wallis test in R is simple:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kruskal.test}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Exter\_Qual, }\AttributeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Kruskal-Wallis rank sum test

data:  Sale_Price by Exter_Qual
Kruskal-Wallis chi-squared = 975.98, df = 3, p-value < 2.2e-16
\end{verbatim}

Our conclusion would be that the distribution of sale price is different
across different levels of exterior quality.

\hypertarget{sec-posthoc}{%
\section{ANOVA Post-hoc Testing}\label{sec-posthoc}}

After performing an ANOVA and learning that there is a difference
between the groups of data, our next natural question ought to be
\emph{which groups of data are different, and how?} In order to explore
this question, we must first consider the notion of
\textbf{experimentwise error}. When conducting multiple hypothesis tests
simultaneously, the experimentwise error rate is the proportion of time
you expect to make an error in at least one test.

Let's suppose we are comparing grocery spending on 4 different credit
card rewards programs. If we'd like to compare the rewards programs
pairwise, that entails 6 different hypothesis tests (each is a
two-sample t-test). If we keep a confidence level of \(\alpha = 0.05\)
and subsequently view ``being wrong in one test'' as a random event
happening with probability \(p=0.05\) then our probability of being
wrong \emph{in at least} one test out of 6 could be as great as 0.26!

To control this experiment-wise error rate, we must lower our
significance thresholds to account for it. Alternatively, we can view
this as an adjustment of our p-values higher while keeping our
significance threshold fixed as usual. This is typically the approach
taken, as we prefer to fix our significance thresholds in accordance
with previous literature or industry standards. There are many methods
of adjustment that have been proposed over the years for this purpose.
Here, we consider two popular methods: Tukey's test for pairwise
comparisons and Dunnett's test for control group comparisons. If the
reader finds themselves in a situation that doesn't fit the prescription
of either of these methods, we suggest looking next at the modified
Bonferroni correction or the notion of false discovery rates proposed by
Benjamini and Hochberg in 1995.

\hypertarget{sec-tukey}{%
\subsection{Tukey-Kramer}\label{sec-tukey}}

If our objective is to compare each group to every other group then
Tukey's test of honest significant differences, also known as the
Tukey-Kramer test is probably the most widely-available and popular
corrections in practice. However, it should be noted that Tukey's test
\emph{should not} be used if one does not plan to make all pairwise
comparisons. If only a subset of comparisons are of interest to the user
(like comparisons only to a control group) then one should opt for the
Dunnett or a modified Bonferroni correction.

To employ Tukey's HSD in R, we must use the \texttt{aov()} function to
create our ANOVA object rather than the \texttt{lm()} function. The
output of the test shows the difference in means and the p-value for
testing the null hypothesis that the means are equal (i.e.~that the
differences are equal to 0).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_aov }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Exter\_Qual, }\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{tukey.ames }\OtherTok{\textless{}{-}} \FunctionTok{TukeyHSD}\NormalTok{(ames\_aov)}
\FunctionTok{print}\NormalTok{(tukey.ames)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = Sale_Price ~ Exter_Qual, data = train)

$Exter_Qual
                       diff       lwr       upr p adj
Typical-Fair       57887.91  30194.31  85581.52 5e-07
Good-Fair         144690.25 116739.87 172640.63 0e+00
Excellent-Fair    291684.79 259752.41 323617.16 0e+00
Good-Typical       86802.34  79910.03  93694.64 0e+00
Excellent-Typical 233796.87 216886.62 250707.12 0e+00
Excellent-Good    146994.54 129666.98 164322.10 0e+00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(tukey.ames, }\AttributeTok{las =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-tukey-1.pdf}

}

\caption{\label{fig-tukey}Confidence intervals for mean differences
adjusted via Tukey-Kramer}

\end{figure}

The p-values in this table have been adjusted higher to account for the
possible experimentwise error rate. For every pairwise comparison shown,
we reject the null hypothesis and conclude that the mean sales price of
the homes is different for each level of \texttt{Exter\_Qual}.
Furthermore, Figure Figure~\ref{fig-tukey} shows us experiment-wise
(family-wise) adjusted confidence intervals for the differences in means
for each pair. The plot option \texttt{las=1} guides the axis labels.
Type \texttt{?par} for a list of plot options for base R, including an
explanation of \texttt{las}.

\hypertarget{sec-dunnett}{%
\subsection{Dunnett's Test}\label{sec-dunnett}}

If the plan is to make fewer comparisons, specifically just \(k-1\)
comparisons where \(k\) is the number of groups in your data (indicating
you plan to compare all the groups to one specific group, usually the
control group), then Dunnett's test would be preferrable to the
Tukey-Kramer test. If all pairwise comparisons are not made, the
Tukey-Kramer test is overly conservative, creating a confidence level
that is much lower than specified by the user. Dunnett's test factors in
fewer comparisons and thus should not be used for tests of all pairwise
comparisons.

To use Dunnett's test, we must add the \texttt{DescTools} package to our
library. The control group to which all other groups will be compared is
designated by the \texttt{control=} option.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{DunnettTest}\NormalTok{(}\AttributeTok{x =}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{g =}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{Exter\_Qual, }\AttributeTok{control =} \StringTok{\textquotesingle{}Typical\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Dunnett's test for comparing several treatments with a control :  
    95% family-wise confidence level

$Typical
                       diff    lwr.ci    upr.ci    pval    
Fair-Typical      -57887.91 -83628.55 -32147.28 2.6e-07 ***
Good-Typical       86802.34  80396.08  93208.59 < 2e-16 ***
Excellent-Typical 233796.87 218079.15 249514.60 < 2e-16 ***

---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the output from Dunnett's test, we notice the p-value comparing
\texttt{Fair} to \texttt{Typical} exterior qualities is lower than it
was in the Tukey-Kramer test. This is consistent with our expectations
for a test that isn't controlling for as many comparisons; it makes a
smaller upward adjustment of p-values to satisfy a given experiment-wise
error rate.

\hypertarget{sec-cor}{%
\section{Pearson Correlation}\label{sec-cor}}

ANOVA is used to formally test the relationship between a categorical
variable and a continuous variable. To formally test the (linear)
relationship between two continuous attributes, we introduce
\textbf{Pearson correlation}, commonly referred to as simply
\textbf{correlation}. Correlation is a number between -1 and 1 which
measures the strength of a linear relationship between two continuous
attributes.

Negative values of correlation indicate a \emph{negative linear
relationship}, meaning that as one of the variables increases, the other
tends to decrease. Similarly, positive values of correlation indicate a
\emph{positive linear relationship} meaning that as one of the variables
increases, the other tends to increase. Absolute values of correlation
equal to 1 indicate a perfect linear relationship. For example, if our
data had a column for ``mile time in minutes'' and a column for ``mile
time in seconds'', these two columns would have a correlation of 1 due
to the fact that there are 60 seconds in a minute. A correlation value
near 0 indicates that the variables have no linear relationship.

It's important to emphasize that Pearson correlation is only designed to
detect \emph{linear} associations between variables. Even when a
correlation between two variables is 0, the two variables may still have
a very clear association, whether it be quadratic, cyclical, or some
other nonlinear pattern of association. Figure @ref(fig:correlation)
illustrates all of these statements. On top of each scatter plot, the
correlation coefficient is shown. The middle row of this figure aims to
illustrate that a perfect correlation has nothing to do with the
\emph{magnitude} or \emph{slope} of the relationship. In the center
image, middle row, we note that the correlation is undefined for any
pair that includes a constant variable. In that image, the value of
\(y\) is constant across the sample. Equation @ref(eq:correlation) makes
this mathematically clear.

\begin{figure}

{\centering \includegraphics{./img/correlation.png}

}

\caption{\label{fig-correlation}`Examples of relationships and their
associated correlations'}

\end{figure}

The population correlation parameter is denoted \(\rho\) and estimated
by the sample correlation, denoted as \(r\). The formula for the sample
correlation between columns of data \(\mathbf{x}\) and \(\mathbf{y}\) is

\begin{equation}\protect\hypertarget{eq-correlation}{}{
r = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{x})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{x})^2}}.
}\label{eq-correlation}\end{equation}

Note that with \emph{centered} variable vectors \(\mathbf{x_c}\) and
\(\mathbf{y_c}\) this formula becomes much cleaner with linear algebra
notation:

\begin{equation}\protect\hypertarget{eq-veccorrelation}{}{
r = \frac{\mathbf{x_c}^T\mathbf{y_c}}{\|\mathbf{x_c}\|\|\mathbf{y_c}\|}.
}\label{eq-veccorrelation}\end{equation}

It is interesting to note that Equation Equation~\ref{eq-veccorrelation}
is identical to the formula for the cosine of the angle between to
vectors. While this geometrical relationship does not benefit our
intuition\footnote{The n-dimensional ``variable vectors'' \mathbf{x_c}
  and \mathbf{y_c} live in the vast \emph{sample space} where the
  \(i^{th}\) axis represents the \(i^th\) observation in your dataset.
  In this space, a single point/vector is one possible set of sample
  values of n observations; this space can be difficult to grasp
  mentally}, it is noteworthy nonetheless.

Pearson's correlation can be calculated in R with the built in
\texttt{cor()} function, with the two continuous variables as input:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Gr\_Liv\_Area,train}\SpecialCharTok{$}\NormalTok{Sale\_Price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.698509
\end{verbatim}

\hypertarget{sec-testcor}{%
\subsection{Statistical Test}\label{sec-testcor}}

To test the statistical significance of correlation, we use a t-test
with the null hypothesis that the correlation is equal to 0:
\[H_0: \rho = 0\] \[H_a: \rho \neq 0\] If we can reject the null
hypothesis, then we declare a significant linear association between the
two variables. The \texttt{cor.test()} function in R will perform the
test:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Gr\_Liv\_Area,train}\SpecialCharTok{$}\NormalTok{Sale\_Price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  train$Gr_Liv_Area and train$Sale_Price
t = 44.185, df = 2049, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.6756538 0.7200229
sample estimates:
     cor 
0.698509 
\end{verbatim}

We conclude that \texttt{Gr\_Liv\_Area} has a linear association with
Sale\_Price.

It must be noted that this t-test for Pearson's correlation is not free
from assumptions. In fact, there are 4 assumptions that must be met, and
they are detailed in Section Section~\ref{sec-slrassumptions}.

\hypertarget{effect-of-anomalous-observations}{%
\subsection{Effect of Anomalous
Observations}\label{effect-of-anomalous-observations}}

One final nuance that is important to note is the effect of anomalous
observations on correlation. In Figure Figure~\ref{fig-nocor} we display
30 random 2-dimensional data points \((x,y)\) with no linear
relationship.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{11}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x,y)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-nocor-1.pdf}

}

\caption{\label{fig-nocor}The variables x and y have no correlation}

\end{figure}

The correlation is not exactly zero (we wouldn't expect perfection from
random data) but it is very close at 0.002.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(x,y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  x and y
t = 0.012045, df = 28, p-value = 0.9905
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.3582868  0.3622484
sample estimates:
        cor 
0.002276214 
\end{verbatim}

Next, we'll add a single anomalous observation to our data and see how
it affects both the correlation value and the correlation test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{31}\NormalTok{] }\OtherTok{=} \DecValTok{4}
\NormalTok{y[}\DecValTok{31}\NormalTok{] }\OtherTok{=} \DecValTok{50}
\FunctionTok{cor.test}\NormalTok{(x,y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  x and y
t = 5.803, df = 29, p-value = 2.738e-06
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.5115236 0.8631548
sample estimates:
      cor 
0.7330043 
\end{verbatim}

The correlation jumps to 0.73 from 0.002 and is declared strongly
significant! Figure Figure~\ref{fig-extremenocor} illustrates the new
data. This simple example shows why exploratory data analysis is so
important! If we don't explore our data and detect anomalous
observations, we might improperly declare relationships are significant
when they are driven by a single observation or a small handful of
observations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(x,y)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-extremenocor-1.pdf}

}

\caption{\label{fig-extremenocor}A single anomalous observation creates
strong correlation (0.73) where there previously was none}

\end{figure}

\hypertarget{the-correlation-matrix}{%
\subsection{The Correlation Matrix}\label{the-correlation-matrix}}

It's common to consider and calculate all pairwise correlations between
variables in a dataset. If many attributes share a high degree of mutual
correlation, this can cause problems for regression as will be discussed
in Chapter 5. The pairwise correlations are generally arranged in an
array called the \textbf{correlation matrix}, where the \((i,j)^{th}\)
entry is the correlation between the \(i^{th}\) variable and \(j^{th}\)
variable in your list. To compute the correlation matrix, we again use
the \texttt{cor()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(train[, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Year\_Built\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Total\_Bsmt\_SF\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}First\_Flr\_SF\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Gr\_Liv\_Area\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Sale\_Price\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Year_Built Total_Bsmt_SF First_Flr_SF Gr_Liv_Area Sale_Price
Year_Built     1.0000000     0.4037104    0.3095407   0.2454325  0.5668889
Total_Bsmt_SF  0.4037104     1.0000000    0.8120419   0.4643838  0.6276502
First_Flr_SF   0.3095407     0.8120419    1.0000000   0.5707205  0.6085229
Gr_Liv_Area    0.2454325     0.4643838    0.5707205   1.0000000  0.6985090
Sale_Price     0.5668889     0.6276502    0.6085229   0.6985090  1.0000000
\end{verbatim}

Not surprisingly, we see strong positive correlation between the square
footage of the basement and that of the first floor, and also between
all of the area variables and the sale price. As demonstrated by Figures
Figure~\ref{fig-correlation} and Figure~\ref{fig-extremenocor}, raw
correlation values can be misleading and it's unwise to calculate them
without a scatter plot for context. The \texttt{pairs()} function in
base R provides a simple matrix of scatterplots for this purpose.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(train[, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Year\_Built\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Total\_Bsmt\_SF\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}First\_Flr\_SF\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Gr\_Liv\_Area\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Sale\_Price\textquotesingle{}}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

\hypertarget{simple-linear-regression}{%
\section{Simple Linear Regression}\label{simple-linear-regression}}

After learning that two variables share a linear relationship, the next
question is natural: \emph{what is that relationship?} How much,on
average, should we expect one variable to change as the other changes by
a single unit? Simple linear regression answers this question by
creating a linear equation that best represents the relationship in the
sense that it minimizes the squared error between the observed data and
the model predictions (i.e.~the sum of the squared residuals). The
simple linear regression equation is typically written \begin{equation}
y=\beta_0 + \beta_1x + \varepsilon
(\#eq:slr)
\end{equation} where \(\beta_0\), the \textbf{intercept}, gives the
expected value of \(y\) when \(x=0\) and \(\beta_1\), the \textbf{slope}
gives the expected change in \(y\) for a one-unit increase in \(x\). The
\textbf{error}, \(\varepsilon\) is the amount each individual \(y\)
differs from the population line (we would not expect all values of
\(y\) to fall directly on the line). When we use a sample of data to
estimate the true population line, we get our prediction equation or
\(\hat{y}=\hat{\beta}_0 + \hat{\beta}_1x\). Residuals from the predicted
line is defined as \(e=y-\hat{y}\). \emph{Ordinary Least Squares seeks
to minimize the sum of squared residuals or sum of squared error}. That
objective is known as a \textbf{loss function}. The sum of squared error
(SSE) or equivalently the mean squared error (MSE) loss functions are by
far the most popular loss functions for continuous prediction problems.

We should note that SSE is not the \emph{only} loss function at our
disposal. Minimizing the \emph{mean absolute error} (MAE) is common in
situations with a highly skewed response variable (squaring very large
errors gives those observations in the tail too much influence on the
regression as we will later discuss). Using MAE to drive our loss
function gives us predictions that are conditional \emph{medians} of the
response, given the input data. Other loss functions, like Huber's M
function, are also used to handle problems with influential observations
as discussed in Chapter 5.

As we mentioned in Section Section~\ref{sec-evp}, a simple linear
regression serves two purposes:

to predict the expected value of \(y\) for each value of \(x\) and

to explain \emph{how} \(y\) is expected to change for a unit change in
\(x\).

In order to accurately use a regression for the second purpose, however,
we must first meet assumptions with our data.

\hypertarget{sec-slrassumptions}{%
\subsection{Assumptions of Linear Regression}\label{sec-slrassumptions}}

Linear regression, in particular the hypothesis tests that are generally
performed as part of linear regression, has 4 assumptions:

The expected value of \(y\) is linear in \(x\) (proper model
specification).

The random errors are independent.

The random errors are normally distributed.

The random errors have equal variance (homoskedasticity).

It must now be noted that these assumptions are also in effect for the
test of Pearson's correlation in Section Section~\ref{sec-testcor},
because the tests in simple linear regression are mathematically
equivalent to that test. When these assumptions are not met, another
approach to testing the significance of a linear relationship should be
considered. The most common non-parametric approach to testing for an
association between two continuous variables is Spearman's correlation.
Spearman's correlation does not limit its findings to linear
relationships; any monotonic relationship (one that is always increasing
or always decreasing) will cause Spearman's correlation to be
significant. Similar to the approach taken by Kruskal-Wallis, Spearman's
correlation replaces the data with its ranks and computes Pearson's
correlation on the ranks. The same \texttt{cor} and \texttt{cor.test()}
functions can be used; simply specify the
\texttt{method=\textquotesingle{}spearman\textquotesingle{}} option.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Gr\_Liv\_Area,train}\SpecialCharTok{$}\NormalTok{Sale\_Price, }\AttributeTok{method =} \StringTok{\textquotesingle{}spearman\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in cor.test.default(train$Gr_Liv_Area, train$Sale_Price, method =
"spearman"): Cannot compute exact p-value with ties
\end{verbatim}

\begin{verbatim}

    Spearman's rank correlation rho

data:  train$Gr_Liv_Area and train$Sale_Price
S = 408364087, p-value < 2.2e-16
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.7160107 
\end{verbatim}

\hypertarget{testing-for-association}{%
\subsection{Testing for Association}\label{testing-for-association}}

The statistical test of correlation is mathematically equivalent to
testing the hypothesis that the slope parameter in Equation @ref(eq:slr)
is zero. This t-test is part of the output from any linear regression
function, like \texttt{lm()} which we saw in Section
Section~\ref{sec-oneway}. Let's confirm this using the example from the
Section Section~\ref{sec-testcor} where we investigate the relationship
between \texttt{Gr\_Liv\_Area} and \texttt{Sale\_Price}. Again, the
t-test in the output tests the following hypothesis: \[H_0: \beta_1=0\]
\[H_a: \beta_1 \neq 0\] The first thing we will do after creating the
linear model is check our assumption using the default plots from
\texttt{lm()} . From these four plots we will be most interested in the
first two.

In the top left plot, we are visually checking for homoskedasticity.
We'd like to see the variability of the points remain constant from left
to right on this chart, indicating that the errors have constant
variance for each value of y. We do not want to see any fan shapes in
this chart. Unfortunately, we do see just that: the variability of the
errors is much smaller for smaller values of Sale Price than it is for
larger values of Sale Price.

In the top right plot, we are visually checking for normality of errors.
We'd like to see the QQ-plot indicate normality with all the points
roughly following the line. Unfortunately, we do not see that here. The
errors do not appear to be normally distributed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slr }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area, }\AttributeTok{data=}\NormalTok{train)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(slr)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-introduction_ANOVA_regression_files/figure-pdf/fig-slrplots-1.pdf}

}

\caption{\label{fig-slrplots}The variables x and y have no correlation}

\end{figure}

Despite the violation of assumptions, let's continue examining the
output from this regression in order to practice our interpretation of
it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(slr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Sale_Price ~ Gr_Liv_Area, data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-478762  -30030   -1405   22273  335855 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 14045.872   3942.503   3.563 0.000375 ***
Gr_Liv_Area   110.726      2.506  44.185  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 57430 on 2049 degrees of freedom
Multiple R-squared:  0.4879,    Adjusted R-squared:  0.4877 
F-statistic:  1952 on 1 and 2049 DF,  p-value: < 2.2e-16
\end{verbatim}

The first thing we're likely to examine in the coefficient table is the
p-value for \texttt{Gr\_Liv\_Area}. It is strongly significant (in fact,
it's the same t-value and p-value as we saw for the \texttt{cor.test} as
these tests are mathematically equivalent), indicating that there is an
association between the size of the home and the sale price.
Furthermore, the \emph{parameter estimate} is 115.5 indicating that we'd
expect the price of a home to increase by \$115.5 for every additional
square foot of living space. Because of the linearity of the model, we
can extend this slope estimate to any unit change in \(x\). For example,
it might be difficult to think in terms of single square feet when
comparing houses, so we might prefer to use a 100 square-foot change and
report our conclusion as follows: For each additional 100 square feet of
living area, we expect the house price to increase by \$11,550.

\bookmarksetup{startatroot}

\hypertarget{sec-mlr}{%
\chapter{Complex ANOVA and Multiple Linear Regression}\label{sec-mlr}}

In in Chapter Chapter~\ref{sec-slr} we were introduced to the One-Way
ANOVA and simple linear regression models. These models only contained a
single variable - categorical for ANOVA and continuous for simple linear
regression - to explain and predict our target variable. Rarely do we
believe that only a single variable will suffice in predicting a
variable of interest. Here in this Chapter we will generalize these
models to the \(n\)-Way ANOVA and multiple linear regression models.
These models contain multiple sets of variables to explain and predict
our target variable.

This Chapter aims to answer the following questions:

How do we include multiple variables in ANOVA?

Exploration

Assumptions

Predictions

What is an interaction between two predictor variables?

Interpretation

Evaluation

Within Category Effects

What is blocking in ANOVA?

Nuisance Factors

Differences Between Blocking and Two-Way ANOVA

How do we include multiple variables in regression?

Model Structure

Global \& Local Inference

Assumptions

Adjusted \(R^2\)

Categorical Variables in Regression

\hypertarget{two-way-anova}{%
\section{Two-Way ANOVA}\label{two-way-anova}}

Section Chapter~\ref{sec-slr} details the One-Way ANOVA model using a
single categorical predictor variable with \(k\) levels to predict our
continuous target variable. Now we will generalize this model to include
\(n\) categorical variables that each have different numbers of levels
(\(k_1, k_2, ..., k_n\)).

\hypertarget{exploration}{%
\subsection{Exploration}\label{exploration}}

Let's use the basic example of two categorical predictor variables in a
Two-Way ANOVA. Previously, we talked about using heating quality as a
factor to explain and predict sale price of homes in Ames, Iowa. Now, we
also consider whether the home has central air. Although similar in
nature, these two factors potentially provide important, unique pieces
of information about the home. Similar to previous data science
problems, let us first explore our variables and their potential
relationships.

Now that we have two variables that we will use to explain and predict
sale price, here are some summary statistics (mean, standard deviation,
minimum, and maximum) for each combination of category. We will use the
\texttt{group\_by} function on both predictor variables of interest to
split the data and then the \texttt{summarise} function to calculate the
metrics we are interested in.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames }\OtherTok{\textless{}{-}} \FunctionTok{make\_ordinal\_ames}\NormalTok{()}

\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{ames }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \FunctionTok{row\_number}\NormalTok{())}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_frac}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}

\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{anti\_join}\NormalTok{(ames, train, }\AttributeTok{by =} \StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Heating\_QC, Central\_Air) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(Sale\_Price), }
            \AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(Sale\_Price), }
            \AttributeTok{max =} \FunctionTok{max}\NormalTok{(Sale\_Price), }
            \AttributeTok{min =} \FunctionTok{min}\NormalTok{(Sale\_Price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'Heating_QC'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{verbatim}
# A tibble: 10 x 6
# Groups:   Heating_QC [5]
   Heating_QC Central_Air    mean     sd    max    min
   <ord>      <fct>         <dbl>  <dbl>  <int>  <int>
 1 Poor       N            50050  52255.  87000  13100
 2 Poor       Y           107000     NA  107000 107000
 3 Fair       N            84748. 28267. 158000  37900
 4 Fair       Y           145165. 38624. 230000  50000
 5 Typical    N           103469. 34663. 209500  12789
 6 Typical    Y           142003. 39657. 375000  60000
 7 Good       N           110811. 38455. 214500  59000
 8 Good       Y           160113. 54158. 415000  52000
 9 Excellent  N           115062. 33271. 184900  64000
10 Excellent  Y           216401. 88518. 745000  58500
\end{verbatim}

We can already see above that there appears to be some differences in
average sale price across the categories overall. Within each grouping
of heating quality, homes with central air appear to have a higher sale
price than homes without. Also, similar to before, homes with higher
heating quality appear to have higher sale prices compared to homes with
lower heating quality.

We also see these relationships in the bar chart in Figure
Figure~\ref{fig-twomeans}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CA\_heat }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Heating\_QC, Central\_Air) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(Sale\_Price), }
            \AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(Sale\_Price), }
            \AttributeTok{max =} \FunctionTok{max}\NormalTok{(Sale\_Price), }
            \AttributeTok{min =} \FunctionTok{min}\NormalTok{(Sale\_Price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'Heating_QC'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ CA\_heat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Heating\_QC, }\AttributeTok{y =}\NormalTok{ mean}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ Central\_Air)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Sales Price (Thousands $)"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Heating Quality Category"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Paired"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/fig-twomeans-1.pdf}

}

\caption{\label{fig-twomeans}Distribution of Variables Heating\_QC and
Central\_Air}

\end{figure}

As before, visually looking at bar charts and mean calculations only
goes so far. We need to statistically be sure of any differences that
exist between average sale price in categories.

\hypertarget{model}{%
\subsection{Model}\label{model}}

We are going to do this with the same approach as in the One-Way ANOVA
of Chapter Chapter~\ref{sec-slr}, just with more variables as shown in
the following equation:

\[
Y_{ijk} = \mu + \alpha_i + \beta_j + \varepsilon_{ijk}
\] where \(\mu\) is the average baseline sales price of a home in Ames,
Iowa, \(\alpha_i\) is the variable representing the impacts of the
levels of heating quality, and \(\beta_j\) is the variable representing
the impacts of the levels of central air. As mentioned previously, the
unexplained error in this model is represented as \(\varepsilon_{ijk}\).

The same F test approach is also used, just for each one of the
variables. Each variable's test has a null hypothesis assuming all
categories have the same mean. The alternative for each test is that at
least one category's mean is different.

Let's view the results of the \texttt{aov} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_aov2 }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Heating\_QC }\SpecialCharTok{+}\NormalTok{ Central\_Air, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(ames\_aov2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Df    Sum Sq   Mean Sq F value   Pr(>F)    
Heating_QC     4 2.891e+12 7.228e+11  147.60  < 2e-16 ***
Central_Air    1 2.903e+11 2.903e+11   59.28 2.11e-14 ***
Residuals   2045 1.002e+13 4.897e+09                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

From the above results, we have low p-values for each of the variables'
F test. Heating quality had 4 degrees of freedom, derived from the 5
categories \((4 = 5-1)\). Similarly, central air's 2 categories produce
1 \((= 2-1)\) degree of freedom. The F values are calculated the exact
same way as described before with the mean square for each variable
divided by the mean square error. Based on these tests, at least one
category in each variable is statistically different than the rest.

\hypertarget{post-hoc-testing}{%
\subsection{Post-Hoc Testing}\label{post-hoc-testing}}

As with the One-Way ANOVA, the next logical question is which of these
categories is different. We will use the same post-hoc tests as before
with the \texttt{TukeyHSD} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tukey.ames2 }\OtherTok{\textless{}{-}} \FunctionTok{TukeyHSD}\NormalTok{(ames\_aov2)}
\FunctionTok{print}\NormalTok{(tukey.ames2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = Sale_Price ~ Heating_QC + Central_Air, data = train)

$Heating_QC
                       diff        lwr       upr     p adj
Fair-Poor          49176.42 -63650.448 162003.29 0.7571980
Typical-Poor       67781.01 -42800.320 178362.35 0.4506761
Good-Poor          87753.89 -23040.253 198548.03 0.1945181
Excellent-Poor    146288.89  35818.859 256758.92 0.0028361
Typical-Fair       18604.59  -6326.425  43535.61 0.2484556
Good-Fair          38577.47  12718.894  64436.04 0.0004622
Excellent-Fair     97112.47  72679.867 121545.07 0.0000000
Good-Typical       19972.87   7050.230  32895.52 0.0002470
Excellent-Typical  78507.88  68746.678  88269.07 0.0000000
Excellent-Good     58535.00  46602.229  70467.78 0.0000000

$Central_Air
        diff      lwr      upr p adj
Y-N 43256.57 31508.27 55004.87     0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(tukey.ames2, }\AttributeTok{las =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-5-2.pdf}

}

\end{figure}

Starting with the variable for central air, we can see there is a
statistical difference between the two categories. This is the exact
same result as the overall F test for the variable since there are only
two categories. For the heating quality variable, we can see some
categories are different from each other, while others are not.
Noticeably, the combination of poor with fair, good, and typical
categories are \textbf{not} statistically different. Notice also the
different widths of these confidence intervals do to the different
combinations of sample sizes for the categories being tested.

\hypertarget{two-way-anova-with-interactions}{%
\section{Two-Way ANOVA with
Interactions}\label{two-way-anova-with-interactions}}

What if the relationship between a predictor and target variable changed
depending on the value of another predictor variable? For our example,
we would say that the average difference in sales price between having
central air and not having central changed depending on what level of
heating quality the home had. In the bar chart in Figure
Figure~\ref{fig-twomeans}, a potential interaction effect is displayed
when the differences between the two bars within each heating category
is different across heating category. If the difference, was the same,
then there is no interaction present. In other words, no matter the
heating quality rating of the home, the average difference in sales
price between having central air and not having central air is the same.

This interaction model is represented as follows:

\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \varepsilon_{ijk}
\]

with the interaction effect, \((\alpha \beta)_{ij}\), as the
multiplication of the two variables involved in the interaction.
Interactions can occur between more than two variables as well.
Interactions are good to evaluate as they can mask the effects of
individual variables. For example, imagine a hypothetical example as
shown in Figure Figure~\ref{fig-twomeansint} where the impact of having
central air is opposite depending on which category of heating quality
you have.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fake\_HQ }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Poor"}\NormalTok{, }\StringTok{"Poor"}\NormalTok{, }\StringTok{"Excellent"}\NormalTok{, }\StringTok{"Excellent"}\NormalTok{)}
\NormalTok{fake\_CA }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"N"}\NormalTok{, }\StringTok{"Y"}\NormalTok{, }\StringTok{"N"}\NormalTok{, }\StringTok{"Y"}\NormalTok{)}
\NormalTok{fake\_mean }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{100}\NormalTok{)}

\NormalTok{fake\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(fake\_HQ, fake\_CA, fake\_mean))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ fake\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fake\_HQ, }\AttributeTok{y =} \FunctionTok{as.numeric}\NormalTok{(fake\_mean), }\AttributeTok{fill =}\NormalTok{ fake\_CA)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Fake Sales Price (Thousands $)"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Fake Heating Quality Category"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Paired"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/fig-twomeansint-1.pdf}

}

\caption{\label{fig-twomeansint}Distribution of Variables Heating\_QC
and Central\_Air}

\end{figure}

If you were to only look at the average sales price across heating
quality, you would notice no difference between the two groups (average
for both heating categories is 125). However, when the interaction is
accounted for, you can clearly see in the bar heights that sales price
is different depending on the value of central air.

Let's evaluate the interaction term in our actual data. To do so, we
just incorporate the multiplication of the two variables in the model
statement by using the formula
\texttt{Sale\_Price\ \textasciitilde{}\ Heating\_QC\ +\ Central\_Air\ +\ Heating\_QC:Central\_Air}.
You could also use the shorthand version of this by using the formula
\texttt{Sale\_Price\ \textasciitilde{}\ Heating\_QC*Central\_Air}. The
\texttt{*} will include both main effects (the individual variables) and
the interaction between them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_aov\_int }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Heating\_QC}\SpecialCharTok{*}\NormalTok{Central\_Air, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(ames\_aov\_int)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                         Df    Sum Sq   Mean Sq F value   Pr(>F)    
Heating_QC                4 2.891e+12 7.228e+11 147.897  < 2e-16 ***
Central_Air               1 2.903e+11 2.903e+11  59.403 1.99e-14 ***
Heating_QC:Central_Air    4 3.972e+10 9.930e+09   2.032   0.0875 .  
Residuals              2041 9.975e+12 4.887e+09                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

As seen by the output above, the interaction effect between heating
quality and central air is \textbf{not} significant at the 0.05 level.
Again, this implies that the average difference in sale price of the
home between having central air and not does not differ depending on
which category of heating quality the home belongs to. If our
interaction was significant (say a 0.02 p-value instead) then we would
keep it in our model, but here we would remove the interaction term from
our model and rerun the analysis.

\hypertarget{post-hoc-testing-1}{%
\subsection{Post-Hoc Testing}\label{post-hoc-testing-1}}

Post-hoc tests are also available for interaction models as well to
determine where the statistical differences exist in all the
combinations of possible categories. We evaluate these post-hoc tests
using the same \texttt{TukeyHSD} function and its corresponding
\texttt{plot} element.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tukey.ames\_int }\OtherTok{\textless{}{-}} \FunctionTok{TukeyHSD}\NormalTok{(ames\_aov\_int)}
\FunctionTok{print}\NormalTok{(tukey.ames\_int)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = Sale_Price ~ Heating_QC * Central_Air, data = train)

$Heating_QC
                       diff        lwr       upr     p adj
Fair-Poor          49176.42 -63536.973 161889.81 0.7565086
Typical-Poor       67781.01 -42689.104 178251.13 0.4496163
Good-Poor          87753.89 -22928.822 198436.60 0.1936558
Excellent-Poor    146288.89  35929.963 256647.82 0.0027979
Typical-Fair       18604.59  -6301.351  43510.54 0.2474998
Good-Fair          38577.47  12744.901  64410.03 0.0004543
Excellent-Fair     97112.47  72704.440 121520.50 0.0000000
Good-Typical       19972.87   7063.227  32882.52 0.0002425
Excellent-Typical  78507.88  68756.495  88259.26 0.0000000
Excellent-Good     58535.00  46614.230  70455.77 0.0000000

$Central_Air
        diff      lwr      upr p adj
Y-N 43256.57 31520.09 54993.05     0

$`Heating_QC:Central_Air`
                              diff         lwr       upr     p adj
Fair:N-Poor:N            34698.276 -127178.615 196575.17 0.9996249
Typical:N-Poor:N         53419.220 -105046.645 211885.08 0.9876643
Good:N-Poor:N            60760.870 -102472.555 223994.29 0.9755371
Excellent:N-Poor:N       65011.727 -105195.635 235219.09 0.9709555
Poor:Y-Poor:N            56950.000 -214233.733 328133.73 0.9996829
Fair:Y-Poor:N            95114.833  -65743.496 255973.16 0.6876708
Typical:Y-Poor:N         91952.772  -64912.040 248817.58 0.6985070
Good:Y-Poor:N           110062.553  -46997.028 267122.13 0.4435353
Excellent:Y-Poor:N      166351.347    9630.224 323072.47 0.0271785
Typical:N-Fair:N         18720.944  -29117.117  66559.00 0.9659507
Good:N-Fair:N            26062.594  -35761.358  87886.55 0.9454988
Excellent:N-Fair:N       30313.451  -48093.155 108720.06 0.9685428
Poor:Y-Fair:N            22251.724 -202954.108 247457.56 0.9999995
Fair:Y-Fair:N            60416.557    5167.556 115665.56 0.0193847
Typical:Y-Fair:N         57254.496   15021.578  99487.41 0.0007697
Good:Y-Fair:N            75364.278   32413.584 118314.97 0.0000014
Excellent:Y-Fair:N      131653.071   89957.021 173349.12 0.0000000
Good:N-Typical:N          7341.650  -44902.998  59586.30 0.9999894
Excellent:N-Typical:N    11592.508  -59505.300  82690.32 0.9999620
Poor:Y-Typical:N          3530.780 -219235.844 226297.41 1.0000000
Fair:Y-Typical:N         41695.614   -2573.500  85964.73 0.0848888
Typical:Y-Typical:N      38533.553   12248.163  64818.94 0.0001576
Good:Y-Typical:N         56643.334   29219.541  84067.13 0.0000000
Excellent:Y-Typical:N   112932.128   87518.295 138345.96 0.0000000
Excellent:N-Good:N        4250.858  -76919.452  85421.17 1.0000000
Poor:Y-Good:N            -3810.870 -229993.738 222372.00 1.0000000
Fair:Y-Good:N            34353.964  -24751.665  93459.59 0.7089196
Typical:Y-Good:N         31191.903  -15974.214  78358.02 0.5315494
Good:Y-Good:N            49301.684    1491.797  97111.57 0.0369201
Excellent:Y-Good:N      105590.478   58904.465 152276.49 0.0000000
Poor:Y-Excellent:N       -8061.727 -239327.991 223204.54 1.0000000
Fair:Y-Excellent:N       30103.106  -46178.414 106384.63 0.9640522
Typical:Y-Excellent:N    26941.045  -40512.921  94395.01 0.9611660
Good:Y-Excellent:N       45050.826  -22854.846 112956.50 0.5267698
Excellent:Y-Excellent:N 101339.620   34220.481 168458.76 0.0000809
Fair:Y-Poor:Y            38164.833 -186309.978 262639.65 0.9999458
Typical:Y-Poor:Y         35002.772 -186627.795 256633.34 0.9999711
Good:Y-Poor:Y            53112.553 -168655.909 274881.02 0.9990789
Excellent:Y-Poor:Y      109401.347 -112127.544 330930.24 0.8652766
Typical:Y-Fair:Y         -3162.061  -41305.131  34981.01 0.9999999
Good:Y-Fair:Y            14947.720  -23988.593  53884.03 0.9699661
Excellent:Y-Fair:Y       71236.514   33688.745 108784.28 0.0000001
Good:Y-Typical:Y         18109.781    2387.068  33832.49 0.0101482
Excellent:Y-Typical:Y    74398.575   62524.140  86273.01 0.0000000
Excellent:Y-Good:Y       56288.794   42071.027  70506.56 0.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(tukey.ames\_int, }\AttributeTok{las =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-8-2.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-8-3.pdf}

}

\end{figure}

In the giant table above as well as the confidence interval plots, you
are able to inspect which combination of categories are statistically
different.

With interactions present in ANOVA models, post-hoc tests might get
overwhelming in trying to find where differences exist. To help guide
the exploration of post-hoc tests with interactions, we can do
\textbf{slicing}. Slicing is when you perform One-Way ANOVA on subsets
of data within categories of other variables. Even though the
interaction in our model was not significant, let's imagine that it was
for the sake of example. For example, to help discover differences in
the interaction between central air and heating quality, we could subset
the data into two groups - homes with central air and homes without.
Within these two groups we perform One-Way ANOVA across heating quality
to find where differences might exist within subgroups.

This can easily be done with the \texttt{group\_by} function to subset
the data. The \texttt{nest} and \texttt{mutate} functions are also used
to applied the \texttt{aov} function to each subgroup. Here we run a
One-Way ANOVA for heating quality within each subset of central air
being present or not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CA\_aov }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Central\_Air) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{aov =} \FunctionTok{map}\NormalTok{(data, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{summary}\NormalTok{(}\FunctionTok{aov}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Heating\_QC, }\AttributeTok{data =}\NormalTok{ .x))))}

\NormalTok{CA\_aov}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
# Groups:   Central_Air [2]
  Central_Air data                  aov       
  <fct>       <list>                <list>    
1 Y           <tibble [1,904 x 81]> <summry.v>
2 N           <tibble [147 x 81]>   <summry.v>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(CA\_aov}\SpecialCharTok{$}\NormalTok{aov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
              Df    Sum Sq   Mean Sq F value Pr(>F)    
Heating_QC     4 2.242e+12 5.606e+11   108.5 <2e-16 ***
Residuals   1899 9.809e+12 5.165e+09                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

[[2]]
             Df    Sum Sq   Mean Sq F value  Pr(>F)   
Heating_QC    4 1.774e+10 4.435e+09   3.793 0.00582 **
Residuals   142 1.660e+11 1.169e+09                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We can see that both of these results are significant at the 0.05 level.
This implies that there are statistical differences in average sale
price across heating quality within homes that have central air as well
as those that don't have central air.

\hypertarget{assumptions}{%
\subsection{Assumptions}\label{assumptions}}

The assumptions for the \(n\)-Way ANOVA are the same as with the One-Way
ANOVA - independence of observations, normality for each category of
variable, and equal variances. With the inclusion of two or more
variables (\(n\)-Way ANOVA with \(n > 1\)), these assumptions can be
harder to evaluate and test. These assumptions are then applied to the
residuals of the model.

For equal variances, we can still apply the Levene Test for equal
variances using the same \texttt{leveneTest} function as in Chapter
Chapter~\ref{sec-slr}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{leveneTest}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Heating\_QC}\SpecialCharTok{*}\NormalTok{Central\_Air, }\AttributeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Levene's Test for Homogeneity of Variance (center = median)
        Df F value    Pr(>F)    
group    9   24.17 < 2.2e-16 ***
      2041                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

From this test, we can see that we \textbf{do not} meet our assumption
of equal variance.

Let's explore the normality assumption. Again, we will assume this
normality on the random error component, \(\varepsilon_{ijk}\), of the
ANOVA model. More details are found for diagnostic testing using the
error component in Chapter Chapter~\ref{sec-diag}. We can check
normality using the same approaches of the QQ-plot or statistical
testing as in Chapter Section~\ref{sec-eda}. Here we will use the
\texttt{plot} function on the \texttt{aov} object. Specifically, we want
the second plot which is why we have a \texttt{2} in the \texttt{plot}
function option. We then use the \texttt{shapiro.test} function on the
error component. The estimate of the error component is calculated using
the \texttt{residuals} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ames\_aov\_int, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: not plotting observations with leverage one:
  1917
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_res }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(ames\_aov\_int)}

\FunctionTok{shapiro.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ames\_res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  ames_res
W = 0.8838, p-value < 2.2e-16
\end{verbatim}

Neither of the normality or equal variance assumptions appear to be met
here. This would be a good scenario to have a non-parametric approach.
Unfortunately, the Kruskal-Wallis approach is not applicable to
\(n\)-Way ANOVA where \(n > 1\). These approaches would need more
non-parametric versions of multiple regression models to account for
them.

\hypertarget{randomized-block-design}{%
\section{Randomized Block Design}\label{randomized-block-design}}

There are two typical groups of data analysis studies that are
conducted. The first are observational/retrospective studies which are
the typical data problems people try to solve. The primary
characteristic of these analysis are looking at what already happened
(retrospective) and potentially inferring those results to further data.
These studies have little control over other factors contributing to the
target of interest because data is collected after the fact.

The other type of data analysis study are controlled experiments. In
these situations, you often want to look at the outcome measure
prospectively. The focus of the controlled experiment is to control for
other factors that might contribute to the target variable. By
manipulating these factors of interest, one can more reasonably claim
causation. We can more reasonably claim causation when random assignment
is used to eliminate potential \textbf{nuisance factors}. Nuisance
factors are variables that can potentially impact the target variable,
but are not of interest in the analysis directly.

\hypertarget{garlic-bulb-weight-example}{%
\subsection{Garlic Bulb Weight
Example}\label{garlic-bulb-weight-example}}

For this analysis we will use a new dataset. This dataset contains the
average garlic bulb weight from different plots of land. We want to
compare the effects of fertilizer on average bulb weight. However,
different plots of land could have different levels of sun exposure, pH
for the soil, and rain amounts. Since we cannot alter the pH of the soil
easily, or control the sun and rain, we can use blocking to account for
these nuisance factors. Each fertilizer was randomly applied in
quadrants of 8 plots of land. These 8 plots have different values for
sun exposure, pH, and rain amount. Therefore, if we only put one
fertilizer on each plot, we would not know if the fertilizer was the
reason the garlic crop grew or if it was one of the nuisance factors.
Since we \textbf{blocked} these 8 plots and applied all four fertilizers
in each we have essentially accounted for (or removed the effect of) the
nuisance factors.

Let's briefly explore this new dataset by looking at all 32 values using
the \texttt{print} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{block }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"garlic\_block.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 32 Columns: 6
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (6): Sector, Position, Fertilizer, BulbWt, Cloves, BedId

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(block, }\AttributeTok{n =} \DecValTok{32}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 32 x 6
   Sector Position Fertilizer BulbWt Cloves BedId
    <dbl>    <dbl>      <dbl>  <dbl>  <dbl> <dbl>
 1      1        1          3  0.259   11.6 22961
 2      1        2          4  0.207   12.6 23884
 3      1        3          1  0.275   12.1 19642
 4      1        4          2  0.245   12.1 20384
 5      2        1          3  0.215   11.6 20303
 6      2        2          4  0.170   12.7 21004
 7      2        3          1  0.225   12.0 16117
 8      2        4          2  0.168   11.9 19686
 9      3        1          4  0.217   12.4 26527
10      3        2          3  0.226   11.7 23574
# ... with 22 more rows
\end{verbatim}

How do we account for this blocking in an ANOVA model context? This
blocking ANOVA model is the exact same as the Two-Way ANOVA model. The
variable that identifies which sector (block) an observation is in
serves as another variable in the model. Think about this variable as
the variable that accounts for all the nuisance factors in your ANOVA.
That means we have two variables in this ANOVA model - fertilizer and
sector (the block that accounts for sun exposure, pH level of soil, rain
amount, etc.).

For this we can use the same \texttt{aov} function we described above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{block\_aov }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(BulbWt }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(Fertilizer) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(Sector), }\AttributeTok{data =}\NormalTok{ block)}

\FunctionTok{summary}\NormalTok{(block\_aov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   Df   Sum Sq   Mean Sq F value   Pr(>F)    
factor(Fertilizer)  3 0.005086 0.0016954   4.307 0.016222 *  
factor(Sector)      7 0.017986 0.0025695   6.527 0.000364 ***
Residuals          21 0.008267 0.0003937                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Using the \texttt{summary} function we can see that both the sector
(block) and fertilizer variables are significant in the model at the
0.05 level. What are the interpretations of this? First, let's address
the blocking variable. Whether it is significant or not, it should
\textbf{always} be included in the model. This is due to the fact that
the data is structured in that way. It is a construct of the data that
should be accounted for regardless of the significance. However, since
the blocking variable (sector) was significant, that implies that
different plots of land have different impacts of the average bulb
weight of garlic. Again, this is most likely due to the differences
between the plots of land - namely sun exposure, pH of soil, rain fall,
etc.

Second, the variable of interest is the fertilizer variable. It is
significant, implying that there is a difference in the average bulb
weight of garlic for different fertilizers. To examine which fertilizer
pairs are statistically difference we can use post-hos testing as
described in the previous parts of this chapter using the
\texttt{TukeyHSD} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tukey.block }\OtherTok{\textless{}{-}} \FunctionTok{TukeyHSD}\NormalTok{(block\_aov)}
\FunctionTok{print}\NormalTok{(tukey.block)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = BulbWt ~ factor(Fertilizer) + factor(Sector), data = block)

$`factor(Fertilizer)`
           diff         lwr          upr     p adj
2-1 -0.02509875 -0.05275125  0.002553751 0.0840024
3-1 -0.01294875 -0.04060125  0.014703751 0.5698678
4-1 -0.03336125 -0.06101375 -0.005708749 0.0144260
3-2  0.01215000 -0.01550250  0.039802501 0.6186232
4-2 -0.00826250 -0.03591500  0.019390001 0.8382800
4-3 -0.02041250 -0.04806500  0.007240001 0.1995492

$`factor(Sector)`
          diff          lwr           upr     p adj
2-1 -0.0520675 -0.099126544 -5.008456e-03 0.0234315
3-1 -0.0145075 -0.061566544  3.255154e-02 0.9634255
4-1 -0.0450550 -0.092114044  2.004044e-03 0.0669646
5-1 -0.0616250 -0.108684044 -1.456596e-02 0.0051483
6-1 -0.0196650 -0.066724044  2.739404e-02 0.8466335
7-1  0.0084950 -0.038564044  5.555404e-02 0.9984089
8-1 -0.0393325 -0.086391544  7.726544e-03 0.1469768
3-2  0.0375600 -0.009499044  8.461904e-02 0.1841786
4-2  0.0070125 -0.040046544  5.407154e-02 0.9995370
5-2 -0.0095575 -0.056616544  3.750154e-02 0.9966777
6-2  0.0324025 -0.014656544  7.946154e-02 0.3337758
7-2  0.0605625  0.013503456  1.076215e-01 0.0061094
8-2  0.0127350 -0.034324044  5.979404e-02 0.9819446
4-3 -0.0305475 -0.077606544  1.651154e-02 0.4025951
5-3 -0.0471175 -0.094176544 -5.845586e-05 0.0495704
6-3 -0.0051575 -0.052216544  4.190154e-02 0.9999400
7-3  0.0230025 -0.024056544  7.006154e-02 0.7227812
8-3 -0.0248250 -0.071884044  2.223404e-02 0.6454690
5-4 -0.0165700 -0.063629044  3.048904e-02 0.9286987
6-4  0.0253900 -0.021669044  7.244904e-02 0.6208608
7-4  0.0535500  0.006490956  1.006090e-01 0.0186102
8-4  0.0057225 -0.041336544  5.278154e-02 0.9998793
6-5  0.0419600 -0.005099044  8.901904e-02 0.1034664
7-5  0.0701200  0.023060956  1.171790e-01 0.0012997
8-5  0.0222925 -0.024766544  6.935154e-02 0.7514897
7-6  0.0281600 -0.018899044  7.521904e-02 0.5004099
8-6 -0.0196675 -0.066726544  2.739154e-02 0.8465530
8-7 -0.0478275 -0.094886544 -7.684559e-04 0.0446174
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(tukey.block, }\AttributeTok{las =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-14-2.pdf}

}

\end{figure}

\hypertarget{assumptions-1}{%
\subsection{Assumptions}\label{assumptions-1}}

Outside of the typical assumptions for ANOVA that still hold here, there
are two additional assumptions to be met: - Treatments are randomly
assigned within each block - The effects of the treatment variable are
constant across the levels of the blocking variable

The first, new assumption of randomness deals with the reliability of
the analysis. Randomness is key to removing the impact of the nuisance
factors. The second, new assumption implies there is \textbf{no
interaction} between the treatment variable and the blocking variable.
For example, we are implying that the fertilizers' impacts ob garlic
bulb weight are not changed depending on what block you are on. In other
words, fertilizers have the same impact regardless of sun exposure, pH
levels, rain fall, etc. We are \textbf{not} saying these nuisance
factors do not impact the target variable or bulb weight of garlic, just
that they do not change the effect of the fertilizer on bulb weight.

\hypertarget{multiple-linear-regression}{%
\section{Multiple Linear Regression}\label{multiple-linear-regression}}

Most practical applications of of regression modeling involve using more
complicated models than a simple linear regression with only one
predictor variable to predict your target. Additional variables in a
model can lead to better explanations and predictions of the target.
These linear regressions with more than one variable are called
\textbf{multiple linear regression} models. However, as we will see in
this section and the following chapters, with more variables comes much
more complication.

\hypertarget{model-structure}{%
\subsection{Model Structure}\label{model-structure}}

Multiple linear regression models have the same structure as simple
linear regression models, only with more variables. The multiple linear
regression model with \(k\) variables is structured like the following:

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
\]

This model has the predictor variables \(x_1, x_2, ..., x_k\) trying to
either explain or predict the target variable \(y\). The intercept,
\(\beta_0\), still gives the expected value of \(y\), when \textbf{all}
of the predictor variables take a value of 0. With the addition of
multiple predictors, the interpretation of the slope coefficients change
slightly. The slopes, \(\beta_1, \beta_2, ..., \beta_k\), give the
expected change in \(y\) for a one unit change in the respective
predictor variable, \textbf{holding all other predictor variables
constant}. The random error term, \(\varepsilon\), is the error between
our predicted value,
\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \cdots + \hat{\beta}_k x_k\),
and our actual value of \(y\).

Unlike simple linear regression that can be visualized as a line through
a 2-dimensional scatterplot of data, a multiple linear regression is
better thought of as a multi-dimensional plane through a
multi-dimensional scatterplot of data.

Let's visual an example with two predictor variables - the square
footage of greater living area and the total number of rooms. These will
predict sale price of a home. When none of the variables have any
relationship with the target variable, we get a horizontal plane like
the one shown below. This is similar in concept to a horizontal line in
simple linear regression having a slope of 0, implying that the target
variable does not change as the predictor variable changes.

Much like if the slope of a simple linear regression line is
\textbf{not} 0 (a relationship exists between the predictor and target
variable), then a relationship between any of the predictor variables
and the target variable shifts and rotates the plane around like the one
shown below.

To the naive viewer, the shifted plane would still make sense because of
the model naming convention of multiple \textbf{linear} regression.
However, the \textbf{linear} in linear regression doesn't have to deal
with the visualization of the fitted plane (or line in two dimensions),
but instead refers to the \textbf{linear combination of variables}. A
linear combination is an expression constructed from a set of terms by
multiplying each term by a constant and adding the results. For example,
\(ax + by\) is a linear combination of \(x\) and \(y\). Therefore, the
linear model

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
\] is a linear combination of predictor variables in their relationship
with the target variable \(y\). These predictor variables do not all
have to contain linear effects though. For example, let's look at a
linear regression model with four predictor variables:

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon
\]

One would not be hard pressed to call this model a linear regression.
However, what if we defined \(x_3 = x_1^2\) and \(x_4 = x_2^2\)?

This model is still a linear regression model. The structure of the
model did not change. The model is still a linear combination of
predictor variables related to the target variable. The predictor
variables just do not all have a linear effect in terms of their
relationship with \(y\). However, mathematically, it is still a linear
combination and a linear regression model. Logistic regression on the
other hand, which will be covered in Chapter Chapter~\ref{sec-cat} is an
example of a \textbf{nonlinear} regression model.

\hypertarget{global-local-inference}{%
\subsection{Global \& Local Inference}\label{global-local-inference}}

In simple linear regression we could just look at the t-test for our
slope parameter estimate to determine the utility of our model. With
multiple parameter estimates comes multiple t-tests. Instead of looking
at every individual parameter estimate initially, there is a way to
determine the model adequacy for predicting the target variable overall.

The utility of a multiple regression model can be tested with a single
test that encompasses all the coefficients from the model. This kind of
test is called a \textbf{global test} since it tests all \(\beta\)'s
simultaneously. The \textbf{Global F-Test} uses the F-distribution to do
just that for multiple linear regression models. The hypotheses for this
test are the following:

\[
H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0 \\
H_A: \text{at least one } \beta \text{ is nonzero}
\]

In simpler terms, the null hypothesis is that none of the variables are
useful in predicting the target variable. The alternative hypothesis is
that \textbf{at least one} of these variables is useful in predicting
the target variable.

The F-distribution is a distribution that has the following
characteristics: - Bounded below by 0 - Right-skewed - Both
\textbf{numerator} and \textbf{denominator} degrees of freedom

A plot of a variety of F distributions is shown here:

\begin{verbatim}
Warning: Removed 1500 rows containing missing values (`geom_line()`).
\end{verbatim}

\includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-18-1.pdf}

If the global test is significant, the next step would be to examine the
individual t-tests to see which variables are significant and which ones
are not. This is similar to post-hoc testing in ANOVA where we explored
which of the categories was statistically different when we knew at
least one was.

These tests are all available using the \texttt{summary} function on an
\texttt{lm} function for linear regression. To build a multiple linear
regression in R using the \texttt{lm} function, you just add another
variable to the formula element. Here we will predict the sales price
(\texttt{Sale\_Price}) based on the square footage of the greater living
area of the home (\texttt{Gr\_Liv\_Area}) as well as total number of
rooms above ground (\texttt{TotRms\_AbvGrd}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lm2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area }\SpecialCharTok{+}\NormalTok{ TotRms\_AbvGrd, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(ames\_lm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-528656  -30077   -1230   21427  361465 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)    42562.657   5365.721   7.932 3.51e-15 ***
Gr_Liv_Area      136.982      4.207  32.558  < 2e-16 ***
TotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 56630 on 2048 degrees of freedom
Multiple R-squared:  0.5024,    Adjusted R-squared:  0.5019 
F-statistic:  1034 on 2 and 2048 DF,  p-value: < 2.2e-16
\end{verbatim}

At the bottom of the above output is the result of the global F-test.
Since the p-value on this test is lower than the significance level of
0.05, we have statistical evidence that at least of the two variables -
\texttt{Gr\_Liv\_Area} and \texttt{TotRms\_AbvGrd} - is significant at
predicting the sale price of the home. By looking at the individual
t-tests in the output above, we can see that both variables are actually
significant.

\hypertarget{assumptions-2}{%
\subsection{Assumptions}\label{assumptions-2}}

The \textbf{main} assumptions for the multiple linear regression model
are the same as with the simple linear regression model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The expected value of \(y\) is linear in the \(x\)'s (proper model
  specification).
\item
  The random errors are independent.
\item
  The random errors are normally distributed.
\item
  The random errors have equal variance (homoskedasticity).
\end{enumerate}

However, with multiple variables there is an additional assumption that
people tend to add to multiple linear regression modeling: 5. No
\textbf{perfect} collinearity (also called multicollinearity)

The new assumption means that no combination of predictor variables is a
perfect linear combination with any other predictor variables.
Collinearity, also called multicollinearity, occurs when predictor
variables are correlated with each other. People often misstate this
additional assumption as having no collinearity at all. This is too
restrictive and basically impossible to meet in a realistic setting.
Only when collinearity has a drastic impact on the linear regression do
we need to concern ourselves. In fact, linear regression only completely
breaks when that collinearity is perfect. Dealing with multicollinearity
is the subject of Chapter Chapter~\ref{sec-diag}.

Similar to simple linear regression, we can evaluate the assumptions by
looking at residual plots. The \texttt{plot} function on the \texttt{lm}
object provides these.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ames\_lm2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-complex_ANOVA_Regression_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These will again be covered in much more detail in Chapter
Chapter~\ref{sec-diag}.

\hypertarget{multiple-coefficients-of-determination}{%
\subsection{Multiple Coefficients of
Determination}\label{multiple-coefficients-of-determination}}

One of the main advantages of multiple linear regression is that the
complexity of the model enables us to investigate the relationship among
\(y\) and several predictor variables simultaneously. However, this
increased complexity makes it more difficult to not only interpret the
models, but also ascertain which model is ``best.''

One example of this would be the coefficient of determination, \(R^2\),
that we discussed in Chapter Chapter~\ref{sec-slr}. The calculation for
\(R^2\) is the exact same:

\[
R^2 = 1 - \frac{SSE}{TSS}
\]

However, the problem with the calculation of \(R^2\) in a multiple
linear regression is that the addition of any variable (useful or not)
will never make the \(R^2\) decrease. In fact, it typically increases
even with the addition of a useless variable. The reason is rather
intuitive. When adding information to a regression model, your
predictions can only get better, not worse. If a new predictor variable
has no impact on the target variable, then the predictions can not get
any worse than what they already were before the addition of the useless
variable. Therefore, the \(SSE\) would never increase, making the
\(R^2\) never decrease.

To account for this problem, there is the \textbf{adjusted coefficient
of determination}, \(R^2_a\). The calculation is the following:

\[
R^2_a = 1 - [(\frac{n-1}{n-(k+1)})\times (\frac{SSE}{TSS})]
\]

Notice what the calculation is doing. It takes the original ratio on the
right hand side of the \(R^2\) equation, \(SSE/TSS\), and penalizes it.
It multiplies this number by a ratio that is always greater than 1 if
\(k > 0\). Remember, \(k\) is the number of variables in the model.
Therefore, as the number of variables increases, the calculation
penalizes the model more and more. However, if the reduction of SSE from
adding a useful variable is low enough, then even with the additional
penalization, the \(R^2_a\) will increase \textbf{if the variable is a
useful addition to the model}. If the variable is not a useful addition
to the model, the \(R^2_a\) will decrease. The \(R^2_a\) is only one of
many ways to select the ``best'' model for multiple linear regression.
Many more metrics will be discussed in model selection in Chapter
Chapter~\ref{sec-sel}.

One downside of this new metric is that the \(R^2_a\) loses its
interpretation. Since \(R^2_a \le R^2\), it is no longer bounded below
by zero. Therefore, it can no longer be the proportion of variation
explained in the target variable by the model. However, we can easily
use \(R^2_a\) to select a model correctly and interpret that model with
\(R^2\). Both of these numbers can be found using the \texttt{summary}
function on the \texttt{lm} object from the previous model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ames\_lm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-528656  -30077   -1230   21427  361465 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)    42562.657   5365.721   7.932 3.51e-15 ***
Gr_Liv_Area      136.982      4.207  32.558  < 2e-16 ***
TotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 56630 on 2048 degrees of freedom
Multiple R-squared:  0.5024,    Adjusted R-squared:  0.5019 
F-statistic:  1034 on 2 and 2048 DF,  p-value: < 2.2e-16
\end{verbatim}

From this output we can say that the combination of
\texttt{Gr\_Liv\_Area} and \texttt{TotRmsAbvGrd} account for 50.24\% of
the variation in \texttt{Sale\_Price}. Now let's add a random variable
to the model. This random variable will take random values from a normal
distribution with mean of 0 and standard deviation of 1 and has no
impact on the target variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{ames\_lm3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area }\SpecialCharTok{+}\NormalTok{ TotRms\_AbvGrd }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(Sale\_Price), }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(ames\_lm3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 
    0, 1), data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-527926  -29943   -1298   21427  363925 

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)                      42589.091   5364.877   7.939 3.34e-15 ***
Gr_Liv_Area                        136.927      4.207  32.548  < 2e-16 ***
TotRms_AbvGrd                   -10552.425   1369.808  -7.704 2.05e-14 ***
rnorm(length(Sale_Price), 0, 1)   1629.854   1259.478   1.294    0.196    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 56620 on 2047 degrees of freedom
Multiple R-squared:  0.5028,    Adjusted R-squared:  0.502 
F-statistic: 689.9 on 3 and 2047 DF,  p-value: < 2.2e-16
\end{verbatim}

Notice that the \(R^2\) of this model actually increased to 0.5028 from
0.5024. However, the \(R^2_a\) value stayed approximately the same at
0.502 since the addition of this new variable did not provide enough
predictive power to outweigh the penalty of adding it.

\hypertarget{categorical-predictor-variables}{%
\subsection{Categorical Predictor
Variables}\label{categorical-predictor-variables}}

As mentioned in Chapter Section~\ref{sec-eda}, there are two types of
variables typically used in modeling: - Quantitative (or numeric) -
Qualitative )or categorical)

Categorical variables need to be coded differently because they are not
numerical in nature. As mentioned in Chapter Section~\ref{sec-eda}, two
common coding techniques for linear regression are \textbf{reference}
and \textbf{effects} coding. The interpretation of the coefficients
(\(\beta\)'s) of these variables in a regression model depend on the
specific coding used. The predictions from the model, however, will
remain the same regardless of the specific coding that is used.

Let's use the example of the \texttt{Central\_Air} variable with 2
categories - Y and N. Using reference coding, the \textbf{reference}
coded variable to describe these 2 categories (with N as the reference
level) would be the following:

\hypertarget{tbl-centralair}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-centralair}Reference variable coding for the
categorical attribute \emph{Central Air}}\tabularnewline
\toprule()
\endhead
Central Air & X1 \\
Y & 1 \\
N & 0 \\
\bottomrule()
\end{longtable}

The linear regression equation would be:

\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
\]

Let's see the mathematical interpretation of the coefficient
\(\hat{\beta}_1\). To do this, let's get the average sale price of a
home prediction for a home with central air (\(\hat{y}_Y\)) and without
central air (\(\hat{y}_N\)):

\[
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot 0 = \hat{\beta}_0
\]

By subtracting these two equations
(\(\hat{y}_Y - \hat{y}_N = \hat{\beta}_1\)), we can get the prediction
for the average difference in price between a home with central air and
without central air. This shows that in reference coding, the
coefficient on each dummy variable is the average difference between
that category and the reference category (the category not represented
with its own variable). The math can be extended to as many categories
as needed.

Using effects coding, the \textbf{effects} coded variable to describe
these 2 categories (with N as the reference level) would be the
following:

\hypertarget{tbl-centralaireff}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-centralaireff}Effects variable coding for the
categorical attribute \emph{Central Air}}\tabularnewline
\toprule()
\endhead
Central Air & X1 \\
Y & 1 \\
N & -1 \\
\bottomrule()
\end{longtable}

The linear regression equation would be:

\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
\]

Let's see the mathematical interpretation of the coefficient
\(\hat{\beta}_1\). To do this, let's get the average sale price of a
home prediction for a home with central air (\(\hat{y}_Y\)) and without
central air (\(\hat{y}_N\)):

\[
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot -1 = \hat{\beta}_0 - \hat{\beta}_1
\]

Similar to reference coding, the coefficient \(\hat{\beta}_1\) is the
average difference between homes with central air and \(\hat{\beta}_0\).
However, what is \(\hat{\beta}_0\)? By taking the average of our two
predictions:

\[
\frac{1}{2} \times (\hat{y}_Y + \hat{y}_N) = \frac{1}{2} \times (\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_0 - \hat{\beta}_1) = \frac{1}{2} \times (2\hat{\beta}_0) = \hat{\beta}_0
\]

From this average we can get the prediction for the average difference
in price between a home with central air and the average price across
all homes. This shows that in effects coding, the coefficient on each
dummy variable is the average difference between that category and the
average price across \textbf{all} homes (including both with and without
central air). The math can be extended to as many categories as needed.

Let's see an example with \texttt{Central\_Air} as a variable added to
our multiple linear regression model as a reference coded variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lm4 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area }\SpecialCharTok{+}\NormalTok{ TotRms\_AbvGrd }\SpecialCharTok{+}\NormalTok{ Central\_Air, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(ames\_lm4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Central_Air, 
    data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-510745  -28984   -2317   20273  356742 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -7169.259   6778.879  -1.058     0.29    
Gr_Liv_Area     129.594      4.131  31.374  < 2e-16 ***
TotRms_AbvGrd -8980.938   1335.669  -6.724 2.29e-11 ***
Central_AirY  54513.082   4762.926  11.445  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 54910 on 2047 degrees of freedom
Multiple R-squared:  0.5323,    Adjusted R-squared:  0.5316 
F-statistic: 776.6 on 3 and 2047 DF,  p-value: < 2.2e-16
\end{verbatim}

With these results we estimate the average difference in sales price
between homes with central air and without central air to be
\$54,513.08.

\bookmarksetup{startatroot}

\hypertarget{sec-sel}{%
\chapter{Model Selection}\label{sec-sel}}

This section will dive into some basic foundations in model selection,
or finding the best model for a data set. In most data sets, there will
most likely be variables that are informative and ones that are
uninformative in predicting the response. With many explanatory
variables, it could be extremely time consuming to try all potential
models by hand, and the use of automatic procedures can greatly assist
in obtaining subsets of variables in which to focus your attention.\\

This Chapter aims to answer the following questions:

What are the different selection criteria that can be used in model
selection?

How to perform a stepwise search algorithm.

Forward Selection

Backward Elimination

Stepwise Selection

Considerations when looking at p-values.

CAUTION: you should NEVER just use the final model created from an
automatic procedure! \emph{Always} explore your data (both automatically
selected \emph{and} excluded variables), and use domain knowledge,
diagnostics, and critical thought to decide on your final model.

We will focus on two techniques for automatic variable selection:
stepwise procedures and LASSO. Within the stepwise procedures, we will
discuss forward, backward and stepwise searches using several different
selection criteria. We will end this section discussing important
considerations in use of p-values when dealing with large data sets.

\hypertarget{selection-criteria}{%
\section{Selection Criteria}\label{selection-criteria}}

When trying to find the best model, there are many selection criteria at
our disposal. For example, you have already been introduced to \(R^{2}\)
(the larger the value of \(R^{2}\), the better the model). However, when
comparing multivariate models, the adjusted \(R^{2}\) is better due to
the fact that \(R^{2}\) can potentially increase even when adding noise.
The adjusted \(R^{2}\) can be thought of as \(R^{2}\) with penalty (for
every additional variable added to the model, we add a penalty). This
allows us to weigh the contribution of adding new variables against the
added complexity of more variables in the model. There are other
selection criteria that are also used in selecting the ``best model''
(or variable selection). As you will notice, these selection criteria
also have a penalty to take into account the addition of variables. We
will use two of the most common ones: AIC and BIC (can also be referred
to as SBC).

The AIC, or \textbf{Akaike Information Criterion}, was developed by
statistician Hirotugu Akaike in the 1970's and is defined by
\[ AIC = -2log(Likelihood) + 2p.  \] In this case, ``Likelihood'' is the
likelihood of the data and \(2p\) is the ``penalty'', where \(p\) is the
number of parameters in the model. A smaller AIC indicates a better
model.

BIC, also known as the \textbf{Bayesian Information Criterion} (also
called SBC or \textbf{Schwarz Bayesian Information}) was first developed
by Gideon E. Schwarz, also back in the 1970's and is defined by
\[BIC = -2log(Likelihood) + plog(n). \] In this case, ``Likelihood'' is
the likelihood of the data and \(plog(n)\) is the ``penalty'', where
\(p\) is the number of parameters in the model and \(n\) is the sample
size. A smaller BIC indicates a better model. Notice that both penalties
have a common term of \(p\) in them (this will be important to remember
when we start discussing the R code).

\hypertarget{stepwise-selection}{%
\section{Stepwise Selection}\label{stepwise-selection}}

The three different algorithms in the stepwise selection search are
forward, backward and stepwise. Each of these algorithms either add or
take away one variable at a time based on a given criterion until this
criterion can no longer be met. At which point the algorithm stops.

\hypertarget{forward}{%
\subsection*{Forward}\label{forward}}
\addcontentsline{toc}{subsection}{Forward}

For forward selection, we start with a null model (only the intercept)
and add one variable at a time until no other variables can be added
based on a given criterion. The algorithm is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  Start with a null model, this is the base model (just the intercept)\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    For each variable not in model, create a linear regression model
    with the base model plus this one variable\\
  \item
    See which linear regression is best (based on criterion)\\
  \item
    Is this regression better than the base model?\\

    \begin{enumerate}
    \def\labelenumiii{\alph{enumiii}.}
    \tightlist
    \item
      Yes, then continue on to step 4\\
    \item
      No, exit the algorithm with the base model as the chosen model\\
    \end{enumerate}
  \item
    The base model is now the previous base model plus the variable
    selected in step 3. Using this as your new base model, go back to
    step 1 and continue.\\
  \end{enumerate}
\end{enumerate}

To do forward selection in R, you will use the step function. The
``empty model'' should be used as your initial model (which is just the
intercept). For the scope of the model, you need to put the ``smallest
model'' (just the intercept) to the ``largest model'' (full model). The
direction is forward. The penalty can be controlled by defining
``\(k\)''. Using a value of 2 for ``\(k\)'' will use the AIC criterion
(remember AIC penalty was \(2p\)) and using \(log(n)\) for ``\(k\)''
will use the BIC criterion (remember BIC penalty was \(plog(n)\)). If
you do not specify anything for ``\(k\)'', the default is AIC. You can
also define ``\(k\)'' as the upper \(\alpha\)-quantile of a \(\chi^{2}\)
distribution with one degree of freedom, which will use p-value for its
selection of variables. In this case, the best ``model'' is the one with
the lowest p-value for the new variable. In order to enter the model,
the p-value of this variable will need to be lower than the ``cut-off''
for the p-values (\(\alpha\)), which means that the criterion to enter
the model would be that the variable has a p-value smaller than
\(\alpha\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create full model and empty model}
\NormalTok{full.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . , }\AttributeTok{data =}\NormalTok{ train\_sel)}
\NormalTok{empty.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train\_sel)}

\CommentTok{\# k = 2 for AIC selection}
\NormalTok{for.model }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(empty.model,}
                  \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                               \AttributeTok{upper =}\NormalTok{ full.model),}
                  \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{, }\AttributeTok{k =} \DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Below is edited output with comments to lead you through the forward
selection. Notice that the above code has \(k\)=2, which means we are
using AIC as our selection criterion. The first information R provides
is for the initial model, which is
\[Y_{i}=\beta_{0} + \varepsilon_{i}.\] The ``empty model'' has an AIC of
46,323.64 (in order for ANY variable to be added, that model must be
better, or in other words have a smaller AIC than this initial model).

\begin{verbatim}
Start:  AIC=46323.64
Sale_Price ~ 1
\end{verbatim}

R then displays all simple linear regressions with their corresponding
AIC values. Notice that R puts the information in ascending AIC values
(so, the best simple linear regression is 43,817 which is indeed smaller
than 46,323.64). You will also see a line with a blank for the
``variable'', this is the current ``base model''. In this first step, it
is just the intercept. Therefore, we will add the variable
Overall\_Qual.

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
+Overall\_Qual & 9 & 9.3437e+12 & 3.8531e+12 & 43817 \\
+Gr\_Liv\_Area & 1 & 6.4389e+12 & 6.7578e+12 & 44953 \\
+Garage\_Area & 1 & 5.3561e+12 & 7.8407e+12 & 45258 \\
+First\_Flr\_SF & 1 & 4.8867e+12 & 8.3100e+12 & 45377 \\
+Full\_Bath & 1 & 3.7827e+12 & 9.4141e+12 & 45633 \\
+TotRms\_AbvGrd & 1 & 3.2304e+12 & 9.9663e+12 & 45750 \\
+Fireplaces & 1 & 2.9715e+12 & 1.0225e+13 & 45802 \\
+Half\_Bath & 1 & 1.1209e+12 & 1.2076e+13 & 46144 \\
+Roof\_Style & 5 & 1.0724e+12 & 1.2124e+13 & 46160 \\
+Central\_Air & 1 & 9.6147e+11 & 1.2235e+13 & 46170 \\
+House\_Style & 7 & 1.0245e+12 & 1.2172e+13 & 46172 \\
+Second\_Flr\_SF & 1 & 9.4611e+11 & 1.2251e+13 & 46173 \\
+Lot\_Area & 1 & 9.0332e+11 & 1.2293e+13 & 46180 \\
+Bldg\_Type & 4 & 4.6434e+11 & 1.2732e+13 & 46258 \\
+Street & 1 & 3.1752e+10 & 1.3165e+13 & 46321 \\
& & 1.3197e+13 & 46324 & \\
\bottomrule()
\end{longtable}

Now, R shows you the new ``base model'', which includes Overall\_Qual
(and of course the intercept is still included) and the new AIC value to
beat which is 43816.66.

\begin{verbatim}
Step:  AIC=43816.66
Sale_Price ~ Overall_Qual
\end{verbatim}

Using this as our base model, we now try adding each of the remaining
variables in separate regression models, output is shown below:

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
+Gr\_Liv\_Area & 1 & 9.8905e+11 & 2.8640e+12 & 43210 \\
+First\_Flr\_SF & 1 & 5.2665e+11 & 3.3264e+12 & 43517 \\
+Garage\_Area & 1 & 4.6644e+11 & 3.3866e+12 & 43554 \\
+TotRms\_AbvGrd & 1 & 4.6123e+11 & 3.3918e+12 & 43557 \\
+Full\_Bath & 1 & 4.1206e+11 & 3.4410e+12 & 43587 \\
+Fireplaces & 1 & 4.0551e+11 & 3.4476e+12 & 43591 \\
+Lot\_Area & 1 & 3.8148e+11 & 3.4716e+12 & 43605 \\
+Bldg\_Type & 4 & 2.3715e+11 & 3.6159e+12 & 43694 \\
+Second\_Flr\_SF & 1 & 1.7555e+11 & 3.6775e+12 & 43723 \\
+Half\_Bath & 1 & 1.3948e+11 & 3.7136e+12 & 43743 \\
+Central\_Air & 1 & 9.1322e+10 & 3.7617e+12 & 43769 \\
+House\_Style & 7 & 6.1815e+10 & 3.7912e+12 & 43797 \\
+Roof\_Style & 5 & 5.1448e+10 & 3.8016e+12 & 43799 \\
& & & 3.8531e+12 & 43817 \\
+Street & 1 & 1.9573e+06 & 3.8531e+12 & 43819 \\
\bottomrule()
\end{longtable}

Best model includes Gr\_Liv\_Area (in addition to the intercept and
Overall\_Qual). The AIC is 43210, which beats the previous one of 43817.
So, we will add Gr\_Liv\_Area to the model. Our new base model includes
the intercept, Overall\_Qual and Gr\_Liv\_Area with an AIC of 43210.

\begin{verbatim}
Step:  AIC=43210.24
Sale_Price ~ Overall_Qual + Gr_Liv_Area
\end{verbatim}

We continue in this fashion until adding a new variable does NOT
decrease the AIC. The last step is shown below:

\begin{verbatim}
Step:  AIC=42676.1
Sale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + Bldg_Type + Fireplaces 
        + Full_Bath + Half_Bath + Lot_Area + Roof_Style + Central_Air + Second_Flr_SF 
        + TotRms_AbvGrd + First_Flr_SF
\end{verbatim}

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
& & & 2.1542e+12 & 42676 \\
+Street & 1 & 1.028e+09 & 2.1532e+12 & 42677 \\
\bottomrule()
\end{longtable}

Notice that the previous step had a model with the intercept,
Overall\_Qual, Gr\_Liv\_Area, House\_Style, Garage\_Area, Bldg\_Type,
Fireplaces, Full\_Bath, Half\_Bath, Lot\_Area, Roof\_Style,
Central\_Air, Second\_Flr\_SF, TotRms\_AbvGrd, and First\_Flr\_SF (as
illustrated by the above formula). The AIC for this model is 42676.1.
There is only one variable left that could be added (Street). However,
when we add street to the model, the AIC now becomes 42677 (in other
words AIC increases, which is a worse model). The algorithm stops here.

The code below illustrates how to do forward selection with BIC and
p-values. In the output from R, it still says ``AIC'', but now these
values are calculated by using the BIC formula or the corresponding
``\(k\)'' values for p-values (take a look at the values R has for the
``AIC'' column and you will see that it has changed!).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# k = log(n) for BIC selection}
\NormalTok{for.model2 }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(empty.model,}
                   \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                               \AttributeTok{upper =}\NormalTok{ full.model),}
                   \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{log}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(train\_sel))) }\CommentTok{\# k = qchisq(alpha, 1, lower.tail = FALSE) for p{-}value with alpha selection}
\NormalTok{alpha.f}\OtherTok{=}\FloatTok{0.05}
\NormalTok{for.model3 }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(empty.model,}
                   \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                                \AttributeTok{upper =}\NormalTok{ full.model),}
                   \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{qchisq}\NormalTok{(alpha.f, }\DecValTok{1}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\hypertarget{backward}{%
\subsection*{Backward}\label{backward}}
\addcontentsline{toc}{subsection}{Backward}

Backward elimination starts with the ``base'' model as the full model
(i.e.~all variables are contained within the model). We remove only one
variable and we do this for each variable in the model. We want to see
if the any of the models improve over the ``base model'' (in other
words, is the model better with that one variable removed based on a
given criterion?). If this new model (with that one variable removed) is
better than the previous model, then we remove that variable and this
new model now becomes the base model. We are looking for the best
improvement, therefore we look at the model that would be the best over
the base model. The algorithm continues in this fashion until no other
variables can be removed, based on the chosen criterion.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  Start with full model with all predictor variables in it, this is the
  base model and calculate the criterion on this model\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Create models such that each model has exactly one predictor
    variable removed from it and calculate the criterion for each
    model\\
  \item
    In step 1, find the best model based on the criterion\\
  \item
    Is this regression model better than the base model?\\

    \begin{enumerate}
    \def\labelenumiii{\alph{enumiii}.}
    \tightlist
    \item
      Yes, then continue on to step 4\\
    \item
      No, exit the algorithm with the base model as the chosen model\\
    \end{enumerate}
  \item
    The base model is now the model with the variable removed. Using
    this as your new base model, go back to step 1 and continue.\\
  \end{enumerate}
\end{enumerate}

To do backward elimination in R, you will use the step function. The
``full model'' should be used as your initial model (which is the model
with all the predictor variables in it). For the scope of the model, you
need to put the ``smallest model'' (just the intercept) to the ``largest
model'' (full model). The direction is backward. The penalty (which
indicates the criteria) can be controlled by defining ``\(k\)''. As
discussed before, a value of 2 will use the AIC criterion, a value of
\(log(n)\) will produce the BIC penalty, and finally the upper
\(\alpha\)-quantile of a \(\chi^{2}\) distribution with one degree of
freedom will use p-value for its removal of variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create full model and empty model}
\NormalTok{full.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . , }\AttributeTok{data =}\NormalTok{ train\_sel)}
\NormalTok{empty.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train\_sel)}

\CommentTok{\# k = 2 for AIC selection}
\NormalTok{back.model }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(full.model,}
                  \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                               \AttributeTok{upper =}\NormalTok{ full.model),}
                  \AttributeTok{direction =} \StringTok{"backward"}\NormalTok{, }\AttributeTok{k =} \DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Below is edited output with comments to lead you through the backward
elimination. Notice that the above code has \(k\)=2, which means we are
using AIC as our selection criterion. The first information R provides
is for the initial model, which is
\[Y_{i}=\beta_{0} + \beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\beta_{3}x_{i,3}+\beta_{4}x_{i,4}+\beta_{5}x_{i,5}+\beta_{6}x_{i,6}+\beta_{i,7}x_{7}+\beta_{8}x_{i,8}+\beta_{9}x_{i,9}+\beta_{10}x_{i,10}+\beta_{11}x_{i,11}+\beta_{12}x_{i,12}+\beta_{13}x_{i,13}+\beta_{14}x_{i,14}+\beta_{15}x_{i,15}+\varepsilon_{i}.\]
The ``full model'' has an AIC of 42,677.12 (in order for ANY variable to
be removed, that model must be better, or in other words have a smaller
AIC than this initial model).

R then displays linear regression models with one variable removed and
their corresponding AIC values. For example, if we remove Street (just
that one variable), the AIC of that model will be 42675. Notice that R
put the information in ascending AIC values (so, best linear regression
is at the top with an AIC value of 42,675 which is indeed smaller than
42,677.12, so Street will be removed). You will also see a line with .
This is the ``base model'', or in this case the full model.

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
- Gr\_Liv\_Area & 1 & 4.9138e+08 & 2.1537e+12 & 42676 \\
- Street & 1 & 1.0280e+09 & 2.1542e+12 & 42676 \\
& & & 2.1532e+12 & 42677 \\
- First\_Flr\_SF & 1 & 3.1548e+09 & 2.1563e+12 & 42678 \\
- TotRms\_AbvGrd & 1 & 3.4112e+09 & 2.1566e+12 & 42678 \\
- Second\_Flr\_SF & 1 & 6.4939e+09 & 2.1597e+12 & 42681 \\
- Central\_Air & 1 & 1.6533e+10 & 2.1697e+12 & 42691 \\
- Roof\_Style & 5 & 2.8786e+10 & 2.1820e+12 & 42694 \\
- Half\_Bath & 1 & 3.5009e+10 & 2.1882e+12 & 42708 \\
- Lot\_Area & 1 & 3.5997e+10 & 2.1892e+12 & 42709 \\
- Fireplaces & 1 & 3.6853e+10 & 2.1900e+12 & 42710 \\
- House\_Style & 7 & 7.0980e+10 & 2.2241e+12 & 42730 \\
- Garage\_Area & 1 & 6.4143e+10 & 2.2173e+12 & 42735 \\
- Bldg\_Type & 4 & 7.1274e+10 & 2.2244e+12 & 42736 \\
- Full\_Bath & 1 & 6.8198e+10 & 2.2214e+12 & 42739 \\
- Overall\_Qual & 9 & 1.7183e+12 & 3.8715e+12 & 43862 \\
\bottomrule()
\end{longtable}

Once Street is removed, the new ``base model'' (without this variable)
has an AIC value of 42,674.6. This is now the new value to beat.
Repeating this process with removing the other variables one at a time,
we see the output:

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
& & & 2.1547e+12 & 42675 \\
- TotRms\_AbvGrd & 1 & 2.9784e+09 & 2.1577e+12 & 42675 \\
- Central\_Air & 1 & 1.7247e+10 & 2.1720e+12 & 42689 \\
- Roof\_Style & 5 & 2.8560e+10 & 2.1833e+12 & 42692 \\
- Half\_Bath & 1 & 3.4751e+10 & 2.1895e+12 & 42705 \\
- Lot\_Area & 1 & 3.5041e+10 & 2.1898e+12 & 42706 \\
- Fireplaces & 1 & 3.6680e+10 & 2.1914e+12 & 42707 \\
- House\_Style & 7 & 7.3149e+10 & 2.2279e+12 & 42729 \\
- Garage\_Area & 1 & 6.3520e+10 & 2.2182e+12 & 42732 \\
- Bldg\_Type & 4 & 7.3044e+10 & 2.2278e+12 & 42735 \\
- Full\_Bath & 1 & 6.8973e+10 & 2.2237e+12 & 42737 \\
- Second\_Flr\_SF & 1 & 1.2513e+11 & 2.2798e+12 & 42788 \\
- First\_Flr\_SF & 1 & 1.4221e+11 & 2.2969e+12 & 42804 \\
- Overall\_Qual & 9 & 1.7202e+12 & 3.8749e+12 & 43860 \\
\bottomrule()
\end{longtable}

Notice that none of the removal of variables is better than the ``base
model'' based on the AIC (best one is TotRms\_AbvGrd and that has an AIC
of 42675, which is not better than the base model). Since none of the
removals improve the model, the algorithm stops here with all variables
in the model except Street. A quick note here: notice that backward
elimination selected the same model as forward selection. This will not
always be the case! Therefore, the final model is:

\begin{verbatim}
Step:  AIC=42674.6
Sale_Price ~ Lot_Area + Bldg_Type + House_Style + Overall_Qual + 
    Roof_Style + Central_Air + First_Flr_SF + Second_Flr_SF + 
    Full_Bath + Half_Bath + Fireplaces + Garage_Area + TotRms_AbvGrd
\end{verbatim}

The code below illustrates how to do backward elimination with BIC and
p-values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# k = log(n) for BIC selection}
\NormalTok{back.model2 }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(full.model,}
                   \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                               \AttributeTok{upper =}\NormalTok{ full.model),}
                   \AttributeTok{direction =} \StringTok{"backward"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{log}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(train\_sel))) }\CommentTok{\# k = qchisq(alpha, 1, lower.tail = FALSE) for p{-}value with alpha selection}
\NormalTok{alpha.f}\OtherTok{=}\FloatTok{0.05}
\NormalTok{back.model3 }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(full.model,}
                   \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                                \AttributeTok{upper =}\NormalTok{ full.model),}
                   \AttributeTok{direction =} \StringTok{"backward"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{qchisq}\NormalTok{(alpha.f, }\DecValTok{1}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\hypertarget{stepwise}{%
\subsection*{Stepwise}\label{stepwise}}
\addcontentsline{toc}{subsection}{Stepwise}

Stepwise selection is a combination of both of these methods. In this
algorithm, we start with the ``base model'' as the empty model
(i.e.~just the intercept) and will add variables (as in forward
selection). However, as we add new variables, we will also check that
the variables in the model are still contributing (in other words, after
a new variable is added, we check to see if the model would be better if
we drop one of the other variables in the model). Keep in mind that this
algorithm is similar to forward selection and backward elimination in
that only one variable may either enter or be removed at each step. The
algorithm stops when no more variables can be added to nor taken away
from the model (in other words, the current ``base model'' is better
than adding any single addition of one variable or any single extraction
of one variable). The addition and removal of variables is again based
upon the criteria specified. See the algorithm below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  Start with empty model with only the intercept in it, this is the base
  model and calculate the criterion on this model\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    For each variable not in model, create a linear regression model
    with the base model plus this variable; create additional models
    with the base model taking away one variable at a time\\
  \item
    See which linear regression is best (based on criterion)\\
  \item
    Is this regression better than the base model?\\

    \begin{enumerate}
    \def\labelenumiii{\alph{enumiii}.}
    \tightlist
    \item
      Yes, then continue on to step 4\\
    \item
      No, exit the algorithm with the base model as the chosen model\\
    \end{enumerate}
  \item
    The base model is now the best model selected in step 3. Using this
    as your new base model, go back to step 1 and continue.\\
  \end{enumerate}
\end{enumerate}

To do stepwise selection in R, you will use the step function. The
``empty model'' should be used as your initial model. For the scope of
the model, you need to put the ``smallest model'' (just the intercept)
to the ``largest model'' (full model). The direction is both. The
penalty (which indicates the criteria) can be controlled by defining
``\(k\)''. As discussed before, a value of 2 will use the AIC criterion,
a value of \(log(n)\) will produce the BIC penalty, and finally the
upper \(\alpha\)-quantile of a \(\chi^{2}\) distribution with one degree
of freedom will use p-value for its removal of variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create full model and empty model}
\NormalTok{full.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . , }\AttributeTok{data =}\NormalTok{ train\_sel)}
\NormalTok{empty.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train\_sel)}

\CommentTok{\# k = 2 for AIC selection}
\NormalTok{step.model }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(empty.model,}
                  \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ empty.model,}
                               \AttributeTok{upper =}\NormalTok{ full.model),}
                  \AttributeTok{direction =} \StringTok{"both"}\NormalTok{, }\AttributeTok{k =} \DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

As you can see, the initial base model is just the intercept
(AIC=46,323.64). From this initial model, all simple linear regressions
are created and the AIC is observed.

\begin{verbatim}
Start:  AIC=46323.64
Sale_Price ~ 1
\end{verbatim}

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
+ Overall\_Qual & 9 & 9.3437e+12 & 3.8531e+12 & 43817 \\
+ Gr\_Liv\_Area & 1 & 6.4389e+12 & 6.7578e+12 & 44953 \\
+ Garage\_Area & 1 & 5.3561e+12 & 7.8407e+12 & 45258 \\
+ First\_Flr\_SF & 1 & 4.8867e+12 & 8.3100e+12 & 45377 \\
+ Full\_Bath & 1 & 3.7827e+12 & 9.4141e+12 & 45633 \\
+ TotRms\_AbvGrd & 1 & 3.2304e+12 & 9.9663e+12 & 45750 \\
+ Fireplaces & 1 & 2.9715e+12 & 1.0225e+13 & 45802 \\
+ Half\_Bath & 1 & 1.1209e+12 & 1.2076e+13 & 46144 \\
+ Roof\_Style & 5 & 1.0724e+12 & 1.2124e+13 & 46160 \\
+ Central\_Air & 1 & 9.6147e+11 & 1.2235e+13 & 46170 \\
+ House\_Style & 7 & 1.0245e+12 & 1.2172e+13 & 46172 \\
+ Second\_Flr\_SF & 1 & 9.4611e+11 & 1.2251e+13 & 46173 \\
+ Lot\_Area & 1 & 9.0332e+11 & 1.2293e+13 & 46180 \\
+ Bldg\_Type & 4 & 4.6434e+11 & 1.2732e+13 & 46258 \\
+ Street & 1 & 3.1752e+10 & 1.3165e+13 & 46321 \\
& & & 1.3197e+13 & 46324 \\
\bottomrule()
\end{longtable}

As you can see from the above output, the ``best'' model is the one
containing Overall\_Qual. Therefore, the new base model is the one
containing just this variable. ~

Using this as the new base model, we look at adding each of the
individual variables to this model (see output below), as well as taking
away the variables in the model (in this case, just Overall\_Qual). From
the output, you can see a ``+'' for when a variable is being added and a
``-'' for when a variable is being taken away.

\begin{verbatim}
Step:  AIC=43816.66
Sale_Price ~ Overall_Qual
\end{verbatim}

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
+ Gr\_Liv\_Area & 1 & 9.8905e+11 & 2.8640e+12 & 43210 \\
+ First\_Flr\_SF & 1 & 5.2665e+11 & 3.3264e+12 & 43517 \\
+ Garage\_Area & 1 & 4.6644e+11 & 3.3866e+12 & 43554 \\
+ TotRms\_AbvGrd & 1 & 4.6123e+11 & 3.3918e+12 & 43557 \\
+ Full\_Bath & 1 & 4.1206e+11 & 3.4410e+12 & 43587 \\
+ Fireplaces & 1 & 4.0551e+11 & 3.4476e+12 & 43591 \\
+ Lot\_Area & 1 & 3.8148e+11 & 3.4716e+12 & 43605 \\
+ Bldg\_Type & 4 & 2.3715e+11 & 3.6159e+12 & 43694 \\
+ Second\_Flr\_SF & 1 & 1.7555e+11 & 3.6775e+12 & 43723 \\
+ Half\_Bath & 1 & 1.3948e+11 & 3.7136e+12 & 43743 \\
+ Central\_Air & 1 & 9.1322e+10 & 3.7617e+12 & 43769 \\
+ House\_Style & 7 & 6.1815e+10 & 3.7912e+12 & 43797 \\
+ Roof\_Style & 5 & 5.1448e+10 & 3.8016e+12 & 43799 \\
& & & 3.8531e+12 & 43817 \\
+ Street & 1 & 1.9573e+06 & 3.8531e+12 & 43819 \\
- Overall\_Qual & 9 & 9.3437e+12 & 1.3197e+13 & 46324 \\
\bottomrule()
\end{longtable}

The best model is now the one adding \texttt{Gr\_Liv\_Area} to the old
base model that just included \texttt{Overall\_Qual}. This becomes the
new base model and the algorithm continues below..

\begin{verbatim}
Step:  AIC=43210.24
Sale_Price ~ Overall_Qual + Gr_Liv_Area
\end{verbatim}

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
+ House\_Style & 7 & 2.5351e+11 & 2.6105e+12 & 43034 \\
+ Garage\_Area & 1 & 2.1638e+11 & 2.6476e+12 & 43051 \\
+ Lot\_Area & 1 & 1.3097e+11 & 2.7330e+12 & 43116 \\
+ First\_Flr\_SF & 1 & 1.2210e+11 & 2.7419e+12 & 43123 \\
+ Fireplaces & 1 & 1.1069e+11 & 2.7533e+12 & 43131 \\
+ Central\_Air & 1 & 1.1050e+11 & 2.7535e+12 & 43132 \\
+ Second\_Flr\_SF & 1 & 1.0207e+11 & 2.7619e+12 & 43138 \\
+ Bldg\_Type & 4 & 1.0299e+11 & 2.7610e+12 & 43143 \\
+ Roof\_Style & 5 & 6.0726e+10 & 2.8033e+12 & 43176 \\
+ Full\_Bath & 1 & 3.2970e+10 & 2.8310e+12 & 43188 \\
+ TotRms\_AbvGrd & 1 & 2.4688e+10 & 2.8393e+12 & 43194 \\
& & & 2.8640e+12 & 43210 \\
+ Half\_Bath & 1 & 4.0261e+07 & 2.8640e+12 & 43212 \\
+ Street & 1 & 2.2632e+07 & 2.8640e+12 & 43212 \\
- Gr\_Liv\_Area & 1 & 9.8905e+11 & 3.8531e+12 & 43817 \\
- Overall\_Qual & 9 & 3.8938e+12 & 6.7578e+12 & 44953 \\
\bottomrule()
\end{longtable}

Skipping ahead to the final step, we see that the base model includes
the variables Overall\_Qual, House\_Style, Garage\_Area, Bldg\_Type,
Fireplaces, Full\_Bath, Half\_Bath, Lot\_Area, Roof\_Style,
Central\_Air, Second\_Flr\_SF, TotRms\_AbvGrd and First\_Flr\_SF.
Looking at the output below, this is the ``best model'' (no other model,
either inputting one more variable nor removing one variable can beat it
based on this criteria).

\begin{verbatim}
Step:  AIC=42674.6
Sale_Price ~ Overall_Qual + House_Style + Garage_Area + Bldg_Type + 
    Fireplaces + Full_Bath + Half_Bath + Lot_Area + Roof_Style + 
    Central_Air + Second_Flr_SF + TotRms_AbvGrd + First_Flr_SF
\end{verbatim}

\begin{longtable}[]{@{}lcrcr@{}}
\toprule()
& Df & Sum of Sq & RSS & AIC \\
\midrule()
\endhead
& & & 2.1547e+12 & 42675 \\
- TotRms\_AbvGrd & 1 & 2.9784e+09 & 2.1577e+12 & 42675 \\
+ Street & 1 & 1.0581e+09 & 2.1537e+12 & 42676 \\
+ Gr\_Liv\_Area & 1 & 5.2156e+08 & 2.1542e+12 & 42676 \\
- Central\_Air & 1 & 1.7247e+10 & 2.1720e+12 & 42689 \\
- Roof\_Style & 5 & 2.8560e+10 & 2.1833e+12 & 42692 \\
- Half\_Bath & 1 & 3.4751e+10 & 2.1895e+12 & 42705 \\
- Lot\_Area & 1 & 3.5041e+10 & 2.1898e+12 & 42706 \\
- Fireplaces & 1 & 3.6680e+10 & 2.1914e+12 & 42707 \\
- House\_Style & 7 & 7.3149e+10 & 2.2279e+12 & 42729 \\
- Garage\_Area & 1 & 6.3520e+10 & 2.2182e+12 & 42732 \\
- Bldg\_Type & 4 & 7.3044e+10 & 2.2278e+12 & 42735 \\
- Full\_Bath & 1 & 6.8973e+10 & 2.2237e+12 & 42737 \\
- Second\_Flr\_SF & 1 & 1.2513e+11 & 2.2798e+12 & 42788 \\
- First\_Flr\_SF & 1 & 1.4221e+11 & 2.2969e+12 & 42804 \\
- Overall\_Qual & 9 & 1.7202e+12 & 3.8749e+12 & 43860 \\
\bottomrule()
\end{longtable}

As you can see from the above stepwise procedures of forward, backward
and stepwise, the algorithms attempt to find the ``best model'' by
either adding one variable or taking away one variable at at a time.
There is no guarantee that these algorithms will find the best model,
but they do provide some guidance in terms of potential significance
variables and can provide assistance when the number of variables under
consideration is very large. As a caution though, you should always do
further investigation once you have found models through these
algorithms. These algorithms can give the same models or different
models as their final selection based on the algorithm selected and the
criteria used.

\hypertarget{significance-levels}{%
\section{Significance Levels}\label{significance-levels}}

If you are going to use the ``p-value'' method, you need to be aware of
some considerations due to sample size. The larger the sample size, the
smaller the p-values will be and the more likely it is that you will end
up seeing many ``significant'' p-values due to the relationship between
p-values and sample size (not necessarily because that variable was
informative). The paper by Raftery on the Moodle page sums up the
problem nicely and provides at least some guidance in terms of potential
alpha-levels to use for different sample sizes. A summary of the table
is provided below.

\begin{longtable}[]{@{}lrrrr@{}}
\toprule()
Evidence & 30 & 50 & 100 & 1000 \\
\midrule()
\endhead
Weak & 0.076 & 0.53 & 0.032 & 0.009 \\
Fair & 0.028 & 0.019 & 0.010 & 0.003 \\
Strong & 0.005 & 0.003 & 0.001 & 0.0003 \\
Very Strong & 0.001 & 0.0005 & 0.0001 & 0.00004 \\
\bottomrule()
\end{longtable}

You can quickly see that for even sample sizes of 1000, one should
consider the value of alpha that is used to determine significance. One
thing that most researchers agree on is that considerations need to be
taken for larger sample size. However, what those considerations should
be and exactly how to do that is not agreed upon. You should consider
your sample size when you are determining what level of significance you
want to choose for your analysis (also take into account which is
worse\ldots.a type I error or a type II error). There is no quick answer
to this question and each decision will depend on the analysis being
conducted.

\bookmarksetup{startatroot}

\hypertarget{sec-diag}{%
\chapter{Diagnostics}\label{sec-diag}}

In this Chapter, we will take a look at each of the assumptions in a
linear model. We will discuss what tools you will use to assess these
assumptions, how to diagnose if the assumptions are met, some common
problems often encountered when these assumptions do not hold and
finally some remedies to fix these common issues. You will notice that a
variety of data sets are used throughout this Chapter. This is to help
visualize and test for various assumptions and how to identify when
these assumptions are not met.

This Chapter aims to answer the following questions:

How to use residuals from a multiple linear regression model to assess
the assumptions.

Does the mean of the response have a linear pattern in the explanatory
variables?

Do the residuals have a constant variance?

Are the residuals normally distributed?

Are the residuals independent?

How to identify potential outliers and influential points.

How to identify potential multicolliearity.

In multiple linear regression, the assumptions are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The mean of the Y's is accurately modeled by a linear function of the
  X's.\\
\item
  The random error term, \(\varepsilon\), is assumed to have a normal
  distribution with a mean of zero.\\
\item
  The random error term, \(\varepsilon\), is assumed to have a constant
  variance, \(\sigma^{2}\).\\
\item
  The errors are independent.\\
\item
  No perfect collinearity.\\
\end{enumerate}

Before exploring the assumptions of a linear model, it is always good to
visually take a look at your data (if it is not too large). The
\texttt{pairs} command in R allows you to look at all scatterplots
between the variables in a data set.

To illustrate this, we will use the Salaries data set in the package
carData in R that has 397 observations of salaries for professors. The
explanatory variables include:

\emph{rank}: a factor with levels AssocProf, AsstProf, Prof

\emph{discipline}: a factor with levels A (``theoretical'' departments)
or B (``applied'' departments)

\emph{yrs.since.phd}: years since PhD

\emph{yrs.service}: years of service

\emph{sex}: a factor with levels Female and Male

\emph{salary}: nine-month salary, in dollars

Using this data set, let's take a look at the relationship between each
pair of variables in Figure Figure~\ref{fig-pairsex}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(carData)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{library}\NormalTok{(nortest)}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(TSA)}
\FunctionTok{library}\NormalTok{(lmtest)}
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{pairs}\NormalTok{(Salaries)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-pairsex-1.pdf}

}

\caption{\label{fig-pairsex}Matrix of Scatter Plots for the Salaries
Data}

\end{figure}

If there are not too many variables, this plot is a nice way to see all
the relationships in the data set. The variables are listed along the
diagonal and each off diagonal plot is a scatterplot of the variables
represented by that row and column. For example, the first row of plots
have rank on the y-axis. The first column of plots have rank along the
x-axis.

\hypertarget{examining-residuals}{%
\section{Examining Residuals}\label{examining-residuals}}

As you can see from the above list of assumptions that most of them
involve the error term which is estimated by the residuals. We will be
using the residuals for many of these diagnostics. One of the most
useful plot is referred as the residual plot. This plot will have the
residuals along the y-axis and either the predicted values or individual
x-values along the x-axis. Figure \ref@(fig:salresid) is an example
using the residuals from a linear model predicting salary using all of
the explanatory variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.model}\OtherTok{=}\FunctionTok{lm}\NormalTok{(salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{Salaries)}

\FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{fitted}\NormalTok{(lm.model),}\AttributeTok{y=}\FunctionTok{resid}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"blue"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Predicted Values"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/salresid-1.pdf}

}

\caption{Residuals vs.~Predicted Values for Salary Model}

\end{figure}

\hypertarget{misspecified-model}{%
\section{Misspecified Model}\label{misspecified-model}}

One of the assumptions assumes that the expected value of the response
is accurately modeled by a linear function of the explanatory variables.
If this is true, then we would expect our residual plots to be random
scatter (in other words, all of the ``signal'' was correctly captured in
the model and there is just noise left over).

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-correctspec-1.pdf}

}

\caption{\label{fig-correctspec}Ideal residual plot showing residual
values randomly distributed with equal variance}

\end{figure}

Looking at the plot in Figure Figure~\ref{fig-correctspec}, we see that
there is no pattern. If you did see some type of pattern in this
residual plot, it would indicate that you are missing something and need
to do some more modeling. For example, a quadratic shape or curvilinear
pattern to the residuals would indicate that one of our input variables
has a nonlinear relationship to the response and transformations should
be made to that input accordingly. For example, a residual scatter plot
like Figure Figure~\ref{fig-quadpattern} would prompt us to consider a
quadratic term.

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-quadpattern-1.pdf}

}

\caption{\label{fig-quadpattern}Residual plot indicating that a
quadratic term is required}

\end{figure}

If your model has more than one x, it is easier to see if an individual
input variable has a quadratic relationship with the response when
looking at plots like Figure Figure~\ref{fig-quadpattern} where the
input variable is on the x-axis.

\textbf{Example}

Let's take a look at an example of where a quadratic linear regression
is needed. This example is studying the effect of a chemical additive on
paper strength. The response variable is the amount of force required to
break the paper (strength) and the explanatory variable is the amount of
chemical additive (amount).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ amount}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{ strength}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{2.4}\NormalTok{,}\FloatTok{2.6}\NormalTok{,}\FloatTok{2.7}\NormalTok{,}\FloatTok{2.5}\NormalTok{,}\FloatTok{2.6}\NormalTok{,}\FloatTok{2.6}\NormalTok{,}\FloatTok{2.7}\NormalTok{,}\FloatTok{2.8}\NormalTok{,}\FloatTok{2.8}\NormalTok{,}\FloatTok{2.8}\NormalTok{,}\FloatTok{3.0}\NormalTok{,}\FloatTok{3.0}\NormalTok{,}\FloatTok{3.0}\NormalTok{,}\FloatTok{2.9}\NormalTok{,}\FloatTok{2.9}\NormalTok{,}\FloatTok{3.0}\NormalTok{,}\FloatTok{3.1}\NormalTok{,}\FloatTok{2.9}\NormalTok{,}\FloatTok{2.9}\NormalTok{,}\FloatTok{3.0}\NormalTok{,}\FloatTok{2.9}\NormalTok{,}\FloatTok{2.8}\NormalTok{)}
\NormalTok{ lm.quad}\OtherTok{=}\FunctionTok{lm}\NormalTok{(strength}\SpecialCharTok{\textasciitilde{}}\NormalTok{amount)}
 \FunctionTok{summary}\NormalTok{(lm.quad)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = strength ~ amount)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.199780 -0.091850  0.004185  0.101707  0.206167 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.50176    0.06920  36.154  < 2e-16 ***
amount       0.09802    0.01998   4.907 8.52e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1283 on 20 degrees of freedom
Multiple R-squared:  0.5462,    Adjusted R-squared:  0.5236 
F-statistic: 24.08 on 1 and 20 DF,  p-value: 8.518e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
 \FunctionTok{ggplot}\NormalTok{(lm.quad,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{amount,}\AttributeTok{y=}\FunctionTok{resid}\NormalTok{(lm.quad)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"blue"}\NormalTok{,}\AttributeTok{size=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{( }\AttributeTok{x=}\StringTok{"Amount"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/quadexample1-1.pdf}

}

\caption{Residual Plot Showing a Quadratic Relationship}

\end{figure}

The above fitted model is \[\hat{Y}_{i} = 2.5 + 0.1x_{i}.\] However,
after looking at the residual plot and noticing the quadratic shape, we
realize that we need a higher order term for amount.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.quad}\OtherTok{=}\FunctionTok{lm}\NormalTok{(strength}\SpecialCharTok{\textasciitilde{}}\NormalTok{amount }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(amount}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}

\FunctionTok{summary}\NormalTok{(lm.quad)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = strength ~ amount + I(amount^2))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.22276 -0.06562 -0.02763  0.07602  0.19466 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.21334    0.13399  16.519 9.97e-13 ***
amount       0.32928    0.09690   3.398  0.00302 ** 
I(amount^2) -0.03728    0.01535  -2.428  0.02526 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.115 on 19 degrees of freedom
Multiple R-squared:  0.6537,    Adjusted R-squared:  0.6173 
F-statistic: 17.93 on 2 and 19 DF,  p-value: 4.212e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(lm.quad,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{amount,}\AttributeTok{y=}\FunctionTok{resid}\NormalTok{(lm.quad)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{,}\AttributeTok{size=}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Residual plot"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Amount"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-quadexample2-1.pdf}

}

\caption{\label{fig-quadexample2}New Residual Plot after Fitting a
Quadratic Term}

\end{figure}

The second order polynomial model is
\[\hat{Y}_{i} = 2.21 + 0.33x_{i}-0.04x^{2}_{i}\] and the residuals from
this model are shown in Figure Figure~\ref{fig-quadexample2}. If you
think there might still be some pattern in Figure
Figure~\ref{fig-quadexample2}, you could try a third degree polynomial:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm}\FloatTok{.3}\OtherTok{=}\FunctionTok{lm}\NormalTok{(strength}\SpecialCharTok{\textasciitilde{}}\NormalTok{amount}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(amount}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{I}\NormalTok{(amount}\SpecialCharTok{\^{}}\DecValTok{3}\NormalTok{))}

\FunctionTok{summary}\NormalTok{(lm}\FloatTok{.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = strength ~ amount + I(amount^2) + I(amount^3))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.15941 -0.06360  0.00272  0.08579  0.14142 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.73280    0.26060  10.487 4.28e-09 ***
amount      -0.36900    0.32208  -1.146   0.2669    
I(amount^2)  0.22339    0.11651   1.917   0.0712 .  
I(amount^3) -0.02862    0.01270  -2.254   0.0369 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1044 on 18 degrees of freedom
Multiple R-squared:  0.7299,    Adjusted R-squared:  0.6849 
F-statistic: 16.22 on 3 and 18 DF,  p-value: 2.344e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(lm}\FloatTok{.3}\NormalTok{,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{amount,}\AttributeTok{y=}\FunctionTok{resid}\NormalTok{(lm}\FloatTok{.3}\NormalTok{)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{,}\AttributeTok{size=}\DecValTok{2}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Amount"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-quadexample3-1.pdf}

}

\caption{\label{fig-quadexample3}New Residual Plot after Fitting a Cubic
Term}

\end{figure}

The new regression equation is now
\[\hat{Y}_{i} = 2.73 - 0.37x_{i}+0.22x^{2}_{i}-0.03x^{3}_{i}.\]

In wrapping up the misspecified model, if a linear model does not look
appropriate (there is a pattern in the residual plot), then you can try
the following remedies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit a polynomial or more complex regression model.\\
\item
  Transform the dependent and/or independent variables to obtain
  linearity.\\
\item
  Fit a nonlinear regression model, if appropriate (will need to decide
  the shape of a nonlinear model).\\
\item
  Fit a nonparametric regression model (for example splines or a LOESS
  regression).\\
\end{enumerate}

\hypertarget{constant-variance}{%
\section{Constant Variance}\label{constant-variance}}

Another assumption for linear regression is that the variance is
constant about the line. Looking at Figure Figure~\ref{fig-constvar},
you see that the variation about the line is constant across the line
(notice the bands that have been drawn around the line).

\begin{verbatim}
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
i Please use `linewidth` instead.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-constvar-1.pdf}

}

\caption{\label{fig-constvar}Residual Plot Showing Non-Constant
Variance}

\end{figure}

However, an example of where this is not true is shown in Figure
Figure~\ref{fig-nonconstvar}.

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-nonconstvar-1.pdf}

}

\caption{\label{fig-nonconstvar}Residual Plot Showing Non-Constant
Variance}

\end{figure}

Notice how the variability increases as the predicted values increase
(the bands get wider). This is referred to as heteroskedasticity in the
variance, which violates the constant variance assumption
(homoskedasticity). The homoskedastic assumption is about the regression
line, so it is best to look at the plot of residuals versus predicted
values (not individual x-values). There are a few tests for this
assumption, but they are limited in what they are able to test. Best way
to evaluate this assumption is by visualizing the residual plot and make
a judgement call. If the variance appears to be heteroskadastic, any
inferences under the traditional assumptions will be incorrect. In other
words, hypothesis tests and confidence intervals based on the t, F, and
\(\chi^{2}\) distributions will not be valid.

\textbf{Example}

The following fictious salary data set is from the online textbook
\href{https://daviddalpiaz.github.io/appliedstats/index.html}{\emph{Applied
Statistics with R}}. The explanatory variable is number of years
employment and the response variable is annual salary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{years}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{13}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{14}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{15}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{16}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{17}\NormalTok{,}\DecValTok{18}\NormalTok{,}\DecValTok{18}\NormalTok{,}\DecValTok{18}\NormalTok{,}\DecValTok{18}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{19}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{21}\NormalTok{,}\DecValTok{21}\NormalTok{,}\DecValTok{21}\NormalTok{,}\DecValTok{21}\NormalTok{,}\DecValTok{22}\NormalTok{,}\DecValTok{22}\NormalTok{,}\DecValTok{22}\NormalTok{,}\DecValTok{22}\NormalTok{,}\DecValTok{23}\NormalTok{,}\DecValTok{23}\NormalTok{,}\DecValTok{23}\NormalTok{,}\DecValTok{23}\NormalTok{,}\DecValTok{24}\NormalTok{,}\DecValTok{24}\NormalTok{,}\DecValTok{24}\NormalTok{,}\DecValTok{24}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{,}\DecValTok{25}\NormalTok{)}

\NormalTok{salary}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{41504}\NormalTok{,}\DecValTok{32619}\NormalTok{,}\DecValTok{44322}\NormalTok{,}\DecValTok{40038}\NormalTok{,}\DecValTok{46147}\NormalTok{,}\DecValTok{38447}\NormalTok{,}\DecValTok{38163}\NormalTok{,}\DecValTok{42104}\NormalTok{,}\DecValTok{25597}\NormalTok{,}\DecValTok{39599}\NormalTok{,}\DecValTok{55698}\NormalTok{,}\DecValTok{47220}\NormalTok{,}\DecValTok{65929}\NormalTok{,}\DecValTok{55794}\NormalTok{,}\DecValTok{45959}\NormalTok{,}\DecValTok{52460}\NormalTok{,}\DecValTok{60308}\NormalTok{,}\DecValTok{61458}\NormalTok{,}\DecValTok{56951}\NormalTok{,}\DecValTok{56174}\NormalTok{,}\DecValTok{59363}\NormalTok{,}\DecValTok{57642}\NormalTok{,}\DecValTok{69792}\NormalTok{,}\DecValTok{59321}\NormalTok{,}\DecValTok{66379}\NormalTok{,}\DecValTok{64282}\NormalTok{,}\DecValTok{48901}\NormalTok{,}\DecValTok{100711}\NormalTok{,}\DecValTok{59324}\NormalTok{,}\DecValTok{54752}\NormalTok{,}\DecValTok{73619}\NormalTok{,}\DecValTok{65382}\NormalTok{,}\DecValTok{58823}\NormalTok{,}\DecValTok{65717}\NormalTok{,}\DecValTok{92816}\NormalTok{,}\DecValTok{72550}\NormalTok{,}\DecValTok{71365}\NormalTok{,}\DecValTok{88888}\NormalTok{,}\DecValTok{62969}\NormalTok{,}\DecValTok{45298}\NormalTok{,}\DecValTok{111292}\NormalTok{,}\DecValTok{91491}\NormalTok{,}\DecValTok{106345}\NormalTok{,}\DecValTok{99009}\NormalTok{,}\DecValTok{73981}\NormalTok{,}\DecValTok{72547}\NormalTok{,}\DecValTok{74991}\NormalTok{,}\DecValTok{139249}\NormalTok{,}\DecValTok{119948}\NormalTok{,}\DecValTok{128962}\NormalTok{,}\DecValTok{98112}\NormalTok{,}\DecValTok{97159}\NormalTok{,}\DecValTok{125246}\NormalTok{,}\DecValTok{89694}\NormalTok{,}\DecValTok{73333}\NormalTok{,}\DecValTok{108710}\NormalTok{,}\DecValTok{97567}\NormalTok{,}\DecValTok{90359}\NormalTok{,}\DecValTok{119806}\NormalTok{,}\DecValTok{101343}\NormalTok{,}\DecValTok{147406}\NormalTok{,}\DecValTok{153020}\NormalTok{,}\DecValTok{143200}\NormalTok{,}\DecValTok{97327}\NormalTok{,}\DecValTok{184807}\NormalTok{,}\DecValTok{146263}\NormalTok{,}\DecValTok{127925}\NormalTok{,}\DecValTok{159785}\NormalTok{,}\DecValTok{174822}\NormalTok{,}\DecValTok{177610}\NormalTok{,}\DecValTok{210984}\NormalTok{,}\DecValTok{160044}\NormalTok{,}\DecValTok{137044}\NormalTok{,}\DecValTok{182996}\NormalTok{,}\DecValTok{184183}\NormalTok{,}\DecValTok{168666}\NormalTok{,}\DecValTok{121350}\NormalTok{,}\DecValTok{193627}\NormalTok{,}\DecValTok{142611}\NormalTok{,}\DecValTok{170131}\NormalTok{,}\DecValTok{134140}\NormalTok{,}\DecValTok{129446}\NormalTok{,}\DecValTok{201469}\NormalTok{,}\DecValTok{202104}\NormalTok{,}\DecValTok{220556}\NormalTok{,}\DecValTok{166419}\NormalTok{,}\DecValTok{149044}\NormalTok{,}\DecValTok{247017}\NormalTok{,}\DecValTok{247730}\NormalTok{,}\DecValTok{252917}\NormalTok{,}\DecValTok{235517}\NormalTok{,}\DecValTok{241276}\NormalTok{,}\DecValTok{197229}\NormalTok{,}\DecValTok{175879}\NormalTok{,}\DecValTok{253682}\NormalTok{,}\DecValTok{262578}\NormalTok{,}\DecValTok{207715}\NormalTok{,}\DecValTok{221179}\NormalTok{,}\DecValTok{212028}\NormalTok{,}\DecValTok{312549}\NormalTok{)}

\NormalTok{lm.var}\OtherTok{=}\FunctionTok{lm}\NormalTok{(salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{years)}

\FunctionTok{ggplot}\NormalTok{(lm.var,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{fitted}\NormalTok{(lm.var),}\AttributeTok{y=}\FunctionTok{resid}\NormalTok{(lm.var)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"blue"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Residual Plot"}\NormalTok{, }\AttributeTok{x=}\StringTok{"Predicted Values"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-varexample-1.pdf}

}

\caption{\label{fig-varexample}Residuals With Heteroskedasticity}

\end{figure}

As you can see from the residual plot in Figure@fig-varexample, the
graph exhibits a fan-shape and the variance is increasing as the
predicted values get larger. We can apply a logarithmic transform to try
to stabilize the variance, the result of which is shown in
Figure@fig-correctexam.

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-correctexam-1.pdf}

}

\caption{\label{fig-correctexam}Residual Plot Showing Stabilized
Variance after Log Transform}

\end{figure}

The log transform stabilized the variance and produces a better residual
plot.

If the assumption of Homoskadicity of variance is violated, there are a
few remedies you can try: 1. Use Weighted Least Squares (WLS) or
iteratively reweighted least squares (IRLS).\\
2. Transform data.\\
3. Use a different distribution (for example if the response is count
data, use Poisson distribution).\\

\hypertarget{normality}{%
\section{Normality}\label{normality}}

Another assumption is that the residuals are normally distributed. We
can test this assumption through use of visual aids or formal hypothesis
tests. visual aids include a histogram of the residuals or Q-Q plot of
residuals. A few tests of normality of residuals, include (but not
limited to) Shapiro-Wilk, Anderson-Darling, Kolmogorov-Smirnov tests. In
addition to the base R package, you should also install and library the
nortest package.

\textbf{Visualization}

To visually inspect whether or not the residuals are normally
distributed, we can either graph the historgram of residuals or create a
Q-Q plot of the residuals. In the histogram, we are looking for a
bell-shaped curve and in the Q-Q plot, we are looking for a straight
line.

\begin{verbatim}
Warning: `qplot()` was deprecated in ggplot2 3.4.0.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/normalvis-1.pdf}

}

\caption{Histogram and Q-Q Plot of Residuals}

\end{figure}

\textbf{Formal Hypothesis tests}

You can also run a formal hypothesis test. The hypotheses are\\
\(H_{0}:\) Residuals are normally distributed\\
\(H_{A}:\) Residuals are not normally distributed.

There are MANY different tests for normality. Only two will be covered
here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Anderson-Darling is based on the empirical cumulative distribution
  function of the data and gives more weight to the tails.\\
\item
  Shapiro-Wilk test uses the idea of correlation between the sample data
  and normal scores. The Shapiro-Wilk is better for smaller data sets.
\end{enumerate}

With large samples, you will most likely end up rejecting these
hypothesis tests (using corrected significance levels is recommended).
Keep in mind that you can also get different result from different
tests. It is up to you to make the decision on whether or not you are
comfortable in the assumption for normality holding in different
situations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{55402}\NormalTok{)}
\NormalTok{x}\OtherTok{=}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\FunctionTok{ad.test}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Anderson-Darling normality test

data:  x
A = 0.30275, p-value = 0.5742
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  x
W = 0.99889, p-value = 0.814
\end{verbatim}

Using a simulated data set (so the ``TRUE'' distribution is actually
normal indicating that we should ``Fail to reject'' the null
hypothesis). The p-values from both of these test illustrate that they
would indicate the distribution is not significantly different from
normal (which it should).

\textbf{Box-cox Transformation}

If the residuals are not normally distributed, one solution is to
transform them to normality. However, the exact transformation might be
difficult to identify. George Box and Sir David Cox developed an
algorithm back in the 1960's to assist in identifying ``power''
transformations to make a variable normally distributed
(think\ldots{}``what power should I raise this variable to?''). Their
algorithm tries out different values of \(\lambda\) or powers of the
response variable in the following way:\\

\[y = \begin{array}
{rr}
\frac{y^{\lambda}-1}{\lambda} & if \lambda \ne 0  \\
log(y) & if \lambda = 0 
\end{array}\]

\textbf{Example}

Using the residuls from the originally salary data, we can take a look
at the box-cox transformation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.var}\OtherTok{=}\FunctionTok{lm}\NormalTok{(salary}\SpecialCharTok{\textasciitilde{}}\NormalTok{years)}

\FunctionTok{boxcox}\NormalTok{(lm.var)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/boxcox-1.pdf}

}

\end{figure}

The output from the graph clearly incates that \(\lambda\) should be 0,
which indicates a log transform.

In conclusion, to deal with data that is NOT normally distributed, we
can either\\
1. Use a robust regression (quantile regression or nonparametric)\\
2. Transform either response or predictor variables or both to obtain
normality in the residuals.\\

\hypertarget{correlated-errors}{%
\section{Correlated Errors}\label{correlated-errors}}

Another important assumption is that the observations are independent.
There are a couple of ways in which we assess this assumtpion:\\

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Experimental design or collection of data\\
\item
  Explore residual plots\\
\end{enumerate}

Depending on how the data was collected or the experimental design,
there could be potential correlation in the observations. For example,
if the data collected information on multiple family members (include
husband and wife and kids). Or if multiple observations were collected
on the same subject. There are various models that exists that can
account for this dependence (longitudinal analysis, hierarchical models,
etc). However, this type of analysis will be covered later throughout
the year. There is no diagnostic measures to indicate this correlation
structure. Your best way of knowing it is there is to know the
experimental design or collection of data.~ We will focus our attention
on a form of dependence that is evident from the residual plots, which
is autocorrelation. This happens when ordered observations are dependent
on prior observations. One of the most common instances of this
correlation structure is through data that is collected over time, also
referred to as time series. You will learn more about time series data
in the Fall semester, but for now, let's explore how to recognize if it
is an issue in the analysis. To diagnose autocorrelation in data, you
can\\

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Visually inspect the plot of residuals by time (see a pattern over
  time\ldots usually cyclical)\\
\item
  Conduct a Durbin-Watson test\\
\end{enumerate}

In visually inspecting a plot of residuals across time, you would expect
to see a type of cyclical pattern if autocorrelation exists as it does
in Figure Figure~\ref{fig-autoplot}.

\begin{figure}

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-autoplot-1.pdf}

}

\caption{\label{fig-autoplot}Residuals vs Time: Autocorrelation Present}

\end{figure}

In using the Durbin-Watson test, the hypotheses are:\\
\(H_{0}:\) No residual correlation\\
\(H_{A}:\) Residual Correlation\\

The Durbin-Watson test statistic is calculated by

\[ d=\frac{\sum_{t=2}^T(e_{t}-e_{t-1})^2}{\sum_{t=1}^T e_{t}^2}\] where
\(e\) represents the error terms, or residuals. The statistic \emph{d}
ranges from 0 to 4, with a value of 2 indicating no residual
correlation. Values of \emph{d} smaller than 2 may indicate a positive
autocorrelation and a value of \emph{d} larger than 2 may indicate a
negative autocorrelation. However, the question becomes when is it
``significantly different'' than 2? Unless there is good reason to
assume a negative autocorrelation, testing for positive autocorrelation
would be the preferred test. The Durbin-Watson test is in the lmtest
package of R. If you want to test for positive autocorrelation, you will
need to specify the alternative of ``greater'' (even though the test
statistic is testing for a value LESS than 2!!).\\

\textbf{Example} The following data set illustrates this test using the
Google data set in the TSA package (returns of Google stock from
08/20/04 - 09/13/06).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(google)}
\NormalTok{x}\OtherTok{=}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FunctionTok{length}\NormalTok{(google))}
\NormalTok{lm.model}\OtherTok{=}\FunctionTok{lm}\NormalTok{(google}\SpecialCharTok{\textasciitilde{}}\NormalTok{x)}
\FunctionTok{dwtest}\NormalTok{(lm.model,}\AttributeTok{alternative=}\StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Durbin-Watson test

data:  lm.model
DW = 1.842, p-value = 0.0321
alternative hypothesis: true autocorrelation is greater than 0
\end{verbatim}

In running this test, you either need to have the data sorted by the
correct date (it assumes the observations are correctly ordered by
time). If they are not, there is an order.by option in which you can
use. For this example, the p-value is 0.0321, which if we use an
\(\alpha\) of 0.05, we would reject the null hypothesis and conclude
there appears to be significant positive autocorrelation.

If your data has significant autocorrelation, then the error terms will
not be correct. We will discuss in the Fall semester how to model time
series data appropriately.

\hypertarget{influential-observations-and-outliers}{%
\section{Influential Observations and
Outliers}\label{influential-observations-and-outliers}}

Influential points and outliers play a large role in the estimation of
the model and its prediction. Influential points are usually those
points on the edges of the x-values and can greatly impact the slopes in
the regression equation. Outliers tend to be those values that do not
follow the trend of the data and are generally found by large deviations
in the y direction. For multiple linear regression, outliers are found
by using residuals. These can be standardize residuals or studentized
residuals. Influential points can be discovered by Cook's D, dffits,
dfbetas or Hat values. These points are important to identify and
recognize their influence on the regression, however, it does NOT mean
that these points should be removed. Removal of data should be taken
very seriously. We would obviously want to omit any observations made in
error, due to typos or reporting inaccuracies, but we do not remove data
points just because they are outside the scope of our expectations. We
have many tools at our disposal for modeling with such observations, and
we should always develop a deep understanding of the
application-specific risks and rewards associated with data removal.\\

In the this section we will cover the following statistics to help us
detect outliers and influential observations respectively.

\emph{Outliers}\\
rstandard\\
rstudent\\
\emph{Influential observations}\\
dffits\\
dfbetas\\
cooks.distance\\
hatvalues\\

\emph{Outliers}\\
In using residuals to detect outliers, we first need to ``standardize''
them, or divide by their standard errors. In this sense, we can think of
these ``standardized'' residuals as an approximate z-score. Therefore,
we look for residuals greater in magnitude than 3 as potential outliers.
R calculates two different types of standardized residuals: 1.
internally studentized residuals and 2. externally studentized
residuals. Internally studentized residuals are definied as
\[ r_{i}=\frac{e_{i}}{\hat\sigma\sqrt{1-H_{ii}}},\] where \(\hat\sigma\)
is the square root of the MSE and \(H_{ii}\) is from the diagonal of the
hat matrix (hat matrix will be discussed in influential observations).
In R, these are denoted by rstandard. Externally studentized residuals
are definied as
\[ t_{i}=\frac{e_{i}}{\hat\sigma_{(i)}\sqrt{1-H_{ii}}},\] where
\(\hat\sigma_{(i)}\) is the square root of the MSE calculated when
observation \emph{i} is deleted and \(H_{ii}\) is from the diagonal of
the hat matrix. The externally studentized residuals follow a
t-distribution with \(n-k-2\), where \(k\) is the number of explanatory
variables(notice that this has one less degree of freedom than the usual
error in regression which is due to the one deleted observation). The R
code to obtain externally studentized residuals is rstudent.

\emph{Influential observations}\\
Influential observations are observations that can dramatically change
the model's estimates. It is important to identify and understand these
where these observations are. There are a number of different measures
to aid in identifying influential observations, which will be discussed
below.

Cook's distance, also referred to as Cook's D, measures the difference
in the regression estimates when the \(i^{th}\) observation is left out.
A rough rule of thumb used as a cutoff is if \(D_{i}\) is greater than
\(\frac{4}{n-p-1}\).

Dffits calculates the difference of fitted values for each point in the
regression versus the fit of the regression line for that point if it
was removed. Large values of dffits indicate that the point is
influential in the calculation of the estimated regression line. As a
general rule of thumb, a cutoff of \(2\sqrt{(p+1)/n}\) is used to
identify potential influential points.\\
Dfbetas follows the same idea as dffits. The difference in the estimated
betas is calculated for each observation (observation included in the
estimated beta and observation NOT included in estimating the beta).
This is done for each individual observation and each estimated beta.
For small data sets, a value greater than 1 is suspect of an influential
observation. For large data sets, the cutoff is
\(\frac{2}{\sqrt{n}}\).\\
The hat values \(H_{ii}\) are the diagonal values of
\[\boldsymbol{X(X^{T}X)^{-1.}X^{T}}.\] Hat values can identify high
leverage points in a regression. A general rule of thumb are hat values
greater than \(\frac{2(p+1)}{n}\).

\textbf{Example} We will use the Scottish hill races as an example to
illustrate how to calculate and visualize these values. The Scottish
hill races include the following variables:\\
Time: Record time to complete course\\
Distance: Distance in the course\\
Climb: Vertical climb in the course\\

::: \{.cell layout-align=``center'' =`\{\}' fig=`true'\}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url }\OtherTok{=} \StringTok{\textquotesingle{}http://www.statsci.org/data/general/hills.txt\textquotesingle{}} 
\NormalTok{races.table }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(url, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep=}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}t}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{n.index}\OtherTok{=}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FunctionTok{nrow}\NormalTok{(races.table))}
\NormalTok{races.table}\OtherTok{=}\FunctionTok{cbind}\NormalTok{(races.table,n.index)}
\NormalTok{lm.model}\OtherTok{=}\FunctionTok{lm}\NormalTok{(Time}\SpecialCharTok{\textasciitilde{}}\NormalTok{Distance}\SpecialCharTok{+}\NormalTok{Climb,}\AttributeTok{data=}\NormalTok{races.table)}


\DocumentationTok{\#\#Plots of outliers}
\NormalTok{a }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{rstandard}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Internal Studentized Residuals"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}

\NormalTok{b }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{rstudent}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"External Studentized Residuals"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}

\DocumentationTok{\#\#Influential points}
\NormalTok{c }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{rstandard}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Internal Studentized Residuals"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}

\DocumentationTok{\#\#Cook\textquotesingle{}s D}
\NormalTok{D.cut}\OtherTok{=}\DecValTok{4}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(races.table)}\SpecialCharTok{{-}}\DecValTok{3{-}1}\NormalTok{)}

\NormalTok{d }\OtherTok{=}\FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{cooks.distance}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\NormalTok{D.cut)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Cook\textquotesingle{}s D"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Cook\textquotesingle{}s Distance"}\NormalTok{)}

\DocumentationTok{\#\#Dffit}
\NormalTok{df.cut}\OtherTok{=}\DecValTok{2}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{((}\DecValTok{3}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(races.table)))}

\NormalTok{e }\OtherTok{=}\FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{dffits}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\NormalTok{df.cut)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\SpecialCharTok{{-}}\NormalTok{df.cut)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"DFFITS"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"DFFITS"}\NormalTok{)}

\NormalTok{db.cut}\OtherTok{=}\DecValTok{2}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(races.table))}


\NormalTok{f }\OtherTok{=}\FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{dfbetas}\NormalTok{(lm.model)[,}\StringTok{\textquotesingle{}Climb\textquotesingle{}}\NormalTok{]))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\NormalTok{db.cut)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\SpecialCharTok{{-}}\NormalTok{db.cut)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"DFBETA for Climb"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"DFBETAS"}\NormalTok{)}

\NormalTok{g }\OtherTok{=}\FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{dfbetas}\NormalTok{(lm.model)[,}\StringTok{\textquotesingle{}Distance\textquotesingle{}}\NormalTok{]))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\NormalTok{db.cut)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\SpecialCharTok{{-}}\NormalTok{db.cut)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"DFBETA for Distance"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"DFBETAS"}\NormalTok{)}

\DocumentationTok{\#\#Hat}
\NormalTok{hat.cut}\OtherTok{=}\DecValTok{2}\SpecialCharTok{*}\NormalTok{(}\DecValTok{3}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(races.table)}

\NormalTok{h }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n.index,}\AttributeTok{y=}\FunctionTok{hatvalues}\NormalTok{(lm.model)))}\SpecialCharTok{+}\FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{y=}\NormalTok{hat.cut)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Hat values"}\NormalTok{,}\AttributeTok{x=}\StringTok{"Observation"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Hat Values"}\NormalTok{)}

\FunctionTok{grid.arrange}\NormalTok{(a,b,c,d,e,f,g,}\AttributeTok{ncol=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/fig-influence-1.pdf}

}

\caption{\label{fig-influence}Plots for Exploring Outliers and
Influential Points}

\end{figure}

:::

Figure Figure~\ref{fig-influence} shows a number of useful graphics that
help us explore outliers and influential points.

A good graph to explore is looking at the external studentized residuals
versus the hat values. An observation that is high leverage AND an
outlier is one that needs to be explored.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(lm.model,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{hatvalues}\NormalTok{(lm.model),}\AttributeTok{y=}\FunctionTok{rstudent}\NormalTok{(lm.model))) }\SpecialCharTok{+}  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"orange"}\NormalTok{)}\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Hat values"}\NormalTok{,}\AttributeTok{y=}\StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./05-diagnostics_files/figure-pdf/influenceresid-1.pdf}

}

\caption{Influential and Outlier Observations}

\end{figure}

\hypertarget{multicollinearity}{%
\section{Multicollinearity}\label{multicollinearity}}

Multicollinearity occurs when one or more predictor variables are
linearly related to each other and will create issues with the
regression. The parameter estimates will not be stable and the standard
errors will be inflated (making it more difficult to find significant
explantory variables). The two most common ways to identify
multicollinearity is by looking at the correlation among the predictor
variables and calculating the variance inflation factor.

The variance inflation factor (also referred to as VIF) will take each
explanatory variable and model it as a linear regression of the
remaining explanatory variables. For example, let's say we have the
following regression equation:\\
\[\widehat{Y}_{i}=b_{0} + b_{1}x_{1}+ b_{2}x_{2}+ b_{3}x_{3}.\] There
are only 3 explanatory variables in this regression. A VIF will be
calculated on each \(x_{i}\) in the following manner:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A regression is fit on each \(x_{i}\) with the remaining \(x_{i}\)'s
  as the explanatory variables. For example, to calculate the VIF for
  \(x_{1}\), we fit the following model:\\
  \[\widehat{x}_{1}=b_{0} + b_{1}x_{2}+ b_{2}x_{3},\] and obtain the
  \(R^2\) value from this model (call it \(R_{1}^2\)).\\
\item
  The VIF for \(x_{1}\) is calculated by \[VIF=\frac{1}{1-R_{1}^2}.\]
  Repeat this process for each of the other explanatory variables. If a
  VIF value is larger than 10, then we say that multicollinearity is an
  issue.
\end{enumerate}

\textbf{Example}

We will use the mtcars data set dealing with fuel consumption and
automobile design. The data set consists of the following variables:

A data frame with 32 observations on 11 (numeric) variables.

mpg: Miles/(US) gallon\\
cyl: Number of cylinders\\
disp: Displacement (cu.in.)\\
hp: Gross horsepower\\
drat: Rear axle ratio\\
wt: Weight (1000 lbs)\\
qsec: 1/4 mile time\\
vs: Engine (0 = V-shaped, 1 = straight)\\
am: Transmission (0 = automatic, 1 = manual)\\
gear: Number of forward gears

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            mpg        cyl       disp         hp        drat         wt
mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594
cyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958
disp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799
hp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479
drat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406
wt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000
qsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159
vs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157
am    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953
gear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870
carb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059
            qsec         vs          am       gear        carb
mpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507
cyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829
disp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686
hp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247
drat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980
wt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594
qsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923
vs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714
am   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435
gear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284
carb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.model}\OtherTok{=}\FunctionTok{lm}\NormalTok{(mpg}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{mtcars)}
\FunctionTok{vif}\NormalTok{(lm.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      cyl      disp        hp      drat        wt      qsec        vs        am 
15.373833 21.620241  9.832037  3.374620 15.164887  7.527958  4.965873  4.648487 
     gear      carb 
 5.357452  7.908747 
\end{verbatim}

From the correlation output and VIF output, it is clear that
multicollinearity is an issue. To deal with multicollinearity, we can do
either of the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove one or more variables that are co-linearly related to another
  variable(s).\\
\item
  Create new transformed variables (take linear combinations of
  variables; create ratio of variables, etc).
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{model}{%
\chapter{Model Building \& Scoring for Prediction}\label{model}}

Chapters Chapter~\ref{sec-slr} and Chapter~\ref{sec-mlr} only scratch
the surface of model building. Linear regression is a great initial
approach to take to model building. In fact, in the realm of statistical
models, linear regression (calculated by ordinary least squares) is the
\textbf{best linear unbiased estimator}. The two key pieces to that
previous statement are ``best'' and ``unbiased.''

What does it mean to be \textbf{unbiased}? Each of the sample
coefficients (\(\hat{\beta}\)'s) in the regression model are estimates
of the true coefficients. Just like the statistics back in Chapter
Section~\ref{sec-eda}, these sample coefficients have sampling
distributions - specifically, normally distributed sampling
distributions. The mean of the sampling distribution of
\(\hat{\beta}_j\) is the true (known) coefficient \(\beta_j\). This
means the coefficient is unbiased.

In the preceding chapters, we have only scratched the surface of model
building. Linear regression is a great initial approach to model
building. In fact, in the realm of statistical models, linear regression
(calculated by ordinary least squares) is the \textbf{best linear
unbiased estimator}. The two key pieces to that previous statement are
``best'' and ``unbiased.''

What does it mean to be \textbf{unbiased}? Each of the sample
coefficients (\(\hat{\beta}\)'s) in the regression model are estimates
of the true coefficients. Just like the statistics back in Chapter
Chapter~\ref{sec-intro-stat}, these sample coefficients have sampling
distributions - specifically, normally distributed sampling
distributions. The mean of the sampling distribution of
\(\hat{\beta}_j\) is the true (known) coefficient \(\beta_j\). This
means the coefficient is unbiased.

What does it mean to be \textbf{best}? \emph{IF} the assumptions of
ordinary least squares are met fully, then the sampling distributions of
the coefficients in the model have the \textbf{minimum} variance of all
unbiased estimators.

These two things combined seem like what we want in a model - estimating
what we want (unbiased) and doing it in a way that has the minimum
amount of variation (best among the unbiased). Again, these rely on the
assumptions of linear regression holding true. Another approach to
regression would be to use \textbf{regularized regression} instead as a
different approach to building the model.

This Chapter aims to answer the following questions:

What is regularized regression?

Penalties in Modeling

Ridge Regression

LASSO

Elastic Net

How do you optimize the penalty term?

Overfitting

Cross-Validation (CV)

CV in Regularized Regression

How do you compare different types of models?

Model Metric

Model Scoring

Test Dataset Comparison

\hypertarget{regularized-regression}{%
\section{Regularized Regression}\label{regularized-regression}}

As the number of variables in a linear regression model increase, the
chances of having a model that meets all of the assumptions starts to
diminish. Multicollinearity can pose a large problem with bigger
regression models. As previously seen in Chapter Chapter~\ref{sec-diag},
the coefficients of a linear regression vary widely in the presence of
multicollinearity. These variations lead to overfitting of a regression
model. \textbf{Overfitting} occurs when a dataset predicts the training
data it was built off of really well, but does not generalize to the
test dataset or the population in general. More formally, these models
have higher variance than desired. In those scenarios, moving out of the
realm of unbiased estimates may provide a lower variance in the model,
even though the model is no longer unbiased as described above. We
wouldn't want to be too biased, but some small degree of bias might
improve the model's fit overall.

Another potential problem for linear regression is when we have more
variables than observations in our dataset. This is a common problem in
the space of genetic modeling. In this scenario, the ordinary least
squares approach leads to multiple solutions instead of just one.
Unfortunately, most of these infinite solutions overfit the problem at
hand anyway.

Regularized (or penalized or shrinkage) regression techniques
potentially alleviate these problems. Regularized regression puts
constraints on the estimated coefficients in our model and \emph{shrink}
these estimates to zero. This helps reduce the variation in the
coefficients (improving the variance of the model), but at the cost of
biasing the coefficients. The specific constraints that are put on the
regression inform the three common approaches - \textbf{ridge
regression}, \textbf{LASSO}, and \textbf{elastic nets}.

\hypertarget{penalties-in-models}{%
\subsection{Penalties in Models}\label{penalties-in-models}}

In ordinary least squares linear regression, we minimize the sum of the
squared residuals (or errors) as laid out in Chapter
Chapter~\ref{sec-slr}.

In ordinary least squares linear regression, we minimize the sum of the
squared residuals (or errors) as laid out in Chapter
Chapter~\ref{sec-slr}.

\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2) = min(SSE)
\]

In regularized regression, however, we add a penalty term to the \(SSE\)
as follows:

\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2 + Penalty) = min(SSE + Penalty)
\]

As mentioned above, the penalties we choose constrain the estimated
coefficients in our model and shrink these estimates to zero. Different
penalties have different effects on the estimated coefficients. Two
common approaches to adding penalties are the ridge and LASSO
approaches. The elastic net approach is a combination of these two.
Let's explore each of these in further detail!

\hypertarget{ridge-regression}{%
\subsection{Ridge Regression}\label{ridge-regression}}

Ridge regression adds what is commonly referred to as an ``\(L_2\)''
penalty:

\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p \hat{\beta}^2_j) = min(SSE + \lambda \sum_{j=1}^p \hat{\beta}^2_j)
\]

This penalty is controlled by the \textbf{tuning parameter} \(\lambda\).
If \(\lambda = 0\), then we have typical OLS linear regression. However,
as \(\lambda \rightarrow \infty\), the coefficients in the model shrink
to zero. This makes intuitive sense. Since the estimated coefficients,
\(\hat{\beta}_j\)'s, are the only thing changing to minimize this
equation, then as \(\lambda \rightarrow \infty\), the equation is best
minimized by forcing the coefficients to be smaller and smaller. We will
see how to optimize this penalty term in a later section.

Let's build a regularized regression for our Ames dataset. To build a
ridge regression we need separate data matrices for our predictors and
our target variable. First, we isolate out the variables we are
interested in using the \texttt{select} function. From there the
\texttt{model.matrix} function will create any categorical dummy
variables needed. We also isolate the target variable into its own
vector.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{library}\NormalTok{(AmesHousing)}
\NormalTok{ames }\OtherTok{\textless{}{-}} \FunctionTok{make\_ordinal\_ames}\NormalTok{()}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{ames }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \FunctionTok{row\_number}\NormalTok{())}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_frac}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}

\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{anti\_join}\NormalTok{(ames, train, }\AttributeTok{by =} \StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train\_reg }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Sale\_Price, }
\NormalTok{         Lot\_Area,}
\NormalTok{         Street,}
\NormalTok{         Bldg\_Type,}
\NormalTok{         House\_Style,}
\NormalTok{         Overall\_Qual,}
\NormalTok{         Roof\_Style,}
\NormalTok{         Central\_Air,}
\NormalTok{         First\_Flr\_SF,}
\NormalTok{         Second\_Flr\_SF,}
\NormalTok{         Full\_Bath,}
\NormalTok{         Half\_Bath,}
\NormalTok{         Fireplaces,}
\NormalTok{         Garage\_Area,}
\NormalTok{         Gr\_Liv\_Area, }
\NormalTok{         TotRms\_AbvGrd) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{replace}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.), }\DecValTok{0}\NormalTok{)}

\NormalTok{train\_x }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train\_reg)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{train\_y }\OtherTok{\textless{}{-}}\NormalTok{ train\_reg}\SpecialCharTok{$}\NormalTok{Sale\_Price}
\end{Highlighting}
\end{Shaded}

We will want to do the same thing for the test dataset as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_reg }\OtherTok{\textless{}{-}}\NormalTok{ test }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Sale\_Price, }
\NormalTok{         Lot\_Area,}
\NormalTok{         Street,}
\NormalTok{         Bldg\_Type,}
\NormalTok{         House\_Style,}
\NormalTok{         Overall\_Qual,}
\NormalTok{         Roof\_Style,}
\NormalTok{         Central\_Air,}
\NormalTok{         First\_Flr\_SF,}
\NormalTok{         Second\_Flr\_SF,}
\NormalTok{         Full\_Bath,}
\NormalTok{         Half\_Bath,}
\NormalTok{         Fireplaces,}
\NormalTok{         Garage\_Area,}
\NormalTok{         Gr\_Liv\_Area, }
\NormalTok{         TotRms\_AbvGrd) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{replace}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.), }\DecValTok{0}\NormalTok{)}

\NormalTok{test\_x }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ test\_reg)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{test\_y }\OtherTok{\textless{}{-}}\NormalTok{ test\_reg}\SpecialCharTok{$}\NormalTok{Sale\_Price}
\end{Highlighting}
\end{Shaded}

From there we use the \texttt{glmnet} function with the \texttt{x\ =}
option where we specify the predictor model matrix and the \texttt{y\ =}
option where we specify the target variable. The \texttt{alpha\ =\ 0}
option specifies that a ridge regression will be used as defined in more
detail below in the elastic net section. The \texttt{plot} function
allows us to see the impact of the penalty on the coefficients in the
model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}

\NormalTok{ames\_ridge }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ train\_x,  }\AttributeTok{y =}\NormalTok{ train\_y,  }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(ames\_ridge, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

The \texttt{glmnet} function automatically standardizes the variables
before fitting the regression model. This is important so that all of
the variables are on the same scale before adjustments are made to the
estimated coefficients. Even with this standardization we can see the
large coefficient values for some of the variables. The top of the plot
lists how many variables are in the model at each value of penalty. This
will never change for ridge regression, but does for LASSO.

What \(\lambda\) term is best? That will be discussed in the optimizing
section below, but let's discuss other possible penalties first.

\hypertarget{lasso}{%
\subsection{LASSO}\label{lasso}}

\textbf{Least absolute shrinkage and selection operator} (LASSO)
regression adds what is commonly referred to as an ``\(L_1\)'' penalty:

\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p |\hat{\beta}_j|) = min(SSE + \lambda \sum_{j=1}^p |\hat{\beta}_j|)
\]

This penalty is controlled by the \textbf{tuning parameter} \(\lambda\).
If \(\lambda = 0\), then we have typical OLS linear regression. However,
as \(\lambda \rightarrow \infty\), the coefficients in the model shrink
to zero. This makes intuitive sense. Since the estimated coefficients,
\(\hat{\beta}_j\)'s, are the only thing changing to minimize this
equation, then as \(\lambda \rightarrow \infty\), the equation is best
minimized by forcing the coefficients to be smaller and smaller. We will
see how to optimize this penalty term in a later section.

However, unlike ridge regression that has the coefficient estimates
approach zero asymptotically, in LASSO regression the coefficients can
actually equal zero. This may not be as intuitive when initially looking
at the penalty terms themselves. It becomes easier to see when dealing
with the solutions to the coefficient estimates. Without going into too
much mathematical detail, this is done by taking the derivative of the
minimization function (objective function) and setting it equal to zero.
From there we can determine the optimal solution for the estimated
coefficients. In OLS regression the estimates for the coefficients can
be shown to equal the following (in matrix form):

\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]

This changes in the presence of penalty terms. For ridge regression, the
solution becomes the following:

\[
\hat{\beta} = (X^TX + \lambda I)^{-1}X^TY
\]

There is no value for \(\lambda\) that can force the coefficients to be
zero by itself. Therefore, unless the data makes the coefficient zero,
the penalty term can only force the estimated coefficient to zero
asymptotically as \(\lambda \rightarrow \infty\).

However, for LASSO, the solution becomes the following:

\[
\hat{\beta} = (X^TX)^{-1}(X^TY - \lambda I)
\]

Notice the distinct difference here. In this scenario, there is a
possible penalty value (\(\lambda = X^TY\)) that will force the
estimated coefficients to equal zero. There is some benefit to this.
This makes LASSO also function as a variable selection criteria as well.

Let's build a regularized regression for our Ames dataset using the
LASSO approach. To build a LASSO regression we need separate data
matrices for our predictors and our target variable just like we did for
ridge. From there we use the \texttt{glmnet} function with the
\texttt{x\ =} option where we specify the predictor model matrix and the
\texttt{y\ =} option where we specify the target variable. The
\texttt{alpha\ =\ 1} option specifies that a LASSO regression will be
used as defined in more detail below in the elastic net section. The
\texttt{plot} function allows us to see the impact of the penalty on the
coefficients in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ train\_x,  }\AttributeTok{y =}\NormalTok{ train\_y,  }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(ames\_lasso, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

The \texttt{glmnet} function automatically standardizes the variables
before fitting the regression model. This is important so that all of
the variables are on the same scale before adjustments are made to the
estimated coefficients. Even with this standardization we can see the
large coefficient values for some of the variables. The top of the plot
lists how many variables are in the model at each value of penalty.
Notice as the penalty increases, the number of variables decreases as
variables are forced to zero.

What \(\lambda\) term is best? That will be discussed in the optimizing
section below, but let's discuss the last possible penalty first - the
combination of both ridge and LASSO.

\hypertarget{elastic-net}{%
\subsection{Elastic Net}\label{elastic-net}}

Which approach is better, ridge or LASSO? Both have advantages and
disadvantages. LASSO performs variable selection while ridge keeps all
variables in the model. However, reducing the number of variables might
impact minimum error. Also, if you have two correlated variables, which
one LASSO chooses to zero out is relatively arbitrary to the context of
the problem.

Elastic nets were designed to take advantage of both penalty approaches.
In elastic nets, we are using both penalties in the minimization:

\[
min(SSE + \lambda_1 \sum_{j=1}^p |\hat{\beta}_j| + \lambda_2 \sum_{j=1}^p \hat{\beta}^2_j) 
\]

In R, the \texttt{glmnet} function takes a slightly different approach
to the elastic net implementation with the following:

\[
min(SSE + \lambda[ \alpha \sum_{j=1}^p |\hat{\beta}_j| + (1-\alpha) \sum_{j=1}^p \hat{\beta}^2_j]) 
\]

R still has one penalty \(\lambda\), however, it includes the \(\alpha\)
parameter to balance between the two penalty terms. This is why in
\texttt{glmnet}, the \texttt{alpha\ =\ 1} option gives a LASSO
regression and \texttt{alpha\ =\ 0} gives a ridge regression. Any value
in between zero and one will provide an elastic net.

Let's build a regularized regression for our Ames dataset using the
elastic net approach with an \(\alpha = 0.5\). To build am elastic net
we need separate data matrices for our predictors and our target
variable just like we did for ridge and LASSO. From there we use the
\texttt{glmnet} function with the \texttt{x\ =} option where we specify
the predictor model matrix and the \texttt{y\ =} option where we specify
the target variable. The \texttt{alpha\ =\ 0.5} option specifies that an
elastic net will be used since it is between zero and one. The
\texttt{plot} function allows us to see the impact of the penalty on the
coefficients in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_en }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ train\_x,  }\AttributeTok{y =}\NormalTok{ train\_y,  }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(ames\_en, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

The \texttt{glmnet} function automatically standardizes the variables
before fitting the regression model. This is important so that all of
the variables are on the same scale before adjustments are made to the
estimated coefficients. Even with this standardization we can see the
large coefficient values for some of the variables. The top of the plot
lists how many variables are in the model at each value of penalty.
Notice as the penalty increases, the number of variables decreases as
variables are forced to zero using the LASSO piece of the elastic net
penalty.

What \(\lambda\) term is best? What is the proper balance between ridge
and LASSO penalties when building an elastic net? That will be discussed
in the following section.

\hypertarget{optimizing-penalties}{%
\section{Optimizing Penalties}\label{optimizing-penalties}}

No matter the approach listed above, a penatly term \(\lambda\) needs to
be picked. However, we do not want to get caught overfitting our
training data by minimizing the variance so much that it is not
generalizable to the overall pattern and other similar data. Take the
following plot:

\includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-8-1.pdf}

The red line is overfitted to the dataset and picks up too much of the
unimportant pattern. The orange dotted line is underfit as it does not
pick up enough of the pattern. The light blue, solid line is fit well to
the dataset as it picks up the general pattern while not overfitting to
the dataset.

\hypertarget{cross-validation}{%
\subsection{Cross-Validation}\label{cross-validation}}

\textbf{Cross-validation} is a common approach in modeling to prevent
overfitting of data when you need to \textbf{tune} a parameter. The idea
of cross-validation is to split the training data into multiple pieces,
build the model on a majority of the pieces while evaluating it on the
remaining piece. Then we do the same process again, but switch out which
pieces the model is built and evaluated on.

A common cross-validation (CV) approach is the \(k\)-fold CV. In the
\(k\)-fold CV approach, the model is built \(k\) times. The data is
initially split into \(k\) equally sized pieces. Each time the model is
built, it is built off of \(k-1\) pieces of the data and evaluated on
the last piece. This process is repeated until each piece is left out
for evaluation. This is diagrammed below in Figure Figure~\ref{fig-kCV}.

\begin{figure}

{\centering \includegraphics{./img/kCV.png}

}

\caption{\label{fig-kCV}Example of a 10-fold Cross-Validation}

\end{figure}

\hypertarget{cv-in-regularized-regression}{%
\subsection{CV in Regularized
Regression}\label{cv-in-regularized-regression}}

In R, the \texttt{cv.glmnet} function will automatically implement a
10-fold CV (by default, but can be adjusted through options) to help
evaluate and optimize the \(\lambda\) values for our regularized
regression models.

Let's perform an example using the LASSO regression. The
\texttt{cv.glmnet} function takes the same inputs as the \texttt{glmnet}
function above. Again, we will use the \texttt{plot} function, but this
time we get a different plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lasso\_cv }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ train\_x,  }\AttributeTok{y =}\NormalTok{ train\_y,  }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(ames\_lasso\_cv)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lasso\_cv}\SpecialCharTok{$}\NormalTok{lambda.min }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 41.25712
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lasso\_cv}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3588.334
\end{verbatim}

The above plot shows the results from our cross-validation. Here the
models are evaluated based on their \textbf{mean-squared error} (MSE).
The MSE is defined as \(\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\).
The \(\lambda\) value that minimizes the MSE is 49.69435 (with a
\(\log(\lambda)\) = 3.91). This is highlighted by the first, vertical
dashed line. The second vertical dashed line is the largest \(\lambda\)
value that is one standard error above the minimum value. This value is
especially useful in LASSO regressions. The largest \(\lambda\) within
one standard error would provide approximately the same MSE, but with a
further reduction in the number of variables. Notice that to go from the
first line to the second, the change in MSE is very small, but the
reduction of variables is from 36 variables to around 12 variables.

Let's look at the impact on the coefficients under this penalty using
the \texttt{glmnet} function as before.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ames\_lasso, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{log}\NormalTok{(ames\_lasso\_cv}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FunctionTok{log}\NormalTok{(ames\_lasso\_cv}\SpecialCharTok{$}\NormalTok{lambda.min), }\AttributeTok{col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{lty =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

To investigate which variables are important at a \(\lambda\) value, we
can view the coefficients using the \texttt{coef} function. They are
ranked here:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(ames\_lasso, }\AttributeTok{s =}\NormalTok{ ames\_lasso\_cv}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{rownames =} \StringTok{"row"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(row }\SpecialCharTok{!=} \StringTok{"(Intercept)"}\NormalTok{, s1 }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(s1, }\FunctionTok{reorder}\NormalTok{(row, s1))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Influential Variables"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Coefficient"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-model_building_scoring_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

The variable describing the overall quality of the home is the driving
factor of this model as well as the other variables listed above.

A similar approach can be taken for CV with ridge regression using the
same structure of code. That will not be covered here. Elastic nets are
more complicated in that they have multiple parameters to optimize. For
that approach, an optimization grid will need to be structured to
evaluate different \(\lambda\) values across different \(\alpha\)
values. A loop can be set up to run the \texttt{cv.glmnet} function
across many different values of \(\alpha\). That will not be covered in
detail here.

\hypertarget{model-comparisons}{%
\section{Model Comparisons}\label{model-comparisons}}

Now we have multiple models built for our dataset. To help evaluate
which model is better, we will use the test dataset as described in
Chapter Chapter~\ref{sec-slr}.

The models we have built are nothing but formulas. All we have to do is
put the test dataset in the formula to predict/score the test data. We
\textbf{do not} rerun the algorithm as the goal is \textbf{not} to fit
the test dataset, but to just score it. We need to make sure that we
have the same structure to the test dataset that we do with the training
dataset. Any variable transformations, new variable creations, and
missing value imputations done on the training dataset must be done on
the test dataset in the same way.

\hypertarget{model-metrics}{%
\subsection{Model Metrics}\label{model-metrics}}

Once the predicted values are obtained from each model we need to
evaluate good these predictions are. There are many different metrics to
evaluate models depending on what type of target variable that you have.
Some common metrics for continuous target variables are the square root
of the mean squared error (RMSE), the mean absolute error (MAE), and
mean absolute percentage error (MAPE).

The RMSE is evaluated as follows:

\[
RMSE = \sqrt {\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
\] The RMSE is an approximation of the standard deviation of the
prediction errors of the model. The downside of the RMSE is a lack of
interpretability.

The MAE is evaluated as follows:

\[
MAE = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
\] The MAE gives the average absolute difference between our predictions
and the actual values. This is a symmetric measure with great
interpretability. The main disadvantage of this metric is that it
depends on the scale of the data. For comparing two models evaluated on
the same data, this isn't important. However, when comparing across
different datasets, this may not be as helpful. For example, in
temperature predictions, having an MAE of five degrees for a model built
on Honolulu, Hawaii weather might not be comparable to a model built on
weather in Raleigh, North Carolina.

The MAPE is evaluated as follows:

\[
MAPE = 100 \times \frac{1}{n} \sum_{i=1}^n |\frac{y_i - \hat{y}_i}{y_i}|
\] The MAPE gives the average absolute \emph{percentage} difference
between our predictions and the actual values. This metric is very
interpretable and not dependent on the scale of the data. However, it is
not symmetric like the MAE.

\hypertarget{test-dataset-comparison}{%
\subsection{Test Dataset Comparison}\label{test-dataset-comparison}}

The final model we had from Chapter Chapter~\ref{sec-diag} had the
variables . From this model we can use the \texttt{predict} function
with the \texttt{newdata\ =} option to use score the \texttt{test}
dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sale\_Price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Overall\_Qual }\SpecialCharTok{+}\NormalTok{ House\_Style }\SpecialCharTok{+}\NormalTok{ Garage\_Area }\SpecialCharTok{+}\NormalTok{ Bldg\_Type }\SpecialCharTok{+} 
\NormalTok{    Fireplaces }\SpecialCharTok{+}\NormalTok{ Full\_Bath }\SpecialCharTok{+}\NormalTok{ Half\_Bath }\SpecialCharTok{+}\NormalTok{ Lot\_Area }\SpecialCharTok{+}\NormalTok{ Roof\_Style }\SpecialCharTok{+} 
\NormalTok{    Central\_Air }\SpecialCharTok{+}\NormalTok{ Second\_Flr\_SF }\SpecialCharTok{+}\NormalTok{ TotRms\_AbvGrd }\SpecialCharTok{+}\NormalTok{ First\_Flr\_SF, }\AttributeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\SpecialCharTok{$}\NormalTok{pred\_lm }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ames\_lm, }\AttributeTok{newdata =}\NormalTok{ test)}

\FunctionTok{head}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{pred\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1        2        3        4        5        6 
166099.4 178687.2 233354.4 111583.7 215426.7 129492.1 
\end{verbatim}

To get predictions from the regularized regression models, a \(\lambda\)
value must be selected. For the previous LASSO regression we will choose
the largest \(\lambda\) value within one standard error of the minimum
\(\lambda\) value to help reduce the number of variables. Again, we will
use the \texttt{predict} function. The \texttt{s\ =} option is where we
input the \(\lambda\) value. The \texttt{newx\ =} option is where we
specify the \texttt{test} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_reg}\SpecialCharTok{$}\NormalTok{pred\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ames\_lasso, }\AttributeTok{s =}\NormalTok{ ames\_lasso\_cv}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se, }\AttributeTok{newx =}\NormalTok{ test\_x)}

\FunctionTok{head}\NormalTok{(test\_reg}\SpecialCharTok{$}\NormalTok{pred\_lasso)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        s1
1 156677.8
2 172432.5
3 239922.1
4 105713.6
5 200908.8
6 124913.5
\end{verbatim}

Now we need to calculate the MAE and MAPE for each model for comparison.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lm\_APE =} \DecValTok{100}\SpecialCharTok{*}\FunctionTok{abs}\NormalTok{((Sale\_Price }\SpecialCharTok{{-}}\NormalTok{ pred\_lm)}\SpecialCharTok{/}\NormalTok{Sale\_Price)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{MAPE\_lm =} \FunctionTok{mean}\NormalTok{(lm\_APE))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  MAPE_lm
    <dbl>
1    12.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_reg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lasso\_APE =} \DecValTok{100}\SpecialCharTok{*}\FunctionTok{abs}\NormalTok{((Sale\_Price }\SpecialCharTok{{-}}\NormalTok{ pred\_lasso)}\SpecialCharTok{/}\NormalTok{Sale\_Price)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{MAPE\_lasso =} \FunctionTok{mean}\NormalTok{(lasso\_APE))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  MAPE_lasso
       <dbl>
1       13.4
\end{verbatim}

From the above results, the linear regression from LASSO has a lower
MAPE.

Once we have scored models with the test dataset, we should \textbf{not}
go back to try and rebuild any models. We will use the model with the
lowest MAE or MAPE. This number is also the number that we report on how
well our model performs. No metrics on the training dataset should be
reported for the performance of the model.

\bookmarksetup{startatroot}

\hypertarget{sec-cat}{%
\chapter{Categorical Data Analysis}\label{sec-cat}}

Everything analysis covered so far has used a continuous variable as a
target variable of interest. What if our target variable was categorical
instead of continuous? Our analysis must change to adjust.

This Chapter aims to answer the following questions:

How do you explore categorical variables?

Nominal vs.~Ordinal

Tests of Association

Measures of Association

How do you model a categorical target variable?

Logistic Regression

Interpreting Logistic Regression

Assessing Logistic Regression

\hypertarget{describing-categorical-data}{%
\section{Describing Categorical
Data}\label{describing-categorical-data}}

Similar to Chapter Section~\ref{sec-eda}, we need to first explore our
data before building any models to try and explain/predict our
categorical target variable. With categorical variables, we can look at
the distribution of the categories as well as see if this distribution
has any association with other variables. For this analysis we are going
to still use our Ames housing data. Imagine you worked for a real estate
agency and got a bonus check if you sold a house above \$175,000 in
value. Let's create this variable in our data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{library}\NormalTok{(AmesHousing)}
\NormalTok{ames }\OtherTok{\textless{}{-}} \FunctionTok{make\_ordinal\_ames}\NormalTok{()}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{ames }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =} \FunctionTok{row\_number}\NormalTok{())}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ ames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_frac}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}

\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{anti\_join}\NormalTok{(ames, train, }\AttributeTok{by =} \StringTok{\textquotesingle{}id\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Bonus =} \FunctionTok{ifelse}\NormalTok{(Sale\_Price }\SpecialCharTok{\textgreater{}} \DecValTok{175000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

You are interested in what variables might be associated with obtaining
a higher chance of getting a bonus (selling a house above \$175,000). An
association exists between two categorical variables if the distribution
of one variable changes when the value of the other categorical changes.
If there is no association, the distribution of the first variable is
the same regardless of the value of the other variable. For example, if
we wanted to know if obtaining a bonus on selling a house in Ames, Iowa
was associated with whether the house had central air we could look at
the distribution of bonus eligible houses. If we observe that 42\% of
homes with central air are bonus eligible and 42\% of homes without
central air are bonus eligible, then it appears that central air has no
bearing on whether the home is bonus eligible. However, if instead we
observe that only 3\% of homes without central air are bonus eligible,
but 44\% of home with central air are bonus eligible, then it appears
that having central air might be related to a home being bonus eligible.

To understand the distribution of categorical variables we need to look
at frequency tables. A frequency table shows the number of observations
that occur in certain categories or intervals. A one way frequency table
examines all the categories of one variable. These are easily visualized
with bar charts.

Let's look at the distribution of both bonus eligibility and central air
using the \texttt{table} function. The \texttt{ggplot} function with the
\texttt{geom\_bar} function allows us to view our data in a bar chart.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Bonus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   0    1 
1211  840 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Bonus))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   N    Y 
 147 1904 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Central\_Air))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

Frequency tables show single variables, but if we want to explore two
variables together we look at \textbf{cross-tabulation} tables. A
cross-tabulation table shows the number of observations for each
combination of the row and column variables.

Let's again examine bonus eligibility, but this time across levels of
central air. Again, we can use the \texttt{table} function. The
\texttt{prop.table} function allows us to compare two variables in terms
of proportions instead of frequencies.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   
       0    1
  N  142    5
  Y 1069  835
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   
              0           1
  N 0.069234520 0.002437835
  Y 0.521209166 0.407118479
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Bonus, }\AttributeTok{fill =}\NormalTok{ Central\_Air))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

From the above output we can see that 147 homes have no central air with
only 5 of them being bonus eligible. However, there are 1904 homes that
have central air with 835 of them being bonus eligible. For an even more
detailed breakdown we can use the \texttt{CrossTable} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gmodels)}

\FunctionTok{CrossTable}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

 
   Cell Contents
|-------------------------|
|                       N |
| Chi-square contribution |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  2051 

 
                  | train$Bonus 
train$Central_Air |         0 |         1 | Row Total | 
------------------|-----------|-----------|-----------|
                N |       142 |         5 |       147 | 
                  |    35.112 |    50.620 |           | 
                  |     0.966 |     0.034 |     0.072 | 
                  |     0.117 |     0.006 |           | 
                  |     0.069 |     0.002 |           | 
------------------|-----------|-----------|-----------|
                Y |      1069 |       835 |      1904 | 
                  |     2.711 |     3.908 |           | 
                  |     0.561 |     0.439 |     0.928 | 
                  |     0.883 |     0.994 |           | 
                  |     0.521 |     0.407 |           | 
------------------|-----------|-----------|-----------|
     Column Total |      1211 |       840 |      2051 | 
                  |     0.590 |     0.410 |           | 
------------------|-----------|-----------|-----------|

 
\end{verbatim}

The advantage of the \texttt{CrossTable} function is that we can easily
get not only the frequencies, but the cell, row, and column proportions.
For example, the third number in each cell gives us the row proportion.
For homes without central air, 96.6\% of them are not bonus eligible,
while 3.4\% of them are. For homes with central air, 56.1\% of the homes
are not bonus eligible, while 43.9\% of them are. This would appear that
the distribution of bonus eligible homes changes across levels of
central air - a relationship between the two variables. This expected
relationship needs to be tested statistically for verification.

\hypertarget{tests-of-association}{%
\section{Tests of Association}\label{tests-of-association}}

Much like in Chapter Chapter~\ref{sec-slr} we have statistical tests to
evaluate relationships between two categorical variables. The null
hypothesis for these statistical tests is that the two variables have no
association - the distribution of one variable does not change across
levels of another variable. The alternative hypothesis is an association
between the two variables - the distribution of one variable changes
across levels of another variable.

These statistical tests follow a \(\chi^2\)-distribution. The
\(\chi^2\)-distribution is a distribution that has the following
characteristics:

Bounded below by 0

Right-skewed

One set of degrees of freedom

A plot of a variety of \(\chi^2\)-distributions is shown here:

\includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-8-1.pdf}

Two common \(\chi^2\) tests are the Pearson and Likelihood Ratio
\(\chi^2\) tests. They compare the observed count of observations in
each cell of a cross-tabulation table between two variables to their
expected count \textbf{if} there was no relationship. The expected cell
count applies the overall distribution of one variable across all the
levels of the other variable. For example, overall 59\% of all homes are
not bonus eligible. \textbf{If} that were to apply to every level of
central air, then the 140 homes without central air would be expected to
have 86.73 ( \$ = 147 \times 0.59 \$ ) of them would be bonus eligible
while 60.27 ( \$ = 147 \times 0.41\$ ) of them would not be bonus
eligible. We actually observe 142 and 5 homes for each of these
categories respectively. The further the observed data is from the
expected data, the more evidence we have that there is a relationship
between the two variables.

The test statistic for the Pearson \(\chi^2\) test is the following:

\[
\chi^2_P = \sum_{i=1}^R \sum_{j=1}^C \frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}
\] From the equation above, the closer that the observed count of each
cross-tabulation table cell to the expected count, the smaller the test
statistic. As with all previous hypothesis tests, the smaller the test
statistic, the larger the p-value, implying less evidence for the
alternative hypothesis.

Let's examine the relationship between central air and bonus eligibility
using the \texttt{chisq.test} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test with Yates' continuity correction

data:  table(train$Central_Air, train$Bonus)
X-squared = 90.686, df = 1, p-value < 2.2e-16
\end{verbatim}

The above results shows an extremely small p-value that is below any
reasonable significance level. This implies that we have statistical
evidence for a relationship between having central air and bonus
eligibility of homes. The p-value comes from a \(\chi^2\)-distribution
with degrees of freedom that equal the product of the number of rows
minus one and the number of columns minus one.

Another common test is the Likelihood Ratio test. The test statistic for
this is the following:

\[
\chi^2_L = 2 \times \sum_{i=1}^R \sum_{j=1}^C Obs_{i,j} \times \log(\frac{Obs_{i,j}}{Exp_{i,j}})
\]

The p-value comes from a \(\chi^2\)-distribution with degrees of freedom
that equal the product of the number of rows minus one and the number of
columns minus one. Both of the above tests have a sample size
requirement. The sample size requirement is 80\% or more of the cells in
the cross-tabulation table need \textbf{expected} count larger than 5.

For smaller sample sizes, this might be hard to meet. In those
situations, we can use a more computationally expensive test called
Fisher's exact test. This test calculates every possible permutation of
the data being evaluated to calculate the p-value without any
distributional assumptions. To perform this test we can use the
\texttt{fisher.test} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fisher.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Fisher's Exact Test for Count Data

data:  table(train$Central_Air, train$Bonus)
p-value < 2.2e-16
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  9.213525 69.646380
sample estimates:
odds ratio 
  22.16545 
\end{verbatim}

We see the same results as with the Pearson test because the assumptions
were met for sample size.

Both the Pearson and Likelihood Ratio \(\chi^2\) tests can handle any
type of categorical variable either ordinal, nominal, or both. However,
ordinal variables provide us extra information since the order of the
categories actually matters compared to nominal. We can test for even
more with ordinal variables against other ordinal variables whether two
ordinal variables have a \textbf{linear relationship} as compared to
just a general one. An ordinal test for association is the
Mantel-Haenszel \(\chi^2\) test. The test statistic for the
Mantel-Haenszel \(\chi^2\) test is the following:

\[
\chi^2_{MH} = (n-1)r^2
\] where \(r^2\) is the Pearson correlation between the column and row
variables. This test follows a \(\chi^2\)-distribution with only one
degree of freedom.

Since both the central air and bonus eligibility variables are binary,
they are ordinal. Since they are both ordinal, we should use the
Mantel-Haenszel \(\chi^2\) test with the \texttt{CMHtest} function. In
the main output table, the first row is the Mantel-Haenszel \(\chi^2\)
test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vcdExtra)}

\FunctionTok{CMHtest}\NormalTok{(}\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus))}\SpecialCharTok{$}\NormalTok{table[}\DecValTok{1}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       Chisq           Df         Prob 
9.230619e+01 1.000000e+00 7.425180e-22 
\end{verbatim}

From here we can see another extremely small p-value as we saw in
earlier, more general \(\chi^2\) tests.

\hypertarget{measures-of-association}{%
\section{Measures of Association}\label{measures-of-association}}

Tests of association are best designed for just that, testing the
existence of an association between two categorical variables. However,
just like we saw in Chapter Section~\ref{sec-eda}, hypothesis tests are
impacted by sample size. When we have the same sample size, tests of
association can rank significance of variables with p-values. However,
when sample sizes are not the same between two tests, the tests of
association are not best for comparing the strength of an association.
In those scenarios, we have measures of strength of association that can
be compared across any sample size.

Measures of association were not designed to test if an association
exists, as that is what statistical testing is for. They are designed to
measure the strength of association. There are dozens of these measures.
Three of the most common are the following: - Odds Ratios (only for
comparing two binary variables) - Cramer's V (able to compare nominal
variables with any number of categories) - Spearman's Correlation (able
to compare ordinal variables with any number of categories)

An \textbf{odds ratio} indicates how much more likely, with respect to
\textbf{odds}, a certain event occurs in one group relative to its
occurrence in another group. The odds of an event occurring is
\emph{not} the same as the probability that an event occurs. The odds of
an event occurring is the probability the event occurs divided by the
probability that event does not occur.

\[
Odds = \frac{p}{1-p}
\]

Let's again examine the cross-tabulation table between central air and
bonus eligibility.

\begin{verbatim}

 
   Cell Contents
|-------------------------|
|                       N |
| Chi-square contribution |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  2051 

 
                  | train$Bonus 
train$Central_Air |         0 |         1 | Row Total | 
------------------|-----------|-----------|-----------|
                N |       142 |         5 |       147 | 
                  |    35.112 |    50.620 |           | 
                  |     0.966 |     0.034 |     0.072 | 
                  |     0.117 |     0.006 |           | 
                  |     0.069 |     0.002 |           | 
------------------|-----------|-----------|-----------|
                Y |      1069 |       835 |      1904 | 
                  |     2.711 |     3.908 |           | 
                  |     0.561 |     0.439 |     0.928 | 
                  |     0.883 |     0.994 |           | 
                  |     0.521 |     0.407 |           | 
------------------|-----------|-----------|-----------|
     Column Total |      1211 |       840 |      2051 | 
                  |     0.590 |     0.410 |           | 
------------------|-----------|-----------|-----------|

 
\end{verbatim}

Let's look at the row without central air. The probability that a home
without central air is not bonus eligible is 96.6\%. That implies that
the odds of not being bonus eligible in homes without central air is
28.41 (= 0.966/0.034). For homes with central air, the odds of not being
bonus eligible are 1.28 (= 0.561/0.439). The odds ratio between these
two would be approximately 22.2 (= 28.41/1.28). In other words, homes
without central air are 22.2 times more likely (in terms of odds) to not
be bonus eligible as compared to homes with central air. This
relationship is intuitive based on the numbers we have seen. Without
going into details, it can also be shown that homes with central air are
22.2 times as likely (in terms of odds) to be bonus eligible.

We can use the \texttt{OddsRatio} function to get these same results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}

\FunctionTok{OddsRatio}\NormalTok{(}\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 22.18335
\end{verbatim}

\textbf{Cramer's V} is another measure of strength of association.
Cramer's V is calculated as follows:

\[
V = \sqrt{\frac{\chi^2_P/n}{\min(Rows-1, Columns-1)}}
\]

Cramer's V is bounded between 0 and 1 for every comparison other than
two binary variables. For two binary variables being compared the bounds
are -1 to 1. The idea is still the same for both. The further the value
is from 0, the stronger the relationship. Unfortunately, unlike \(R^2\),
Cramer's V has no interpretative value. It can only be used for
comparison.

We use the \texttt{assocstats} function to get the Cramer's V value.
This function also provides the Pearson and Likelihood Ratio \(\chi^2\)
tests as well.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{assocstats}\NormalTok{(}\FunctionTok{table}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air, train}\SpecialCharTok{$}\NormalTok{Bonus))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                     X^2 df P(> X^2)
Likelihood Ratio 121.499  1        0
Pearson           92.351  1        0

Phi-Coefficient   : 0.212 
Contingency Coeff.: 0.208 
Cramer's V        : 0.212 
\end{verbatim}

Lastly, we have Spearman's correlation. Much like the Mantel-Haenszel
test of association was specifically designed for comparing two ordinal
variables, Spearman correlation measures the strength of association
between two ordinal variables. Spearman is not limited to only
categorical data analysis as it was also seen back in Chapter
Chapter~\ref{sec-diag} with detecting heteroskedasticity. Remember,
Spearman correlation is a correlation on the ranks of the observations
as compared to the actual values of the observations.

The \texttt{cor.test} function that gave us Pearson's correlation also
provides Spearman's correlation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{ordered}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Central\_Air)), }
         \AttributeTok{y =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{ordered}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Bonus)), }
         \AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Spearman's rank correlation rho

data:  x and y
S = 1132826666, p-value < 2.2e-16
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.2121966 
\end{verbatim}

As previously mentioned, these are only a few of the dozens of different
measures of association that exist. However, they are the most used
ones.

\hypertarget{introduction-to-logistic-regression}{%
\section{Introduction to Logistic
Regression}\label{introduction-to-logistic-regression}}

After exploring the categorical target variable, we can move on to
modeling the categorical target variable. Logistic regression is a
fundamental statistical analysis for data science and analytics. It part
of a class of modeling techniques known as classification models since
they are trying to predict categorical target variables. This target
variable can be binary, ordinal, or even nominal in its structure. The
primary focus will be binary logistic regression. It is the most common
type of logistic regression, and sets up the foundation for both ordinal
and nominal logistic regression.

Ordinary least squares regression is not the best approach to modeling
categorical target variables. Mathematically, it can be shown that with
a binary target variable coded as 0 and 1, an OLS linear regression
model will produce the \textbf{linear probability model}.

\hypertarget{linear-probability-model}{%
\subsection{Linear Probability Model}\label{linear-probability-model}}

The linear probability model is not as widely used since probabilities
do not tend to follow the properties of linearity in relation to their
predictors. Also, the linear probability model possibly produces
predictions outside of the bounds of 0 and 1 (where probabilities should
be!). For completeness sake however, here is the linear probability
model using the \texttt{lm} function to try and predict bonus
eligibility.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lp.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Bonus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{with}\NormalTok{(train, }\FunctionTok{plot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Gr\_Liv\_Area, }\AttributeTok{y =}\NormalTok{ Bonus,}
               \AttributeTok{main =} \StringTok{\textquotesingle{}OLS Regression?\textquotesingle{}}\NormalTok{,}
               \AttributeTok{xlab =} \StringTok{\textquotesingle{}Greater Living Area (Sqft)\textquotesingle{}}\NormalTok{,}
               \AttributeTok{ylab =} \StringTok{\textquotesingle{}Bonus Eligibility\textquotesingle{}}\NormalTok{))}
\FunctionTok{abline}\NormalTok{(lp.model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

Even though it doesn't appear to really look like our data, let's fit
this linear probability model anyway for completeness sake.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lp.model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Bonus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area, }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(lp.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Bonus ~ Gr_Liv_Area, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.70766 -0.29160 -0.09983  0.39432  0.86198 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -4.149e-01  2.776e-02  -14.94   <2e-16 ***
Gr_Liv_Area  5.534e-04  1.765e-05   31.36   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4044 on 2049 degrees of freedom
Multiple R-squared:  0.3243,    Adjusted R-squared:  0.324 
F-statistic: 983.6 on 1 and 2049 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(}\FunctionTok{rstandard}\NormalTok{(lp.model),}
       \AttributeTok{ylab =} \StringTok{"Standardized Residuals"}\NormalTok{,}
       \AttributeTok{xlab =} \StringTok{"Normal Scores"}\NormalTok{,}
       \AttributeTok{main =} \StringTok{"QQ{-}Plot of Residuals"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(}\FunctionTok{rstandard}\NormalTok{(lp.model))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-17-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{predict}\NormalTok{(lp.model), }\FunctionTok{resid}\NormalTok{(lp.model), }
     \AttributeTok{ylab=}\StringTok{"Residuals"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Predicted Values"}\NormalTok{, }
     \AttributeTok{main=}\StringTok{"Residuals of Linear Probability Model"}\NormalTok{) }
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-categorical_files/figure-pdf/unnamed-chunk-17-2.pdf}

}

\end{figure}

As we can see from the charts above, the assumptions of ordinary least
squares don't really hold in this situation. Therefore, we should be
careful interpreting the results of the model. Maybe a better model
won't have these problems?

\hypertarget{binary-logistic-regression}{%
\subsection{Binary Logistic
Regression}\label{binary-logistic-regression}}

Due to the limitations of the linear probability model, people typically
just use the binary logistic regression model. The logistic regression
model does not have the limitations of the linear probability model. The
outcome of the logistic regression model is the probability of getting a
1 in a binary variable. That probability is calculated as follows:

\[
p_i = \frac{1}{1+e^{-(\beta_0 + \beta_1x_{1,i} + \cdots + \beta_k x_{k,i})}}
\]

This function has the desired properties for predicting probabilities.
The predicted probability from the above equation will always be between
0 and 1. The parameter estimates do not enter the function linearly
(this is a non-linear regression model), and the rate of change of the
probability varies as the predictor variables vary as seen in Figure
Figure~\ref{fig-logistic}.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./img/logistic.png}

}

\caption{\label{fig-logistic}Example of a Logistic Curve}

\end{figure}

To create a linear model, a \textbf{link function} is applied to the
probabilities. The specific link function for logistic regression is
called the \textbf{logit} function.

\[
logit(p_i) = \log(\frac{p_i}{1-p_i}) = \beta_0 + \beta_1x_{1,i} + \cdots + \beta_k x_{k,i}
\]

The relationship between the predictor variables and the logits are
linear in nature as the logits themselves are unbounded. This structure
looks much more like our linear regression model structure. However,
logistic regression does not use OLS to estimate the coefficients in our
model. OLS requires residuals which the logistic regression model does
not provide. The target variable is binary in nature, but the
predictions are probabilities. Therefore, we cannot calculate a
traditional residual. Instead, logistic regression uses maximum
likelihood estimation. This is not covered here.

There are two main assumptions for logistic regression:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independence of observations
\item
  Linearity of the logit
\end{enumerate}

The first assumption of independence is the same as we had for linear
regression. The second assumption implies that the logistic function
transformation (the logit) actually makes a linear relationship with our
predictor variables. This assumption can be tested, but will not be
covered in this brief introduction to logistic regression.

Let's build a logistic regression model. We will use the \texttt{glm}
function to do this. The \texttt{glm} function has a similar structure
to the \texttt{lm} function. The main difference is the
\texttt{family\ =\ binomial(link\ =\ "logit")} option to specify that we
are uses a logistic regression model. Again, there are many different
link functions, but only the logistic link function (the logit) is being
used here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_logit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Bonus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area, }
                   \AttributeTok{data =}\NormalTok{ train, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(ames\_logit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = Bonus ~ Gr_Liv_Area, family = binomial(link = "logit"), 
    data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.5796  -0.6942  -0.3647   0.8060   2.1857  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -6.1348858  0.2757473  -22.25   <2e-16 ***
Gr_Liv_Area  0.0038463  0.0001799   21.38   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2775.8  on 2050  degrees of freedom
Residual deviance: 1926.4  on 2049  degrees of freedom
AIC: 1930.4

Number of Fisher Scoring iterations: 5
\end{verbatim}

Let's examine the above output. Scanning down the output, you can see
the actual logistic regression equation for the variable
\texttt{Gr\_Liv\_Area}. Here we can see that it appears to be a
significant variable at predicting bonus eligibility. However, the
coefficient reported does not have the same usable interpretation as in
linear regression. An increase of one unit of greater living area square
footage is linearly related to the logit \textbf{not} the probability of
bonus eligibility. We can transform this coefficient to make it more
interpretable. A single unit increase in greater living area square
footage \textbf{does} have a \(100 \times (e^\hat{\beta}-1)\%\) increase
in the average \textbf{odds} of bonus eligibility. We can use a
combination of the \texttt{exp} and \texttt{coef} functions to obtain
this number.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{100}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\FunctionTok{coef}\NormalTok{(ames\_logit), }\FunctionTok{confint}\NormalTok{(ames\_logit)))}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Waiting for profiling to be done...
\end{verbatim}

\begin{verbatim}
                              2.5 %      97.5 %
(Intercept) -99.7834027 -99.8755103 -99.6328865
Gr_Liv_Area   0.3853699   0.3508132   0.4216532
\end{verbatim}

In other words, every additional square foot in greater living area in
the home leads to an average increase in odds of 0.385\% to be bonus
eligible.

\hypertarget{adding-categorical-variables}{%
\subsection{Adding Categorical
Variables}\label{adding-categorical-variables}}

Similar to linear regression as we learned in Chapter
Chapter~\ref{sec-mlr}, logistic regression can have both continuous and
categorical predictors for our categorical target variable. Let's add
both central air as well as number of fireplaces to our logistic
regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ames\_logit2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Bonus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gr\_Liv\_Area }\SpecialCharTok{+}\NormalTok{ Central\_Air }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(Fireplaces), }
                  \AttributeTok{data =}\NormalTok{ train, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(ames\_logit2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), 
    family = binomial(link = "logit"), data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.4340  -0.6467  -0.2822   0.6610   2.8866  

Coefficients:
                      Estimate Std. Error z value Pr(>|z|)    
(Intercept)         -9.970e+00  6.549e-01 -15.223  < 2e-16 ***
Gr_Liv_Area          3.759e-03  2.031e-04  18.506  < 2e-16 ***
Central_AirY         3.564e+00  5.310e-01   6.711 1.93e-11 ***
factor(Fireplaces)1  9.822e-01  1.253e-01   7.837 4.60e-15 ***
factor(Fireplaces)2  6.734e-01  2.406e-01   2.799  0.00513 ** 
factor(Fireplaces)3 -3.993e-02  8.711e-01  -0.046  0.96344    
factor(Fireplaces)4  9.025e+00  3.247e+02   0.028  0.97783    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2775.8  on 2050  degrees of freedom
Residual deviance: 1746.2  on 2044  degrees of freedom
AIC: 1760.2

Number of Fisher Scoring iterations: 11
\end{verbatim}

Just like with linear regression, categorical predictor variables are a
comparison between two categories. Again, the coefficients from the
logistic regression model need to be transformed to be interpreted.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{100}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\FunctionTok{coef}\NormalTok{(ames\_logit2), }\FunctionTok{confint}\NormalTok{(ames\_logit2)))}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                         2.5 %        97.5 %
(Intercept)         -9.999532e+01  -99.9988238   -99.9843742
Gr_Liv_Area          3.766413e-01    0.3376191     0.4175879
Central_AirY         3.429115e+03 1264.8396217 11177.2021480
factor(Fireplaces)1  1.670410e+02  108.9856565   241.6360910
factor(Fireplaces)2  9.607987e+01   22.4764491   214.9571145
factor(Fireplaces)3 -3.914247e+00  -81.9507819   497.4825425
factor(Fireplaces)4  8.308750e+05 -100.0000000            NA
\end{verbatim}

Let's use the first fireplace variable as an example. A home with one
fireplace has, on average, 167.04\% higher odds of being bonus eligible
as compared to a home with zero fireplaces.

\hypertarget{model-assessment}{%
\subsection{Model Assessment}\label{model-assessment}}

There are dozens of different ways to evaluate a logistic regression
model. We will cover one popular way here - concordance. Counting the
number of \textbf{concordant}, \textbf{discordant}, and \textbf{tied}
pairs is a way to to assess how well the model fits the data.

To find concordant, discordant, and tied pairs, we must compare all of
the 0's in the target variable to all of the 1's. For our example, we
will compare every pair of homes where one home is bonus eligible and
one is not (every 0 and 1 pair). A \textbf{concordant} pair is a 0 and 1
pair where the bonus eligible home (the 1 in our model) has a higher
predicted probability than the non-bonus eligible home (the 0 in our
model) - our model successfully ordered these two observations by
probability. It does not matter what the actual predicted probability
values are as long as the bonus eligible home has a higher predicted
probability than the non-bonus eligible home. A \textbf{discordant} pair
is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has
a lower predicted probability than the non-bonus eligible home (the 0 in
our model) - our model unsuccessfully ordered the homes. It does not
matter what the actual predicted probability values are as long as the
bonus eligible home has a lower predicted probability than the non-bonus
eligible home. A \textbf{tied} pair is a 0 and 1 pair where the bonus
eligible home has the same predicted probability as the non-bonus
eligible home - the model is confused and sees these two different
things as the same. In general, you want a high percentage of concordant
pairs and low percentages of discordant and tied pairs.

We can use the \texttt{Concordance} function to obtain these values on
our predictions from the \texttt{predict} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(InformationValue)}

\FunctionTok{Concordance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Bonus, }\FunctionTok{predict}\NormalTok{(ames\_logit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$Concordance
[1] 0.862933

$Discordance
[1] 0.137067

$Tied
[1] -5.551115e-17

$Pairs
[1] 1017240
\end{verbatim}

From the above output we have a concordance of 86.3\% for our model.
There is no good or bad value as this can only be compared with another
model to see which is better. Let's compare this to our model with the
categorical variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Concordance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Bonus, }\FunctionTok{predict}\NormalTok{(ames\_logit2, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$Concordance
[1] 0.8839182

$Discordance
[1] 0.1160818

$Tied
[1] 4.163336e-17

$Pairs
[1] 1017240
\end{verbatim}

We can see that the model with categorical predictors added to it has a
higher concordance at 88.4\%. That implies that our model is correctly
able to rank our observations 88.4\% of the time. This is \textbf{NOT}
the same thing as saying our model is 88.4\% accurate. Accuracy (which
is not covered here) deals with a prediction being correct or incorrect.
Concordance is only measuring how often we are able to predict 1's with
higher probability than 0's - again, correctly ranking the observations.

\hypertarget{variable-selection-and-regularized-regression}{%
\subsection{Variable Selection and Regularized
Regression}\label{variable-selection-and-regularized-regression}}

As with linear regression in Chapters Chapter~\ref{sec-mlr} and
Chapter~\ref{sec-sel}, logistic regression uses the same approaches to
doing variable selection. In fact, the same function are used as well.
Let's use the \texttt{step} function to apply a forward and backward
selection to the logistic regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train\_sel\_log }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Bonus, }
\NormalTok{         Lot\_Area,}
\NormalTok{         Street,}
\NormalTok{         Bldg\_Type,}
\NormalTok{         House\_Style,}
\NormalTok{         Overall\_Qual,}
\NormalTok{         Roof\_Style,}
\NormalTok{         Central\_Air,}
\NormalTok{         First\_Flr\_SF,}
\NormalTok{         Second\_Flr\_SF,}
\NormalTok{         Full\_Bath,}
\NormalTok{         Half\_Bath,}
\NormalTok{         Fireplaces,}
\NormalTok{         Garage\_Area,}
\NormalTok{         Gr\_Liv\_Area, }
\NormalTok{         TotRms\_AbvGrd) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{replace}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.), }\DecValTok{0}\NormalTok{)}

\NormalTok{full.model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Bonus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . , }\AttributeTok{data =}\NormalTok{ train\_sel\_log)}

\NormalTok{empty.model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Bonus }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ train\_sel\_log)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{for.model }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(empty.model,}
                  \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =} \FunctionTok{formula}\NormalTok{(empty.model),}
                               \AttributeTok{upper =} \FunctionTok{formula}\NormalTok{(full.model)),}
                  \AttributeTok{direction =} \StringTok{"forward"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{log}\NormalTok{(}\FunctionTok{dim}\NormalTok{(train\_sel\_log)[}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=2918.59
Bonus ~ 1

                Df Deviance    AIC
+ Overall_Qual   9   234.74 1453.0
+ Full_Bath      1   320.51 2030.8
+ Gr_Liv_Area    1   335.11 2122.1
+ Garage_Area    1   367.62 2312.0
+ First_Flr_SF   1   396.15 2465.3
+ Fireplaces     1   415.26 2561.9
+ TotRms_AbvGrd  1   422.53 2597.6
+ House_Style    7   444.22 2745.9
+ Half_Bath      1   459.40 2769.1
+ Second_Flr_SF  1   461.26 2777.4
+ Central_Air    1   473.64 2831.7
+ Lot_Area       1   478.42 2852.3
+ Bldg_Type      4   477.21 2870.0
<none>               495.97 2918.6
+ Street         1   495.86 2925.8
+ Roof_Style     5   491.40 2937.7

Step:  AIC=1452.99
Bonus ~ Overall_Qual

                Df Deviance    AIC
+ Full_Bath      1   203.87 1171.4
+ Gr_Liv_Area    1   208.62 1218.7
+ First_Flr_SF   1   218.70 1315.5
+ Garage_Area    1   223.10 1356.3
+ TotRms_AbvGrd  1   223.12 1356.5
+ Fireplaces     1   223.33 1358.5
+ Lot_Area       1   225.07 1374.3
+ Bldg_Type      4   227.31 1417.6
+ Second_Flr_SF  1   230.98 1427.5
+ Half_Bath      1   231.81 1434.8
+ Central_Air    1   233.55 1450.2
<none>               234.74 1453.0
+ Street         1   234.30 1456.8
+ House_Style    7   230.28 1467.1
+ Roof_Style     5   234.18 1486.2

Step:  AIC=1171.42
Bonus ~ Overall_Qual + Full_Bath

                Df Deviance    AIC
+ Fireplaces     1   193.70 1074.1
+ First_Flr_SF   1   195.11 1089.0
+ Gr_Liv_Area    1   196.26 1101.0
+ Lot_Area       1   197.22 1111.0
+ Bldg_Type      4   195.24 1113.2
+ Garage_Area    1   197.63 1115.3
+ Half_Bath      1   201.31 1153.1
+ TotRms_AbvGrd  1   202.51 1165.3
+ Central_Air    1   202.61 1166.4
<none>               203.87 1171.4
+ Street         1   203.38 1174.2
+ Second_Flr_SF  1   203.85 1178.9
+ House_Style    7   201.34 1199.2
+ Roof_Style     5   203.40 1204.8

Step:  AIC=1074.14
Bonus ~ Overall_Qual + Full_Bath + Fireplaces

                Df Deviance    AIC
+ Garage_Area    1   188.96 1030.9
+ First_Flr_SF   1   189.34 1035.0
+ Bldg_Type      4   187.45 1037.3
+ Lot_Area       1   190.10 1043.3
+ Gr_Liv_Area    1   190.64 1049.1
+ Half_Bath      1   192.04 1064.0
<none>               193.70 1074.1
+ Central_Air    1   193.02 1074.5
+ Street         1   193.35 1078.0
+ TotRms_AbvGrd  1   193.42 1078.7
+ Second_Flr_SF  1   193.69 1081.7
+ House_Style    7   190.38 1092.0
+ Roof_Style     5   193.16 1106.5

Step:  AIC=1030.9
Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area

                Df Deviance    AIC
+ Bldg_Type      4   183.72 1003.8
+ First_Flr_SF   1   186.24 1008.8
+ Lot_Area       1   186.46 1011.2
+ Gr_Liv_Area    1   186.87 1015.7
+ Half_Bath      1   187.55 1023.2
<none>               188.96 1030.9
+ Street         1   188.68 1035.5
+ Central_Air    1   188.76 1036.3
+ TotRms_AbvGrd  1   188.78 1036.6
+ Second_Flr_SF  1   188.96 1038.5
+ House_Style    7   186.42 1056.5
+ Roof_Style     5   188.20 1060.8

Step:  AIC=1003.78
Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
    Bldg_Type

                Df Deviance     AIC
+ First_Flr_SF   1   181.17  982.68
+ Half_Bath      1   181.99  991.99
+ Gr_Liv_Area    1   182.00  992.09
+ Lot_Area       1   182.09  993.12
<none>               183.72 1003.78
+ Street         1   183.36 1007.35
+ Central_Air    1   183.60 1009.98
+ TotRms_AbvGrd  1   183.66 1010.71
+ Second_Flr_SF  1   183.72 1011.35
+ House_Style    7   180.41 1019.87
+ Roof_Style     5   182.90 1032.69

Step:  AIC=982.68
Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
    Bldg_Type + First_Flr_SF

                Df Deviance     AIC
+ Half_Bath      1   177.46  947.94
+ Second_Flr_SF  1   180.10  978.18
+ Lot_Area       1   180.24  979.73
+ Gr_Liv_Area    1   180.28  980.18
<none>               181.17  982.68
+ Street         1   180.77  985.80
+ House_Style    7   176.95  987.73
+ Central_Air    1   181.06  989.10
+ TotRms_AbvGrd  1   181.16  990.19
+ Roof_Style     5   179.76 1004.77

Step:  AIC=947.94
Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
    Bldg_Type + First_Flr_SF + Half_Bath

                Df Deviance    AIC
+ Lot_Area       1   176.75 947.31
<none>               177.46 947.94
+ TotRms_AbvGrd  1   177.03 950.49
+ Street         1   177.09 951.18
+ Central_Air    1   177.43 955.17
+ Gr_Liv_Area    1   177.46 955.45
+ Second_Flr_SF  1   177.46 955.56
+ Roof_Style     5   175.95 968.54
+ House_Style    7   174.78 970.01

Step:  AIC=947.31
Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
    Bldg_Type + First_Flr_SF + Half_Bath + Lot_Area

                Df Deviance    AIC
<none>               176.75 947.31
+ TotRms_AbvGrd  1   176.32 949.96
+ Street         1   176.54 952.49
+ Central_Air    1   176.72 954.56
+ Gr_Liv_Area    1   176.73 954.71
+ Second_Flr_SF  1   176.75 954.89
+ Roof_Style     5   175.25 968.01
+ House_Style    7   174.09 969.62
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{back.model }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(full.model,}
                   \AttributeTok{scope =} \FunctionTok{list}\NormalTok{(}\AttributeTok{lower =} \FunctionTok{formula}\NormalTok{(empty.model),}
                                \AttributeTok{upper =} \FunctionTok{formula}\NormalTok{(full.model)),}
                   \AttributeTok{direction =} \StringTok{"backward"}\NormalTok{, }\AttributeTok{k =} \FunctionTok{log}\NormalTok{(}\FunctionTok{dim}\NormalTok{(train\_sel\_log)[}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Start:  AIC=1016.59
Bonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + 
    Roof_Style + Central_Air + First_Flr_SF + Second_Flr_SF + 
    Full_Bath + Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + 
    TotRms_AbvGrd

                Df Deviance     AIC
- House_Style    7   174.31  995.05
- Roof_Style     5   173.24  997.67
- Gr_Liv_Area    1   171.63 1009.07
- Central_Air    1   171.64 1009.12
- Second_Flr_SF  1   171.71 1009.95
- First_Flr_SF   1   171.84 1011.48
- Street         1   171.89 1012.09
- TotRms_AbvGrd  1   171.94 1012.72
- Lot_Area       1   172.08 1014.43
<none>               171.62 1016.59
- Garage_Area    1   172.50 1019.38
- Half_Bath      1   173.37 1029.70
- Fireplaces     1   173.90 1035.95
- Bldg_Type      4   177.07 1050.13
- Full_Bath      1   187.25 1187.64
- Overall_Qual   9   217.74 1436.04

Step:  AIC=995.05
Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + Roof_Style + 
    Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath + 
    Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd

                Df Deviance     AIC
- Roof_Style     5   175.81  974.50
- Central_Air    1   174.35  987.91
- Gr_Liv_Area    1   174.43  988.84
- Second_Flr_SF  1   174.49  989.53
- Street         1   174.53  989.99
- TotRms_AbvGrd  1   174.74  992.45
- Lot_Area       1   174.80  993.19
- First_Flr_SF   1   174.89  994.22
<none>               174.31  995.05
- Garage_Area    1   175.57 1002.23
- Fireplaces     1   176.38 1011.68
- Half_Bath      1   176.86 1017.24
- Bldg_Type      4   179.33 1022.77
- Full_Bath      1   191.76 1183.08
- Overall_Qual   9   222.96 1431.28

Step:  AIC=974.5
Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + Central_Air + 
    First_Flr_SF + Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + 
    Garage_Area + Gr_Liv_Area + TotRms_AbvGrd

                Df Deviance     AIC
- Central_Air    1   175.85  967.29
- Gr_Liv_Area    1   175.91  968.02
- Second_Flr_SF  1   175.97  968.79
- Street         1   176.01  969.19
- TotRms_AbvGrd  1   176.30  972.55
- Lot_Area       1   176.31  972.65
- First_Flr_SF   1   176.31  972.66
<none>               175.81  974.50
- Garage_Area    1   177.04  981.15
- Fireplaces     1   177.86  990.64
- Half_Bath      1   178.19  994.42
- Bldg_Type      4   180.75 1000.81
- Full_Bath      1   193.59 1164.44
- Overall_Qual   9   224.35 1405.92

Step:  AIC=967.29
Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + First_Flr_SF + 
    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + 
    Gr_Liv_Area + TotRms_AbvGrd

                Df Deviance     AIC
- Gr_Liv_Area    1   175.95  960.84
- Second_Flr_SF  1   176.01  961.59
- Street         1   176.03  961.83
- TotRms_AbvGrd  1   176.34  965.38
- First_Flr_SF   1   176.35  965.52
- Lot_Area       1   176.35  965.52
<none>               175.85  967.29
- Garage_Area    1   177.19  975.25
- Fireplaces     1   177.95  984.00
- Half_Bath      1   178.34  988.55
- Bldg_Type      4   180.88  994.63
- Full_Bath      1   193.82 1159.28
- Overall_Qual   9   224.36 1398.33

Step:  AIC=960.84
Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + First_Flr_SF + 
    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + 
    TotRms_AbvGrd

                Df Deviance     AIC
- Street         1   176.13  955.40
- Second_Flr_SF  1   176.14  955.45
- Lot_Area       1   176.44  959.00
- TotRms_AbvGrd  1   176.54  960.09
<none>               175.95  960.84
- Garage_Area    1   177.32  969.16
- Fireplaces     1   178.04  977.46
- Half_Bath      1   178.50  982.75
- Bldg_Type      4   180.94  987.72
- First_Flr_SF   1   179.15  990.17
- Full_Bath      1   193.95 1153.02
- Overall_Qual   9   224.57 1392.65

Step:  AIC=955.4
Bonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + 
    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + 
    TotRms_AbvGrd

                Df Deviance     AIC
- Second_Flr_SF  1   176.32  949.96
- TotRms_AbvGrd  1   176.75  954.89
- Lot_Area       1   176.78  955.30
<none>               176.13  955.40
- Garage_Area    1   177.53  963.91
- Fireplaces     1   178.25  972.22
- Half_Bath      1   178.71  977.53
- Bldg_Type      4   181.03  981.12
- First_Flr_SF   1   179.28  984.03
- Full_Bath      1   194.12 1147.16
- Overall_Qual   9   224.74 1386.53

Step:  AIC=949.96
Bonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + 
    Full_Bath + Half_Bath + Fireplaces + Garage_Area + TotRms_AbvGrd

                Df Deviance     AIC
- TotRms_AbvGrd  1   176.75  947.31
<none>               176.32  949.96
- Lot_Area       1   177.03  950.49
- Garage_Area    1   177.78  959.22
- Fireplaces     1   178.72  970.00
- Bldg_Type      4   181.35  977.15
- Half_Bath      1   180.23  987.27
- First_Flr_SF   1   180.28  987.88
- Full_Bath      1   198.37 1183.99
- Overall_Qual   9   225.34 1384.40

Step:  AIC=947.31
Bonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + 
    Full_Bath + Half_Bath + Fireplaces + Garage_Area

               Df Deviance     AIC
<none>              176.75  947.31
- Lot_Area      1   177.46  947.94
- Garage_Area   1   178.30  957.58
- Fireplaces    1   178.97  965.27
- Bldg_Type     4   181.72  973.64
- Half_Bath     1   180.24  979.73
- First_Flr_SF  1   180.34  980.88
- Full_Bath     1   199.82 1191.34
- Overall_Qual  9   225.73 1380.31
\end{verbatim}

In the above two approaches we used the BIC selection criteria. Here
both forward and backward selection actually picked the same model.
Let's check the concordance of this model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Concordance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Bonus, }\FunctionTok{predict}\NormalTok{(back.model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$Concordance
[1] 0.9612668

$Discordance
[1] 0.03873324

$Tied
[1] -2.081668e-17

$Pairs
[1] 1017240
\end{verbatim}

Not surprisingly, this model outperforms the previous model that we had
with a concordance of 96.1\%.

Although not covered in detail here, regularized regression can also be
applied to logistic regression to get a different view. This might be
helpful with the multicollinearity present in these predictor variables.
Again, we can use the \texttt{glmnet} function with the addition of a
\texttt{family\ =\ "binomial"} option.



\end{document}
